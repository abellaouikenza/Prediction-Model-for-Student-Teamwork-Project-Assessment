{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"T2_features_final.csv\")\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "rf_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", rf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Create an SVM classifier\n",
    "svm_classifier = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "svm_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", svm_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "gb_classifier = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "gb_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", gb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4166666666666667\n"
     ]
    }
   ],
   "source": [
    "# Create a logistic regression classifier\n",
    "logreg_classifier = LogisticRegression()\n",
    "logreg_classifier = LogisticRegression(max_iter=1000)\n",
    "# Train the classifier\n",
    "logreg_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logreg_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "lr_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", lr_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalizer(norm='max')\n",
    "norm.fit(X_train)\n",
    "X_train = norm.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1482 - accuracy: 0.4375\n",
      "Epoch 1: val_accuracy improved from -inf to 0.66667, saving model to best_model_T2.h5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.1482 - accuracy: 0.4375 - val_loss: 0.7879 - val_accuracy: 0.6667\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8260 - accuracy: 0.5417\n",
      "Epoch 2: val_accuracy did not improve from 0.66667\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.8260 - accuracy: 0.5417 - val_loss: 0.8991 - val_accuracy: 0.6667\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7164 - accuracy: 0.4375\n",
      "Epoch 3: val_accuracy improved from 0.66667 to 0.75000, saving model to best_model_T2.h5\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.7164 - accuracy: 0.4375 - val_loss: 0.7603 - val_accuracy: 0.7500\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6893 - accuracy: 0.6458\n",
      "Epoch 4: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.6893 - accuracy: 0.6458 - val_loss: 0.8259 - val_accuracy: 0.5833\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6660 - accuracy: 0.5833\n",
      "Epoch 5: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.6660 - accuracy: 0.5833 - val_loss: 0.7513 - val_accuracy: 0.7500\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6505 - accuracy: 0.6458\n",
      "Epoch 6: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.6505 - accuracy: 0.6458 - val_loss: 0.7885 - val_accuracy: 0.5833\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6368 - accuracy: 0.6458\n",
      "Epoch 7: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.6368 - accuracy: 0.6458 - val_loss: 0.7420 - val_accuracy: 0.7500\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6258 - accuracy: 0.6875\n",
      "Epoch 8: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.6258 - accuracy: 0.6875 - val_loss: 0.7642 - val_accuracy: 0.6667\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6159 - accuracy: 0.7083\n",
      "Epoch 9: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.6159 - accuracy: 0.7083 - val_loss: 0.7342 - val_accuracy: 0.7500\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6074 - accuracy: 0.7083\n",
      "Epoch 10: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.6074 - accuracy: 0.7083 - val_loss: 0.7476 - val_accuracy: 0.7500\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5995 - accuracy: 0.7083\n",
      "Epoch 11: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.5995 - accuracy: 0.7083 - val_loss: 0.7271 - val_accuracy: 0.6667\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5923 - accuracy: 0.7500\n",
      "Epoch 12: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.5923 - accuracy: 0.7500 - val_loss: 0.7351 - val_accuracy: 0.7500\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5857 - accuracy: 0.7500\n",
      "Epoch 13: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.5857 - accuracy: 0.7500 - val_loss: 0.7208 - val_accuracy: 0.6667\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5796 - accuracy: 0.7292\n",
      "Epoch 14: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.5796 - accuracy: 0.7292 - val_loss: 0.7254 - val_accuracy: 0.6667\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5739 - accuracy: 0.7500\n",
      "Epoch 15: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.5739 - accuracy: 0.7500 - val_loss: 0.7155 - val_accuracy: 0.6667\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5686 - accuracy: 0.7500\n",
      "Epoch 16: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.5686 - accuracy: 0.7500 - val_loss: 0.7187 - val_accuracy: 0.6667\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5637 - accuracy: 0.7708\n",
      "Epoch 17: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.5637 - accuracy: 0.7708 - val_loss: 0.7118 - val_accuracy: 0.6667\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5592 - accuracy: 0.7500\n",
      "Epoch 18: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.5592 - accuracy: 0.7500 - val_loss: 0.7140 - val_accuracy: 0.6667\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5551 - accuracy: 0.7500\n",
      "Epoch 19: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 1s 703ms/step - loss: 0.5551 - accuracy: 0.7500 - val_loss: 0.7094 - val_accuracy: 0.6667\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5512 - accuracy: 0.7708\n",
      "Epoch 20: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.5512 - accuracy: 0.7708 - val_loss: 0.7110 - val_accuracy: 0.6667\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5476 - accuracy: 0.7708\n",
      "Epoch 21: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.5476 - accuracy: 0.7708 - val_loss: 0.7078 - val_accuracy: 0.6667\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5440 - accuracy: 0.7708\n",
      "Epoch 22: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 0.5440 - accuracy: 0.7708 - val_loss: 0.7087 - val_accuracy: 0.6667\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5407 - accuracy: 0.7917\n",
      "Epoch 23: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.5407 - accuracy: 0.7917 - val_loss: 0.7067 - val_accuracy: 0.5833\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5374 - accuracy: 0.7917\n",
      "Epoch 24: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.5374 - accuracy: 0.7917 - val_loss: 0.7073 - val_accuracy: 0.5833\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5343 - accuracy: 0.7917\n",
      "Epoch 25: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.5343 - accuracy: 0.7917 - val_loss: 0.7059 - val_accuracy: 0.5833\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5313 - accuracy: 0.7917\n",
      "Epoch 26: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.5313 - accuracy: 0.7917 - val_loss: 0.7065 - val_accuracy: 0.5833\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5284 - accuracy: 0.7917\n",
      "Epoch 27: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.5284 - accuracy: 0.7917 - val_loss: 0.7058 - val_accuracy: 0.5833\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.7917\n",
      "Epoch 28: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.5258 - accuracy: 0.7917 - val_loss: 0.7063 - val_accuracy: 0.5833\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5231 - accuracy: 0.7917\n",
      "Epoch 29: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.5231 - accuracy: 0.7917 - val_loss: 0.7058 - val_accuracy: 0.5833\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5206 - accuracy: 0.7917\n",
      "Epoch 30: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.5206 - accuracy: 0.7917 - val_loss: 0.7063 - val_accuracy: 0.5833\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5180 - accuracy: 0.7917\n",
      "Epoch 31: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.5180 - accuracy: 0.7917 - val_loss: 0.7062 - val_accuracy: 0.5833\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5156 - accuracy: 0.7917\n",
      "Epoch 32: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.5156 - accuracy: 0.7917 - val_loss: 0.7068 - val_accuracy: 0.5833\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5133 - accuracy: 0.7917\n",
      "Epoch 33: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.5133 - accuracy: 0.7917 - val_loss: 0.7069 - val_accuracy: 0.5833\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5109 - accuracy: 0.7917\n",
      "Epoch 34: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.5109 - accuracy: 0.7917 - val_loss: 0.7077 - val_accuracy: 0.5833\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5087 - accuracy: 0.7917\n",
      "Epoch 35: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.5087 - accuracy: 0.7917 - val_loss: 0.7082 - val_accuracy: 0.5833\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5066 - accuracy: 0.7917\n",
      "Epoch 36: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 0.5066 - accuracy: 0.7917 - val_loss: 0.7092 - val_accuracy: 0.5833\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5045 - accuracy: 0.7917\n",
      "Epoch 37: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.5045 - accuracy: 0.7917 - val_loss: 0.7093 - val_accuracy: 0.5833\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5026 - accuracy: 0.7917\n",
      "Epoch 38: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.5026 - accuracy: 0.7917 - val_loss: 0.7104 - val_accuracy: 0.5833\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5008 - accuracy: 0.7917\n",
      "Epoch 39: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.5008 - accuracy: 0.7917 - val_loss: 0.7108 - val_accuracy: 0.6667\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4989 - accuracy: 0.7917\n",
      "Epoch 40: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.4989 - accuracy: 0.7917 - val_loss: 0.7116 - val_accuracy: 0.6667\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4971 - accuracy: 0.7917\n",
      "Epoch 41: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.4971 - accuracy: 0.7917 - val_loss: 0.7123 - val_accuracy: 0.6667\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4954 - accuracy: 0.7917\n",
      "Epoch 42: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.4954 - accuracy: 0.7917 - val_loss: 0.7129 - val_accuracy: 0.6667\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4937 - accuracy: 0.7917\n",
      "Epoch 43: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.4937 - accuracy: 0.7917 - val_loss: 0.7137 - val_accuracy: 0.6667\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4921 - accuracy: 0.7917\n",
      "Epoch 44: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.4921 - accuracy: 0.7917 - val_loss: 0.7145 - val_accuracy: 0.6667\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4904 - accuracy: 0.7917\n",
      "Epoch 45: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.4904 - accuracy: 0.7917 - val_loss: 0.7156 - val_accuracy: 0.6667\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4889 - accuracy: 0.7917\n",
      "Epoch 46: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.4889 - accuracy: 0.7917 - val_loss: 0.7162 - val_accuracy: 0.6667\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4873 - accuracy: 0.7917\n",
      "Epoch 47: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.4873 - accuracy: 0.7917 - val_loss: 0.7172 - val_accuracy: 0.6667\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4858 - accuracy: 0.7917\n",
      "Epoch 48: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.4858 - accuracy: 0.7917 - val_loss: 0.7179 - val_accuracy: 0.6667\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4844 - accuracy: 0.7917\n",
      "Epoch 49: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.4844 - accuracy: 0.7917 - val_loss: 0.7190 - val_accuracy: 0.6667\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4830 - accuracy: 0.8125\n",
      "Epoch 50: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.4830 - accuracy: 0.8125 - val_loss: 0.7198 - val_accuracy: 0.6667\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4816 - accuracy: 0.7917\n",
      "Epoch 51: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.4816 - accuracy: 0.7917 - val_loss: 0.7208 - val_accuracy: 0.5833\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4803 - accuracy: 0.8125\n",
      "Epoch 52: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.4803 - accuracy: 0.8125 - val_loss: 0.7217 - val_accuracy: 0.6667\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4790 - accuracy: 0.8125\n",
      "Epoch 53: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 0.4790 - accuracy: 0.8125 - val_loss: 0.7229 - val_accuracy: 0.5833\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4777 - accuracy: 0.8125\n",
      "Epoch 54: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.4777 - accuracy: 0.8125 - val_loss: 0.7237 - val_accuracy: 0.5833\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4764 - accuracy: 0.8125\n",
      "Epoch 55: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.4764 - accuracy: 0.8125 - val_loss: 0.7250 - val_accuracy: 0.5833\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4751 - accuracy: 0.8125\n",
      "Epoch 56: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.4751 - accuracy: 0.8125 - val_loss: 0.7259 - val_accuracy: 0.5833\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4739 - accuracy: 0.8125\n",
      "Epoch 57: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.4739 - accuracy: 0.8125 - val_loss: 0.7273 - val_accuracy: 0.5833\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4727 - accuracy: 0.8125\n",
      "Epoch 58: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.4727 - accuracy: 0.8125 - val_loss: 0.7282 - val_accuracy: 0.5833\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4715 - accuracy: 0.8125\n",
      "Epoch 59: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.4715 - accuracy: 0.8125 - val_loss: 0.7296 - val_accuracy: 0.5833\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4704 - accuracy: 0.8125\n",
      "Epoch 60: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.4704 - accuracy: 0.8125 - val_loss: 0.7305 - val_accuracy: 0.5833\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4692 - accuracy: 0.8125\n",
      "Epoch 61: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.4692 - accuracy: 0.8125 - val_loss: 0.7317 - val_accuracy: 0.5833\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4681 - accuracy: 0.7917\n",
      "Epoch 62: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.4681 - accuracy: 0.7917 - val_loss: 0.7326 - val_accuracy: 0.5833\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4670 - accuracy: 0.8125\n",
      "Epoch 63: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4670 - accuracy: 0.8125 - val_loss: 0.7337 - val_accuracy: 0.5833\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4660 - accuracy: 0.7917\n",
      "Epoch 64: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 1s 895ms/step - loss: 0.4660 - accuracy: 0.7917 - val_loss: 0.7348 - val_accuracy: 0.5833\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4649 - accuracy: 0.8125\n",
      "Epoch 65: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 1s 649ms/step - loss: 0.4649 - accuracy: 0.8125 - val_loss: 0.7358 - val_accuracy: 0.5833\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4639 - accuracy: 0.8125\n",
      "Epoch 66: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.4639 - accuracy: 0.8125 - val_loss: 0.7371 - val_accuracy: 0.5833\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4629 - accuracy: 0.8125\n",
      "Epoch 67: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.4629 - accuracy: 0.8125 - val_loss: 0.7381 - val_accuracy: 0.5833\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4619 - accuracy: 0.8125\n",
      "Epoch 68: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.4619 - accuracy: 0.8125 - val_loss: 0.7392 - val_accuracy: 0.5833\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4609 - accuracy: 0.8125\n",
      "Epoch 69: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.4609 - accuracy: 0.8125 - val_loss: 0.7404 - val_accuracy: 0.5833\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4600 - accuracy: 0.8125\n",
      "Epoch 70: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.4600 - accuracy: 0.8125 - val_loss: 0.7413 - val_accuracy: 0.5833\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4590 - accuracy: 0.8125\n",
      "Epoch 71: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.4590 - accuracy: 0.8125 - val_loss: 0.7422 - val_accuracy: 0.5833\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4581 - accuracy: 0.8125\n",
      "Epoch 72: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 0.4581 - accuracy: 0.8125 - val_loss: 0.7431 - val_accuracy: 0.5833\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4572 - accuracy: 0.8125\n",
      "Epoch 73: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.4572 - accuracy: 0.8125 - val_loss: 0.7441 - val_accuracy: 0.5833\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4563 - accuracy: 0.8125\n",
      "Epoch 74: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.4563 - accuracy: 0.8125 - val_loss: 0.7451 - val_accuracy: 0.5833\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4554 - accuracy: 0.8125\n",
      "Epoch 75: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.4554 - accuracy: 0.8125 - val_loss: 0.7458 - val_accuracy: 0.5833\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4545 - accuracy: 0.8125\n",
      "Epoch 76: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.4545 - accuracy: 0.8125 - val_loss: 0.7467 - val_accuracy: 0.5833\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4536 - accuracy: 0.8125\n",
      "Epoch 77: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.4536 - accuracy: 0.8125 - val_loss: 0.7477 - val_accuracy: 0.5833\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4527 - accuracy: 0.8125\n",
      "Epoch 78: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.4527 - accuracy: 0.8125 - val_loss: 0.7484 - val_accuracy: 0.5833\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4518 - accuracy: 0.8125\n",
      "Epoch 79: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.4518 - accuracy: 0.8125 - val_loss: 0.7493 - val_accuracy: 0.5833\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4510 - accuracy: 0.8333\n",
      "Epoch 80: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.4510 - accuracy: 0.8333 - val_loss: 0.7503 - val_accuracy: 0.5833\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4501 - accuracy: 0.8333\n",
      "Epoch 81: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.4501 - accuracy: 0.8333 - val_loss: 0.7513 - val_accuracy: 0.5833\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4493 - accuracy: 0.8333\n",
      "Epoch 82: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.4493 - accuracy: 0.8333 - val_loss: 0.7522 - val_accuracy: 0.5833\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4485 - accuracy: 0.8333\n",
      "Epoch 83: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.4485 - accuracy: 0.8333 - val_loss: 0.7533 - val_accuracy: 0.5833\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4477 - accuracy: 0.8333\n",
      "Epoch 84: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.4477 - accuracy: 0.8333 - val_loss: 0.7544 - val_accuracy: 0.5833\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4469 - accuracy: 0.8333\n",
      "Epoch 85: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.4469 - accuracy: 0.8333 - val_loss: 0.7552 - val_accuracy: 0.5833\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4461 - accuracy: 0.8333\n",
      "Epoch 86: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.4461 - accuracy: 0.8333 - val_loss: 0.7563 - val_accuracy: 0.5833\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4454 - accuracy: 0.8333\n",
      "Epoch 87: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.4454 - accuracy: 0.8333 - val_loss: 0.7576 - val_accuracy: 0.5833\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4446 - accuracy: 0.8333\n",
      "Epoch 88: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.4446 - accuracy: 0.8333 - val_loss: 0.7582 - val_accuracy: 0.5833\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4439 - accuracy: 0.8333\n",
      "Epoch 89: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.4439 - accuracy: 0.8333 - val_loss: 0.7596 - val_accuracy: 0.5833\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4431 - accuracy: 0.8333\n",
      "Epoch 90: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.4431 - accuracy: 0.8333 - val_loss: 0.7602 - val_accuracy: 0.5833\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4424 - accuracy: 0.8333\n",
      "Epoch 91: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.4424 - accuracy: 0.8333 - val_loss: 0.7615 - val_accuracy: 0.5833\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4416 - accuracy: 0.8333\n",
      "Epoch 92: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.4416 - accuracy: 0.8333 - val_loss: 0.7624 - val_accuracy: 0.5833\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4409 - accuracy: 0.8333\n",
      "Epoch 93: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.4409 - accuracy: 0.8333 - val_loss: 0.7637 - val_accuracy: 0.5833\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4401 - accuracy: 0.8333\n",
      "Epoch 94: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4401 - accuracy: 0.8333 - val_loss: 0.7644 - val_accuracy: 0.5833\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4394 - accuracy: 0.8333\n",
      "Epoch 95: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4394 - accuracy: 0.8333 - val_loss: 0.7655 - val_accuracy: 0.5833\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4386 - accuracy: 0.8333\n",
      "Epoch 96: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4386 - accuracy: 0.8333 - val_loss: 0.7664 - val_accuracy: 0.5833\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4379 - accuracy: 0.8333\n",
      "Epoch 97: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 0.4379 - accuracy: 0.8333 - val_loss: 0.7678 - val_accuracy: 0.5833\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4371 - accuracy: 0.8333\n",
      "Epoch 98: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.4371 - accuracy: 0.8333 - val_loss: 0.7685 - val_accuracy: 0.5833\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4364 - accuracy: 0.8333\n",
      "Epoch 99: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.4364 - accuracy: 0.8333 - val_loss: 0.7697 - val_accuracy: 0.5833\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4357 - accuracy: 0.8333\n",
      "Epoch 100: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.4357 - accuracy: 0.8333 - val_loss: 0.7706 - val_accuracy: 0.5833\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4350 - accuracy: 0.8333\n",
      "Epoch 101: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.4350 - accuracy: 0.8333 - val_loss: 0.7720 - val_accuracy: 0.5833\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4343 - accuracy: 0.8333\n",
      "Epoch 102: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.4343 - accuracy: 0.8333 - val_loss: 0.7729 - val_accuracy: 0.5833\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4336 - accuracy: 0.8542\n",
      "Epoch 103: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.4336 - accuracy: 0.8542 - val_loss: 0.7741 - val_accuracy: 0.5833\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4329 - accuracy: 0.8333\n",
      "Epoch 104: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.4329 - accuracy: 0.8333 - val_loss: 0.7750 - val_accuracy: 0.5833\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4322 - accuracy: 0.8542\n",
      "Epoch 105: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.4322 - accuracy: 0.8542 - val_loss: 0.7763 - val_accuracy: 0.5833\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4316 - accuracy: 0.8333\n",
      "Epoch 106: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.4316 - accuracy: 0.8333 - val_loss: 0.7770 - val_accuracy: 0.5833\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4309 - accuracy: 0.8542\n",
      "Epoch 107: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.4309 - accuracy: 0.8542 - val_loss: 0.7787 - val_accuracy: 0.5833\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4302 - accuracy: 0.8333\n",
      "Epoch 108: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.4302 - accuracy: 0.8333 - val_loss: 0.7794 - val_accuracy: 0.5833\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4296 - accuracy: 0.8542\n",
      "Epoch 109: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.4296 - accuracy: 0.8542 - val_loss: 0.7809 - val_accuracy: 0.5833\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4289 - accuracy: 0.8542\n",
      "Epoch 110: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.4289 - accuracy: 0.8542 - val_loss: 0.7814 - val_accuracy: 0.5833\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4282 - accuracy: 0.8542\n",
      "Epoch 111: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.4282 - accuracy: 0.8542 - val_loss: 0.7831 - val_accuracy: 0.5833\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4276 - accuracy: 0.8542\n",
      "Epoch 112: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.4276 - accuracy: 0.8542 - val_loss: 0.7834 - val_accuracy: 0.5833\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4269 - accuracy: 0.8542\n",
      "Epoch 113: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.4269 - accuracy: 0.8542 - val_loss: 0.7850 - val_accuracy: 0.5833\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4263 - accuracy: 0.8542\n",
      "Epoch 114: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.4263 - accuracy: 0.8542 - val_loss: 0.7856 - val_accuracy: 0.5833\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4256 - accuracy: 0.8542\n",
      "Epoch 115: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.4256 - accuracy: 0.8542 - val_loss: 0.7872 - val_accuracy: 0.5833\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4250 - accuracy: 0.8542\n",
      "Epoch 116: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.4250 - accuracy: 0.8542 - val_loss: 0.7876 - val_accuracy: 0.5833\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4244 - accuracy: 0.8542\n",
      "Epoch 117: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.4244 - accuracy: 0.8542 - val_loss: 0.7893 - val_accuracy: 0.5833\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4237 - accuracy: 0.8542\n",
      "Epoch 118: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.4237 - accuracy: 0.8542 - val_loss: 0.7897 - val_accuracy: 0.5833\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4231 - accuracy: 0.8542\n",
      "Epoch 119: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.4231 - accuracy: 0.8542 - val_loss: 0.7914 - val_accuracy: 0.5833\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4225 - accuracy: 0.8542\n",
      "Epoch 120: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.4225 - accuracy: 0.8542 - val_loss: 0.7919 - val_accuracy: 0.5833\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4219 - accuracy: 0.8542\n",
      "Epoch 121: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.4219 - accuracy: 0.8542 - val_loss: 0.7932 - val_accuracy: 0.5833\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4213 - accuracy: 0.8542\n",
      "Epoch 122: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.4213 - accuracy: 0.8542 - val_loss: 0.7939 - val_accuracy: 0.5833\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4207 - accuracy: 0.8542\n",
      "Epoch 123: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.4207 - accuracy: 0.8542 - val_loss: 0.7951 - val_accuracy: 0.5833\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4201 - accuracy: 0.8542\n",
      "Epoch 124: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.4201 - accuracy: 0.8542 - val_loss: 0.7961 - val_accuracy: 0.5833\n",
      "Epoch 125/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4195 - accuracy: 0.8542\n",
      "Epoch 125: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.4195 - accuracy: 0.8542 - val_loss: 0.7974 - val_accuracy: 0.5833\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4189 - accuracy: 0.8542\n",
      "Epoch 126: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.4189 - accuracy: 0.8542 - val_loss: 0.7981 - val_accuracy: 0.5833\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.8542\n",
      "Epoch 127: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.4183 - accuracy: 0.8542 - val_loss: 0.7993 - val_accuracy: 0.5833\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4177 - accuracy: 0.8542\n",
      "Epoch 128: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.4177 - accuracy: 0.8542 - val_loss: 0.8001 - val_accuracy: 0.5833\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4172 - accuracy: 0.8542\n",
      "Epoch 129: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.4172 - accuracy: 0.8542 - val_loss: 0.8014 - val_accuracy: 0.5833\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4166 - accuracy: 0.8542\n",
      "Epoch 130: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.4166 - accuracy: 0.8542 - val_loss: 0.8024 - val_accuracy: 0.5833\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4160 - accuracy: 0.8542\n",
      "Epoch 131: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.4160 - accuracy: 0.8542 - val_loss: 0.8036 - val_accuracy: 0.5833\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4154 - accuracy: 0.8542\n",
      "Epoch 132: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.4154 - accuracy: 0.8542 - val_loss: 0.8041 - val_accuracy: 0.5833\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4148 - accuracy: 0.8542\n",
      "Epoch 133: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.4148 - accuracy: 0.8542 - val_loss: 0.8055 - val_accuracy: 0.5833\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4142 - accuracy: 0.8542\n",
      "Epoch 134: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.4142 - accuracy: 0.8542 - val_loss: 0.8063 - val_accuracy: 0.5833\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4136 - accuracy: 0.8542\n",
      "Epoch 135: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.4136 - accuracy: 0.8542 - val_loss: 0.8075 - val_accuracy: 0.5833\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4130 - accuracy: 0.8542\n",
      "Epoch 136: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.4130 - accuracy: 0.8542 - val_loss: 0.8082 - val_accuracy: 0.5833\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4124 - accuracy: 0.8542\n",
      "Epoch 137: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.4124 - accuracy: 0.8542 - val_loss: 0.8093 - val_accuracy: 0.5833\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4118 - accuracy: 0.8542\n",
      "Epoch 138: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.4118 - accuracy: 0.8542 - val_loss: 0.8102 - val_accuracy: 0.5833\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4113 - accuracy: 0.8542\n",
      "Epoch 139: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.4113 - accuracy: 0.8542 - val_loss: 0.8113 - val_accuracy: 0.5833\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4107 - accuracy: 0.8542\n",
      "Epoch 140: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.4107 - accuracy: 0.8542 - val_loss: 0.8120 - val_accuracy: 0.5833\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4101 - accuracy: 0.8542\n",
      "Epoch 141: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.4101 - accuracy: 0.8542 - val_loss: 0.8129 - val_accuracy: 0.5833\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4095 - accuracy: 0.8542\n",
      "Epoch 142: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.4095 - accuracy: 0.8542 - val_loss: 0.8137 - val_accuracy: 0.5833\n",
      "Epoch 143/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4090 - accuracy: 0.8542\n",
      "Epoch 143: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.4090 - accuracy: 0.8542 - val_loss: 0.8148 - val_accuracy: 0.5833\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4085 - accuracy: 0.8542\n",
      "Epoch 144: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.4085 - accuracy: 0.8542 - val_loss: 0.8156 - val_accuracy: 0.5833\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4080 - accuracy: 0.8542\n",
      "Epoch 145: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.4080 - accuracy: 0.8542 - val_loss: 0.8166 - val_accuracy: 0.5833\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4075 - accuracy: 0.8542\n",
      "Epoch 146: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.4075 - accuracy: 0.8542 - val_loss: 0.8180 - val_accuracy: 0.5833\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4070 - accuracy: 0.8542\n",
      "Epoch 147: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.4070 - accuracy: 0.8542 - val_loss: 0.8189 - val_accuracy: 0.5833\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4065 - accuracy: 0.8542\n",
      "Epoch 148: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.4065 - accuracy: 0.8542 - val_loss: 0.8198 - val_accuracy: 0.5833\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4059 - accuracy: 0.8542\n",
      "Epoch 149: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.4059 - accuracy: 0.8542 - val_loss: 0.8210 - val_accuracy: 0.5833\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4054 - accuracy: 0.8542\n",
      "Epoch 150: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.4054 - accuracy: 0.8542 - val_loss: 0.8216 - val_accuracy: 0.5833\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4049 - accuracy: 0.8542\n",
      "Epoch 151: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.4049 - accuracy: 0.8542 - val_loss: 0.8231 - val_accuracy: 0.5833\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4044 - accuracy: 0.8542\n",
      "Epoch 152: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.4044 - accuracy: 0.8542 - val_loss: 0.8237 - val_accuracy: 0.5833\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4039 - accuracy: 0.8542\n",
      "Epoch 153: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.4039 - accuracy: 0.8542 - val_loss: 0.8247 - val_accuracy: 0.5833\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4035 - accuracy: 0.8542\n",
      "Epoch 154: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.4035 - accuracy: 0.8542 - val_loss: 0.8257 - val_accuracy: 0.5833\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4030 - accuracy: 0.8542\n",
      "Epoch 155: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.4030 - accuracy: 0.8542 - val_loss: 0.8264 - val_accuracy: 0.5833\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4025 - accuracy: 0.8542\n",
      "Epoch 156: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.4025 - accuracy: 0.8542 - val_loss: 0.8275 - val_accuracy: 0.5833\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4020 - accuracy: 0.8542\n",
      "Epoch 157: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.4020 - accuracy: 0.8542 - val_loss: 0.8285 - val_accuracy: 0.5833\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4015 - accuracy: 0.8542\n",
      "Epoch 158: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.4015 - accuracy: 0.8542 - val_loss: 0.8291 - val_accuracy: 0.5833\n",
      "Epoch 159/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4010 - accuracy: 0.8542\n",
      "Epoch 159: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.4010 - accuracy: 0.8542 - val_loss: 0.8305 - val_accuracy: 0.5833\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4005 - accuracy: 0.8542\n",
      "Epoch 160: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 0.4005 - accuracy: 0.8542 - val_loss: 0.8310 - val_accuracy: 0.5833\n",
      "Epoch 161/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4001 - accuracy: 0.8542\n",
      "Epoch 161: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.4001 - accuracy: 0.8542 - val_loss: 0.8322 - val_accuracy: 0.5833\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3996 - accuracy: 0.8542\n",
      "Epoch 162: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.3996 - accuracy: 0.8542 - val_loss: 0.8328 - val_accuracy: 0.5833\n",
      "Epoch 163/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3991 - accuracy: 0.8542\n",
      "Epoch 163: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.3991 - accuracy: 0.8542 - val_loss: 0.8341 - val_accuracy: 0.5833\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3986 - accuracy: 0.8542\n",
      "Epoch 164: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.3986 - accuracy: 0.8542 - val_loss: 0.8347 - val_accuracy: 0.5833\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3982 - accuracy: 0.8542\n",
      "Epoch 165: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.3982 - accuracy: 0.8542 - val_loss: 0.8358 - val_accuracy: 0.5833\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3977 - accuracy: 0.8542\n",
      "Epoch 166: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3977 - accuracy: 0.8542 - val_loss: 0.8362 - val_accuracy: 0.5833\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3972 - accuracy: 0.8542\n",
      "Epoch 167: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3972 - accuracy: 0.8542 - val_loss: 0.8377 - val_accuracy: 0.5833\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3967 - accuracy: 0.8542\n",
      "Epoch 168: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.3967 - accuracy: 0.8542 - val_loss: 0.8380 - val_accuracy: 0.5833\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3963 - accuracy: 0.8542\n",
      "Epoch 169: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3963 - accuracy: 0.8542 - val_loss: 0.8392 - val_accuracy: 0.5833\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3958 - accuracy: 0.8542\n",
      "Epoch 170: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3958 - accuracy: 0.8542 - val_loss: 0.8399 - val_accuracy: 0.5833\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3953 - accuracy: 0.8542\n",
      "Epoch 171: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 0.3953 - accuracy: 0.8542 - val_loss: 0.8407 - val_accuracy: 0.5833\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3949 - accuracy: 0.8542\n",
      "Epoch 172: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.3949 - accuracy: 0.8542 - val_loss: 0.8411 - val_accuracy: 0.5833\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3944 - accuracy: 0.8542\n",
      "Epoch 173: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.3944 - accuracy: 0.8542 - val_loss: 0.8424 - val_accuracy: 0.5833\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3940 - accuracy: 0.8542\n",
      "Epoch 174: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.3940 - accuracy: 0.8542 - val_loss: 0.8428 - val_accuracy: 0.5833\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3935 - accuracy: 0.8542\n",
      "Epoch 175: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3935 - accuracy: 0.8542 - val_loss: 0.8438 - val_accuracy: 0.5833\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3931 - accuracy: 0.8542\n",
      "Epoch 176: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.3931 - accuracy: 0.8542 - val_loss: 0.8443 - val_accuracy: 0.5833\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3927 - accuracy: 0.8542\n",
      "Epoch 177: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.3927 - accuracy: 0.8542 - val_loss: 0.8454 - val_accuracy: 0.5833\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3922 - accuracy: 0.8542\n",
      "Epoch 178: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.3922 - accuracy: 0.8542 - val_loss: 0.8459 - val_accuracy: 0.5833\n",
      "Epoch 179/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3918 - accuracy: 0.8542\n",
      "Epoch 179: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.3918 - accuracy: 0.8542 - val_loss: 0.8473 - val_accuracy: 0.5833\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3914 - accuracy: 0.8542\n",
      "Epoch 180: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.3914 - accuracy: 0.8542 - val_loss: 0.8474 - val_accuracy: 0.5833\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3909 - accuracy: 0.8542\n",
      "Epoch 181: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.3909 - accuracy: 0.8542 - val_loss: 0.8487 - val_accuracy: 0.5833\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3905 - accuracy: 0.8542\n",
      "Epoch 182: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.3905 - accuracy: 0.8542 - val_loss: 0.8490 - val_accuracy: 0.5833\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3901 - accuracy: 0.8542\n",
      "Epoch 183: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.3901 - accuracy: 0.8542 - val_loss: 0.8506 - val_accuracy: 0.5833\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3897 - accuracy: 0.8542\n",
      "Epoch 184: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.3897 - accuracy: 0.8542 - val_loss: 0.8503 - val_accuracy: 0.5833\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3892 - accuracy: 0.8542\n",
      "Epoch 185: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.3892 - accuracy: 0.8542 - val_loss: 0.8521 - val_accuracy: 0.5833\n",
      "Epoch 186/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3888 - accuracy: 0.8542\n",
      "Epoch 186: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3888 - accuracy: 0.8542 - val_loss: 0.8516 - val_accuracy: 0.5833\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3884 - accuracy: 0.8542\n",
      "Epoch 187: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.3884 - accuracy: 0.8542 - val_loss: 0.8536 - val_accuracy: 0.5833\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3880 - accuracy: 0.8542\n",
      "Epoch 188: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.3880 - accuracy: 0.8542 - val_loss: 0.8530 - val_accuracy: 0.5000\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3876 - accuracy: 0.8542\n",
      "Epoch 189: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.3876 - accuracy: 0.8542 - val_loss: 0.8552 - val_accuracy: 0.5833\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3871 - accuracy: 0.8542\n",
      "Epoch 190: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.3871 - accuracy: 0.8542 - val_loss: 0.8547 - val_accuracy: 0.5000\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3867 - accuracy: 0.8542\n",
      "Epoch 191: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3867 - accuracy: 0.8542 - val_loss: 0.8562 - val_accuracy: 0.5833\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3863 - accuracy: 0.8542\n",
      "Epoch 192: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3863 - accuracy: 0.8542 - val_loss: 0.8562 - val_accuracy: 0.5000\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3859 - accuracy: 0.8542\n",
      "Epoch 193: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.3859 - accuracy: 0.8542 - val_loss: 0.8579 - val_accuracy: 0.5833\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3855 - accuracy: 0.8542\n",
      "Epoch 194: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3855 - accuracy: 0.8542 - val_loss: 0.8576 - val_accuracy: 0.5000\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3851 - accuracy: 0.8542\n",
      "Epoch 195: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.3851 - accuracy: 0.8542 - val_loss: 0.8594 - val_accuracy: 0.5833\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3847 - accuracy: 0.8542\n",
      "Epoch 196: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.3847 - accuracy: 0.8542 - val_loss: 0.8589 - val_accuracy: 0.5000\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3843 - accuracy: 0.8542\n",
      "Epoch 197: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.3843 - accuracy: 0.8542 - val_loss: 0.8609 - val_accuracy: 0.5833\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3840 - accuracy: 0.8542\n",
      "Epoch 198: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3840 - accuracy: 0.8542 - val_loss: 0.8606 - val_accuracy: 0.5000\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3836 - accuracy: 0.8542\n",
      "Epoch 199: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.3836 - accuracy: 0.8542 - val_loss: 0.8623 - val_accuracy: 0.5833\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3832 - accuracy: 0.8542\n",
      "Epoch 200: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.3832 - accuracy: 0.8542 - val_loss: 0.8623 - val_accuracy: 0.5000\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3828 - accuracy: 0.8542\n",
      "Epoch 201: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.3828 - accuracy: 0.8542 - val_loss: 0.8642 - val_accuracy: 0.5833\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3825 - accuracy: 0.8542\n",
      "Epoch 202: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.3825 - accuracy: 0.8542 - val_loss: 0.8637 - val_accuracy: 0.5000\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3821 - accuracy: 0.8542\n",
      "Epoch 203: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.3821 - accuracy: 0.8542 - val_loss: 0.8657 - val_accuracy: 0.5833\n",
      "Epoch 204/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3817 - accuracy: 0.8542\n",
      "Epoch 204: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.3817 - accuracy: 0.8542 - val_loss: 0.8656 - val_accuracy: 0.5000\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3813 - accuracy: 0.8542\n",
      "Epoch 205: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.3813 - accuracy: 0.8542 - val_loss: 0.8672 - val_accuracy: 0.5833\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3809 - accuracy: 0.8542\n",
      "Epoch 206: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.3809 - accuracy: 0.8542 - val_loss: 0.8671 - val_accuracy: 0.5000\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3806 - accuracy: 0.8542\n",
      "Epoch 207: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.3806 - accuracy: 0.8542 - val_loss: 0.8693 - val_accuracy: 0.5833\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3802 - accuracy: 0.8542\n",
      "Epoch 208: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.3802 - accuracy: 0.8542 - val_loss: 0.8685 - val_accuracy: 0.5000\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3798 - accuracy: 0.8750\n",
      "Epoch 209: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3798 - accuracy: 0.8750 - val_loss: 0.8706 - val_accuracy: 0.5833\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3795 - accuracy: 0.8542\n",
      "Epoch 210: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3795 - accuracy: 0.8542 - val_loss: 0.8706 - val_accuracy: 0.5000\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3791 - accuracy: 0.8750\n",
      "Epoch 211: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.3791 - accuracy: 0.8750 - val_loss: 0.8724 - val_accuracy: 0.5000\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3787 - accuracy: 0.8542\n",
      "Epoch 212: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.3787 - accuracy: 0.8542 - val_loss: 0.8723 - val_accuracy: 0.5000\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3784 - accuracy: 0.8750\n",
      "Epoch 213: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.3784 - accuracy: 0.8750 - val_loss: 0.8740 - val_accuracy: 0.5000\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3780 - accuracy: 0.8542\n",
      "Epoch 214: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.3780 - accuracy: 0.8542 - val_loss: 0.8738 - val_accuracy: 0.5000\n",
      "Epoch 215/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3777 - accuracy: 0.8750\n",
      "Epoch 215: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.3777 - accuracy: 0.8750 - val_loss: 0.8758 - val_accuracy: 0.5000\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3773 - accuracy: 0.8542\n",
      "Epoch 216: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.3773 - accuracy: 0.8542 - val_loss: 0.8755 - val_accuracy: 0.5000\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3770 - accuracy: 0.8750\n",
      "Epoch 217: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.3770 - accuracy: 0.8750 - val_loss: 0.8778 - val_accuracy: 0.5000\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3766 - accuracy: 0.8542\n",
      "Epoch 218: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.3766 - accuracy: 0.8542 - val_loss: 0.8770 - val_accuracy: 0.5000\n",
      "Epoch 219/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3763 - accuracy: 0.8750\n",
      "Epoch 219: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.3763 - accuracy: 0.8750 - val_loss: 0.8793 - val_accuracy: 0.5000\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3759 - accuracy: 0.8542\n",
      "Epoch 220: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.3759 - accuracy: 0.8542 - val_loss: 0.8787 - val_accuracy: 0.5000\n",
      "Epoch 221/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3755 - accuracy: 0.8750\n",
      "Epoch 221: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3755 - accuracy: 0.8750 - val_loss: 0.8806 - val_accuracy: 0.5000\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3752 - accuracy: 0.8542\n",
      "Epoch 222: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.3752 - accuracy: 0.8542 - val_loss: 0.8802 - val_accuracy: 0.5000\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3749 - accuracy: 0.8750\n",
      "Epoch 223: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.3749 - accuracy: 0.8750 - val_loss: 0.8823 - val_accuracy: 0.5000\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3745 - accuracy: 0.8542\n",
      "Epoch 224: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.3745 - accuracy: 0.8542 - val_loss: 0.8816 - val_accuracy: 0.5000\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3742 - accuracy: 0.8750\n",
      "Epoch 225: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3742 - accuracy: 0.8750 - val_loss: 0.8839 - val_accuracy: 0.5000\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3739 - accuracy: 0.8542\n",
      "Epoch 226: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.3739 - accuracy: 0.8542 - val_loss: 0.8830 - val_accuracy: 0.5000\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3735 - accuracy: 0.8750\n",
      "Epoch 227: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.3735 - accuracy: 0.8750 - val_loss: 0.8856 - val_accuracy: 0.5000\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3732 - accuracy: 0.8542\n",
      "Epoch 228: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.3732 - accuracy: 0.8542 - val_loss: 0.8846 - val_accuracy: 0.5000\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3729 - accuracy: 0.8750\n",
      "Epoch 229: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.3729 - accuracy: 0.8750 - val_loss: 0.8867 - val_accuracy: 0.5000\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3726 - accuracy: 0.8542\n",
      "Epoch 230: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.3726 - accuracy: 0.8542 - val_loss: 0.8860 - val_accuracy: 0.5000\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3722 - accuracy: 0.8750\n",
      "Epoch 231: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.3722 - accuracy: 0.8750 - val_loss: 0.8883 - val_accuracy: 0.5000\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3719 - accuracy: 0.8542\n",
      "Epoch 232: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3719 - accuracy: 0.8542 - val_loss: 0.8877 - val_accuracy: 0.5000\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3716 - accuracy: 0.8750\n",
      "Epoch 233: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.3716 - accuracy: 0.8750 - val_loss: 0.8898 - val_accuracy: 0.5000\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3713 - accuracy: 0.8542\n",
      "Epoch 234: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.3713 - accuracy: 0.8542 - val_loss: 0.8894 - val_accuracy: 0.5000\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3710 - accuracy: 0.8750\n",
      "Epoch 235: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.3710 - accuracy: 0.8750 - val_loss: 0.8915 - val_accuracy: 0.5000\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3706 - accuracy: 0.8542\n",
      "Epoch 236: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3706 - accuracy: 0.8542 - val_loss: 0.8908 - val_accuracy: 0.5000\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3703 - accuracy: 0.8750\n",
      "Epoch 237: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3703 - accuracy: 0.8750 - val_loss: 0.8926 - val_accuracy: 0.5000\n",
      "Epoch 238/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3700 - accuracy: 0.8542\n",
      "Epoch 238: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3700 - accuracy: 0.8542 - val_loss: 0.8922 - val_accuracy: 0.5000\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3697 - accuracy: 0.8750\n",
      "Epoch 239: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.3697 - accuracy: 0.8750 - val_loss: 0.8940 - val_accuracy: 0.5000\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3694 - accuracy: 0.8542\n",
      "Epoch 240: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3694 - accuracy: 0.8542 - val_loss: 0.8936 - val_accuracy: 0.5000\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3691 - accuracy: 0.8750\n",
      "Epoch 241: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.3691 - accuracy: 0.8750 - val_loss: 0.8950 - val_accuracy: 0.5000\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3688 - accuracy: 0.8542\n",
      "Epoch 242: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3688 - accuracy: 0.8542 - val_loss: 0.8948 - val_accuracy: 0.5000\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3685 - accuracy: 0.8750\n",
      "Epoch 243: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.3685 - accuracy: 0.8750 - val_loss: 0.8966 - val_accuracy: 0.5000\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3682 - accuracy: 0.8542\n",
      "Epoch 244: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.3682 - accuracy: 0.8542 - val_loss: 0.8964 - val_accuracy: 0.5000\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3679 - accuracy: 0.8750\n",
      "Epoch 245: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.3679 - accuracy: 0.8750 - val_loss: 0.8979 - val_accuracy: 0.5000\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3676 - accuracy: 0.8542\n",
      "Epoch 246: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.3676 - accuracy: 0.8542 - val_loss: 0.8974 - val_accuracy: 0.5000\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3673 - accuracy: 0.8750\n",
      "Epoch 247: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.3673 - accuracy: 0.8750 - val_loss: 0.8993 - val_accuracy: 0.5000\n",
      "Epoch 248/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3670 - accuracy: 0.8542\n",
      "Epoch 248: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3670 - accuracy: 0.8542 - val_loss: 0.8989 - val_accuracy: 0.5000\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3667 - accuracy: 0.8750\n",
      "Epoch 249: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3667 - accuracy: 0.8750 - val_loss: 0.9003 - val_accuracy: 0.5000\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3664 - accuracy: 0.8542\n",
      "Epoch 250: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.3664 - accuracy: 0.8542 - val_loss: 0.9001 - val_accuracy: 0.5000\n",
      "Epoch 251/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3661 - accuracy: 0.8750\n",
      "Epoch 251: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.3661 - accuracy: 0.8750 - val_loss: 0.9018 - val_accuracy: 0.5000\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3658 - accuracy: 0.8750\n",
      "Epoch 252: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.3658 - accuracy: 0.8750 - val_loss: 0.9016 - val_accuracy: 0.5000\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3655 - accuracy: 0.8750\n",
      "Epoch 253: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.3655 - accuracy: 0.8750 - val_loss: 0.9030 - val_accuracy: 0.5000\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3652 - accuracy: 0.8750\n",
      "Epoch 254: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.3652 - accuracy: 0.8750 - val_loss: 0.9026 - val_accuracy: 0.5000\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3649 - accuracy: 0.8750\n",
      "Epoch 255: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3649 - accuracy: 0.8750 - val_loss: 0.9043 - val_accuracy: 0.5000\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3646 - accuracy: 0.8750\n",
      "Epoch 256: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.3646 - accuracy: 0.8750 - val_loss: 0.9039 - val_accuracy: 0.5000\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3644 - accuracy: 0.8750\n",
      "Epoch 257: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.3644 - accuracy: 0.8750 - val_loss: 0.9055 - val_accuracy: 0.5000\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3641 - accuracy: 0.8542\n",
      "Epoch 258: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.3641 - accuracy: 0.8542 - val_loss: 0.9047 - val_accuracy: 0.5000\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3638 - accuracy: 0.8750\n",
      "Epoch 259: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3638 - accuracy: 0.8750 - val_loss: 0.9066 - val_accuracy: 0.5000\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3635 - accuracy: 0.8750\n",
      "Epoch 260: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.3635 - accuracy: 0.8750 - val_loss: 0.9060 - val_accuracy: 0.5000\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3632 - accuracy: 0.8750\n",
      "Epoch 261: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.3632 - accuracy: 0.8750 - val_loss: 0.9079 - val_accuracy: 0.5000\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3629 - accuracy: 0.8750\n",
      "Epoch 262: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.3629 - accuracy: 0.8750 - val_loss: 0.9070 - val_accuracy: 0.5000\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3626 - accuracy: 0.8750\n",
      "Epoch 263: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3626 - accuracy: 0.8750 - val_loss: 0.9091 - val_accuracy: 0.5000\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3624 - accuracy: 0.8750\n",
      "Epoch 264: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.3624 - accuracy: 0.8750 - val_loss: 0.9081 - val_accuracy: 0.5000\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3621 - accuracy: 0.8750\n",
      "Epoch 265: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.3621 - accuracy: 0.8750 - val_loss: 0.9099 - val_accuracy: 0.5000\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3618 - accuracy: 0.8750\n",
      "Epoch 266: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.3618 - accuracy: 0.8750 - val_loss: 0.9094 - val_accuracy: 0.5000\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3615 - accuracy: 0.8750\n",
      "Epoch 267: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.3615 - accuracy: 0.8750 - val_loss: 0.9114 - val_accuracy: 0.5000\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3613 - accuracy: 0.8750\n",
      "Epoch 268: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.3613 - accuracy: 0.8750 - val_loss: 0.9104 - val_accuracy: 0.5000\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3610 - accuracy: 0.8750\n",
      "Epoch 269: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.3610 - accuracy: 0.8750 - val_loss: 0.9125 - val_accuracy: 0.5000\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3607 - accuracy: 0.8750\n",
      "Epoch 270: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.3607 - accuracy: 0.8750 - val_loss: 0.9114 - val_accuracy: 0.5000\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3605 - accuracy: 0.8750\n",
      "Epoch 271: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.3605 - accuracy: 0.8750 - val_loss: 0.9138 - val_accuracy: 0.5000\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.8750\n",
      "Epoch 272: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3602 - accuracy: 0.8750 - val_loss: 0.9128 - val_accuracy: 0.5000\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3599 - accuracy: 0.8750\n",
      "Epoch 273: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.3599 - accuracy: 0.8750 - val_loss: 0.9147 - val_accuracy: 0.5000\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3596 - accuracy: 0.8750\n",
      "Epoch 274: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.3596 - accuracy: 0.8750 - val_loss: 0.9138 - val_accuracy: 0.5000\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3594 - accuracy: 0.8750\n",
      "Epoch 275: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.3594 - accuracy: 0.8750 - val_loss: 0.9162 - val_accuracy: 0.5000\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3591 - accuracy: 0.8750\n",
      "Epoch 276: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.3591 - accuracy: 0.8750 - val_loss: 0.9151 - val_accuracy: 0.5000\n",
      "Epoch 277/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3588 - accuracy: 0.8750\n",
      "Epoch 277: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3588 - accuracy: 0.8750 - val_loss: 0.9175 - val_accuracy: 0.5000\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3586 - accuracy: 0.8750\n",
      "Epoch 278: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.3586 - accuracy: 0.8750 - val_loss: 0.9163 - val_accuracy: 0.5000\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3583 - accuracy: 0.8750\n",
      "Epoch 279: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3583 - accuracy: 0.8750 - val_loss: 0.9186 - val_accuracy: 0.5000\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3580 - accuracy: 0.8750\n",
      "Epoch 280: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.3580 - accuracy: 0.8750 - val_loss: 0.9176 - val_accuracy: 0.5000\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3578 - accuracy: 0.8750\n",
      "Epoch 281: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.3578 - accuracy: 0.8750 - val_loss: 0.9198 - val_accuracy: 0.5000\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3575 - accuracy: 0.8750\n",
      "Epoch 282: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.3575 - accuracy: 0.8750 - val_loss: 0.9187 - val_accuracy: 0.5000\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3573 - accuracy: 0.8750\n",
      "Epoch 283: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.3573 - accuracy: 0.8750 - val_loss: 0.9211 - val_accuracy: 0.5000\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3570 - accuracy: 0.8750\n",
      "Epoch 284: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3570 - accuracy: 0.8750 - val_loss: 0.9196 - val_accuracy: 0.5000\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3567 - accuracy: 0.8750\n",
      "Epoch 285: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3567 - accuracy: 0.8750 - val_loss: 0.9222 - val_accuracy: 0.5000\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3565 - accuracy: 0.8750\n",
      "Epoch 286: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.3565 - accuracy: 0.8750 - val_loss: 0.9205 - val_accuracy: 0.5000\n",
      "Epoch 287/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3562 - accuracy: 0.8750\n",
      "Epoch 287: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.3562 - accuracy: 0.8750 - val_loss: 0.9235 - val_accuracy: 0.5000\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3559 - accuracy: 0.8750\n",
      "Epoch 288: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.3559 - accuracy: 0.8750 - val_loss: 0.9219 - val_accuracy: 0.5000\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3557 - accuracy: 0.8750\n",
      "Epoch 289: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.3557 - accuracy: 0.8750 - val_loss: 0.9246 - val_accuracy: 0.5000\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3554 - accuracy: 0.8750\n",
      "Epoch 290: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.3554 - accuracy: 0.8750 - val_loss: 0.9232 - val_accuracy: 0.5000\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3551 - accuracy: 0.8750\n",
      "Epoch 291: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3551 - accuracy: 0.8750 - val_loss: 0.9259 - val_accuracy: 0.5000\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3549 - accuracy: 0.8750\n",
      "Epoch 292: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3549 - accuracy: 0.8750 - val_loss: 0.9244 - val_accuracy: 0.5000\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3546 - accuracy: 0.8750\n",
      "Epoch 293: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.3546 - accuracy: 0.8750 - val_loss: 0.9272 - val_accuracy: 0.5000\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3544 - accuracy: 0.8750\n",
      "Epoch 294: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.3544 - accuracy: 0.8750 - val_loss: 0.9253 - val_accuracy: 0.5000\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3541 - accuracy: 0.8750\n",
      "Epoch 295: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3541 - accuracy: 0.8750 - val_loss: 0.9281 - val_accuracy: 0.5000\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3539 - accuracy: 0.8750\n",
      "Epoch 296: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3539 - accuracy: 0.8750 - val_loss: 0.9265 - val_accuracy: 0.5000\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3536 - accuracy: 0.8750\n",
      "Epoch 297: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.3536 - accuracy: 0.8750 - val_loss: 0.9292 - val_accuracy: 0.5000\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3534 - accuracy: 0.8750\n",
      "Epoch 298: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.3534 - accuracy: 0.8750 - val_loss: 0.9278 - val_accuracy: 0.5000\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3531 - accuracy: 0.8750\n",
      "Epoch 299: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.3531 - accuracy: 0.8750 - val_loss: 0.9302 - val_accuracy: 0.5000\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3529 - accuracy: 0.8750\n",
      "Epoch 300: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3529 - accuracy: 0.8750 - val_loss: 0.9285 - val_accuracy: 0.5000\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3526 - accuracy: 0.8750\n",
      "Epoch 301: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.3526 - accuracy: 0.8750 - val_loss: 0.9317 - val_accuracy: 0.5000\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3524 - accuracy: 0.8750\n",
      "Epoch 302: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.3524 - accuracy: 0.8750 - val_loss: 0.9302 - val_accuracy: 0.5000\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3521 - accuracy: 0.8750\n",
      "Epoch 303: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.3521 - accuracy: 0.8750 - val_loss: 0.9328 - val_accuracy: 0.5000\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3519 - accuracy: 0.8750\n",
      "Epoch 304: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.3519 - accuracy: 0.8750 - val_loss: 0.9313 - val_accuracy: 0.5000\n",
      "Epoch 305/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3516 - accuracy: 0.8750\n",
      "Epoch 305: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.3516 - accuracy: 0.8750 - val_loss: 0.9338 - val_accuracy: 0.5000\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3514 - accuracy: 0.8750\n",
      "Epoch 306: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.3514 - accuracy: 0.8750 - val_loss: 0.9324 - val_accuracy: 0.5000\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3511 - accuracy: 0.8750\n",
      "Epoch 307: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.3511 - accuracy: 0.8750 - val_loss: 0.9349 - val_accuracy: 0.5000\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3509 - accuracy: 0.8750\n",
      "Epoch 308: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3509 - accuracy: 0.8750 - val_loss: 0.9336 - val_accuracy: 0.5000\n",
      "Epoch 309/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 0.8750\n",
      "Epoch 309: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3506 - accuracy: 0.8750 - val_loss: 0.9359 - val_accuracy: 0.5000\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3504 - accuracy: 0.8750\n",
      "Epoch 310: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.3504 - accuracy: 0.8750 - val_loss: 0.9348 - val_accuracy: 0.5000\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3502 - accuracy: 0.8750\n",
      "Epoch 311: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.3502 - accuracy: 0.8750 - val_loss: 0.9372 - val_accuracy: 0.5000\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3499 - accuracy: 0.8750\n",
      "Epoch 312: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.3499 - accuracy: 0.8750 - val_loss: 0.9360 - val_accuracy: 0.5000\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3497 - accuracy: 0.8750\n",
      "Epoch 313: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3497 - accuracy: 0.8750 - val_loss: 0.9381 - val_accuracy: 0.5000\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3494 - accuracy: 0.8750\n",
      "Epoch 314: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3494 - accuracy: 0.8750 - val_loss: 0.9371 - val_accuracy: 0.5000\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3492 - accuracy: 0.8750\n",
      "Epoch 315: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.3492 - accuracy: 0.8750 - val_loss: 0.9395 - val_accuracy: 0.5000\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3490 - accuracy: 0.8750\n",
      "Epoch 316: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.3490 - accuracy: 0.8750 - val_loss: 0.9381 - val_accuracy: 0.5000\n",
      "Epoch 317/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3487 - accuracy: 0.8750\n",
      "Epoch 317: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3487 - accuracy: 0.8750 - val_loss: 0.9409 - val_accuracy: 0.5000\n",
      "Epoch 318/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3485 - accuracy: 0.8750\n",
      "Epoch 318: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.3485 - accuracy: 0.8750 - val_loss: 0.9394 - val_accuracy: 0.5000\n",
      "Epoch 319/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3483 - accuracy: 0.8750\n",
      "Epoch 319: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.3483 - accuracy: 0.8750 - val_loss: 0.9418 - val_accuracy: 0.5000\n",
      "Epoch 320/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3480 - accuracy: 0.8750\n",
      "Epoch 320: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.3480 - accuracy: 0.8750 - val_loss: 0.9406 - val_accuracy: 0.5000\n",
      "Epoch 321/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3478 - accuracy: 0.8750\n",
      "Epoch 321: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.3478 - accuracy: 0.8750 - val_loss: 0.9429 - val_accuracy: 0.5000\n",
      "Epoch 322/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3475 - accuracy: 0.8750\n",
      "Epoch 322: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3475 - accuracy: 0.8750 - val_loss: 0.9415 - val_accuracy: 0.5000\n",
      "Epoch 323/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3473 - accuracy: 0.8750\n",
      "Epoch 323: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.3473 - accuracy: 0.8750 - val_loss: 0.9444 - val_accuracy: 0.5000\n",
      "Epoch 324/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3471 - accuracy: 0.8750\n",
      "Epoch 324: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.3471 - accuracy: 0.8750 - val_loss: 0.9427 - val_accuracy: 0.5000\n",
      "Epoch 325/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.8750\n",
      "Epoch 325: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3468 - accuracy: 0.8750 - val_loss: 0.9454 - val_accuracy: 0.5000\n",
      "Epoch 326/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3466 - accuracy: 0.8750\n",
      "Epoch 326: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3466 - accuracy: 0.8750 - val_loss: 0.9438 - val_accuracy: 0.5000\n",
      "Epoch 327/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3464 - accuracy: 0.8750\n",
      "Epoch 327: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.3464 - accuracy: 0.8750 - val_loss: 0.9463 - val_accuracy: 0.5000\n",
      "Epoch 328/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3461 - accuracy: 0.8750\n",
      "Epoch 328: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.3461 - accuracy: 0.8750 - val_loss: 0.9449 - val_accuracy: 0.5000\n",
      "Epoch 329/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3459 - accuracy: 0.8750\n",
      "Epoch 329: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.3459 - accuracy: 0.8750 - val_loss: 0.9478 - val_accuracy: 0.5000\n",
      "Epoch 330/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3457 - accuracy: 0.8750\n",
      "Epoch 330: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.3457 - accuracy: 0.8750 - val_loss: 0.9463 - val_accuracy: 0.5000\n",
      "Epoch 331/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3454 - accuracy: 0.8750\n",
      "Epoch 331: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.3454 - accuracy: 0.8750 - val_loss: 0.9491 - val_accuracy: 0.5000\n",
      "Epoch 332/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3452 - accuracy: 0.8750\n",
      "Epoch 332: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3452 - accuracy: 0.8750 - val_loss: 0.9478 - val_accuracy: 0.5000\n",
      "Epoch 333/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3450 - accuracy: 0.8750\n",
      "Epoch 333: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.3450 - accuracy: 0.8750 - val_loss: 0.9502 - val_accuracy: 0.5000\n",
      "Epoch 334/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3448 - accuracy: 0.8750\n",
      "Epoch 334: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.3448 - accuracy: 0.8750 - val_loss: 0.9483 - val_accuracy: 0.5000\n",
      "Epoch 335/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3445 - accuracy: 0.8750\n",
      "Epoch 335: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.3445 - accuracy: 0.8750 - val_loss: 0.9512 - val_accuracy: 0.5000\n",
      "Epoch 336/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3443 - accuracy: 0.8750\n",
      "Epoch 336: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.3443 - accuracy: 0.8750 - val_loss: 0.9497 - val_accuracy: 0.5000\n",
      "Epoch 337/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.8750\n",
      "Epoch 337: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3441 - accuracy: 0.8750 - val_loss: 0.9523 - val_accuracy: 0.5000\n",
      "Epoch 338/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3439 - accuracy: 0.8750\n",
      "Epoch 338: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.3439 - accuracy: 0.8750 - val_loss: 0.9506 - val_accuracy: 0.5000\n",
      "Epoch 339/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3436 - accuracy: 0.8750\n",
      "Epoch 339: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.3436 - accuracy: 0.8750 - val_loss: 0.9533 - val_accuracy: 0.5000\n",
      "Epoch 340/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3434 - accuracy: 0.8750\n",
      "Epoch 340: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3434 - accuracy: 0.8750 - val_loss: 0.9522 - val_accuracy: 0.5000\n",
      "Epoch 341/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3432 - accuracy: 0.8750\n",
      "Epoch 341: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.3432 - accuracy: 0.8750 - val_loss: 0.9542 - val_accuracy: 0.5000\n",
      "Epoch 342/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3430 - accuracy: 0.8750\n",
      "Epoch 342: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 0.3430 - accuracy: 0.8750 - val_loss: 0.9531 - val_accuracy: 0.5000\n",
      "Epoch 343/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3427 - accuracy: 0.8750\n",
      "Epoch 343: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3427 - accuracy: 0.8750 - val_loss: 0.9554 - val_accuracy: 0.5000\n",
      "Epoch 344/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3425 - accuracy: 0.8750\n",
      "Epoch 344: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3425 - accuracy: 0.8750 - val_loss: 0.9541 - val_accuracy: 0.5000\n",
      "Epoch 345/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3423 - accuracy: 0.8750\n",
      "Epoch 345: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3423 - accuracy: 0.8750 - val_loss: 0.9569 - val_accuracy: 0.5000\n",
      "Epoch 346/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3421 - accuracy: 0.8750\n",
      "Epoch 346: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3421 - accuracy: 0.8750 - val_loss: 0.9554 - val_accuracy: 0.5000\n",
      "Epoch 347/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3418 - accuracy: 0.8750\n",
      "Epoch 347: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.3418 - accuracy: 0.8750 - val_loss: 0.9580 - val_accuracy: 0.5000\n",
      "Epoch 348/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3417 - accuracy: 0.8750\n",
      "Epoch 348: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3417 - accuracy: 0.8750 - val_loss: 0.9566 - val_accuracy: 0.5000\n",
      "Epoch 349/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3414 - accuracy: 0.8750\n",
      "Epoch 349: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3414 - accuracy: 0.8750 - val_loss: 0.9585 - val_accuracy: 0.5000\n",
      "Epoch 350/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3412 - accuracy: 0.8750\n",
      "Epoch 350: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.3412 - accuracy: 0.8750 - val_loss: 0.9577 - val_accuracy: 0.5000\n",
      "Epoch 351/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3410 - accuracy: 0.8750\n",
      "Epoch 351: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.3410 - accuracy: 0.8750 - val_loss: 0.9599 - val_accuracy: 0.5000\n",
      "Epoch 352/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3408 - accuracy: 0.8750\n",
      "Epoch 352: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.3408 - accuracy: 0.8750 - val_loss: 0.9588 - val_accuracy: 0.5000\n",
      "Epoch 353/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3406 - accuracy: 0.8750\n",
      "Epoch 353: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3406 - accuracy: 0.8750 - val_loss: 0.9614 - val_accuracy: 0.5000\n",
      "Epoch 354/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3404 - accuracy: 0.8750\n",
      "Epoch 354: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3404 - accuracy: 0.8750 - val_loss: 0.9597 - val_accuracy: 0.5000\n",
      "Epoch 355/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3402 - accuracy: 0.8750\n",
      "Epoch 355: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3402 - accuracy: 0.8750 - val_loss: 0.9621 - val_accuracy: 0.5000\n",
      "Epoch 356/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3400 - accuracy: 0.8750\n",
      "Epoch 356: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.3400 - accuracy: 0.8750 - val_loss: 0.9607 - val_accuracy: 0.5000\n",
      "Epoch 357/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3397 - accuracy: 0.8750\n",
      "Epoch 357: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.3397 - accuracy: 0.8750 - val_loss: 0.9631 - val_accuracy: 0.5000\n",
      "Epoch 358/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3395 - accuracy: 0.8750\n",
      "Epoch 358: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.3395 - accuracy: 0.8750 - val_loss: 0.9619 - val_accuracy: 0.5000\n",
      "Epoch 359/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3393 - accuracy: 0.8750\n",
      "Epoch 359: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.3393 - accuracy: 0.8750 - val_loss: 0.9643 - val_accuracy: 0.5000\n",
      "Epoch 360/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3391 - accuracy: 0.8750\n",
      "Epoch 360: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3391 - accuracy: 0.8750 - val_loss: 0.9629 - val_accuracy: 0.5000\n",
      "Epoch 361/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3389 - accuracy: 0.8750\n",
      "Epoch 361: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.3389 - accuracy: 0.8750 - val_loss: 0.9652 - val_accuracy: 0.5000\n",
      "Epoch 362/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3387 - accuracy: 0.8750\n",
      "Epoch 362: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.3387 - accuracy: 0.8750 - val_loss: 0.9640 - val_accuracy: 0.5000\n",
      "Epoch 363/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3385 - accuracy: 0.8750\n",
      "Epoch 363: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3385 - accuracy: 0.8750 - val_loss: 0.9664 - val_accuracy: 0.5000\n",
      "Epoch 364/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3383 - accuracy: 0.8750\n",
      "Epoch 364: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.3383 - accuracy: 0.8750 - val_loss: 0.9648 - val_accuracy: 0.5000\n",
      "Epoch 365/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3381 - accuracy: 0.8750\n",
      "Epoch 365: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3381 - accuracy: 0.8750 - val_loss: 0.9674 - val_accuracy: 0.5000\n",
      "Epoch 366/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3379 - accuracy: 0.8750\n",
      "Epoch 366: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.3379 - accuracy: 0.8750 - val_loss: 0.9661 - val_accuracy: 0.5000\n",
      "Epoch 367/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3377 - accuracy: 0.8750\n",
      "Epoch 367: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3377 - accuracy: 0.8750 - val_loss: 0.9682 - val_accuracy: 0.5000\n",
      "Epoch 368/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3374 - accuracy: 0.8750\n",
      "Epoch 368: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.3374 - accuracy: 0.8750 - val_loss: 0.9669 - val_accuracy: 0.5000\n",
      "Epoch 369/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3373 - accuracy: 0.8750\n",
      "Epoch 369: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.3373 - accuracy: 0.8750 - val_loss: 0.9694 - val_accuracy: 0.5000\n",
      "Epoch 370/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3371 - accuracy: 0.8750\n",
      "Epoch 370: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3371 - accuracy: 0.8750 - val_loss: 0.9679 - val_accuracy: 0.5000\n",
      "Epoch 371/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3368 - accuracy: 0.8750\n",
      "Epoch 371: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3368 - accuracy: 0.8750 - val_loss: 0.9707 - val_accuracy: 0.5000\n",
      "Epoch 372/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3367 - accuracy: 0.8750\n",
      "Epoch 372: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.3367 - accuracy: 0.8750 - val_loss: 0.9688 - val_accuracy: 0.5000\n",
      "Epoch 373/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3364 - accuracy: 0.8750\n",
      "Epoch 373: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.3364 - accuracy: 0.8750 - val_loss: 0.9717 - val_accuracy: 0.5000\n",
      "Epoch 374/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3362 - accuracy: 0.8750\n",
      "Epoch 374: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.3362 - accuracy: 0.8750 - val_loss: 0.9698 - val_accuracy: 0.5000\n",
      "Epoch 375/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3360 - accuracy: 0.8750\n",
      "Epoch 375: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3360 - accuracy: 0.8750 - val_loss: 0.9723 - val_accuracy: 0.5000\n",
      "Epoch 376/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3358 - accuracy: 0.8750\n",
      "Epoch 376: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.3358 - accuracy: 0.8750 - val_loss: 0.9711 - val_accuracy: 0.5000\n",
      "Epoch 377/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3356 - accuracy: 0.8750\n",
      "Epoch 377: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.3356 - accuracy: 0.8750 - val_loss: 0.9738 - val_accuracy: 0.5000\n",
      "Epoch 378/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3355 - accuracy: 0.8750\n",
      "Epoch 378: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.3355 - accuracy: 0.8750 - val_loss: 0.9720 - val_accuracy: 0.5000\n",
      "Epoch 379/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3352 - accuracy: 0.8750\n",
      "Epoch 379: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.3352 - accuracy: 0.8750 - val_loss: 0.9749 - val_accuracy: 0.5000\n",
      "Epoch 380/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3351 - accuracy: 0.8750\n",
      "Epoch 380: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.3351 - accuracy: 0.8750 - val_loss: 0.9729 - val_accuracy: 0.5000\n",
      "Epoch 381/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3348 - accuracy: 0.8750\n",
      "Epoch 381: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.3348 - accuracy: 0.8750 - val_loss: 0.9755 - val_accuracy: 0.5000\n",
      "Epoch 382/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3346 - accuracy: 0.8750\n",
      "Epoch 382: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.3346 - accuracy: 0.8750 - val_loss: 0.9742 - val_accuracy: 0.5000\n",
      "Epoch 383/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3344 - accuracy: 0.8750\n",
      "Epoch 383: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.3344 - accuracy: 0.8750 - val_loss: 0.9766 - val_accuracy: 0.5000\n",
      "Epoch 384/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3342 - accuracy: 0.8750\n",
      "Epoch 384: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.3342 - accuracy: 0.8750 - val_loss: 0.9752 - val_accuracy: 0.5000\n",
      "Epoch 385/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3340 - accuracy: 0.8750\n",
      "Epoch 385: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.3340 - accuracy: 0.8750 - val_loss: 0.9780 - val_accuracy: 0.5000\n",
      "Epoch 386/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3339 - accuracy: 0.8750\n",
      "Epoch 386: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.3339 - accuracy: 0.8750 - val_loss: 0.9760 - val_accuracy: 0.5000\n",
      "Epoch 387/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3336 - accuracy: 0.8750\n",
      "Epoch 387: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.3336 - accuracy: 0.8750 - val_loss: 0.9790 - val_accuracy: 0.5000\n",
      "Epoch 388/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3335 - accuracy: 0.8750\n",
      "Epoch 388: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.3335 - accuracy: 0.8750 - val_loss: 0.9770 - val_accuracy: 0.5000\n",
      "Epoch 389/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3332 - accuracy: 0.8750\n",
      "Epoch 389: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.3332 - accuracy: 0.8750 - val_loss: 0.9799 - val_accuracy: 0.5000\n",
      "Epoch 390/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3330 - accuracy: 0.8750\n",
      "Epoch 390: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3330 - accuracy: 0.8750 - val_loss: 0.9780 - val_accuracy: 0.5000\n",
      "Epoch 391/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3329 - accuracy: 0.8750\n",
      "Epoch 391: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.3329 - accuracy: 0.8750 - val_loss: 0.9812 - val_accuracy: 0.5000\n",
      "Epoch 392/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3327 - accuracy: 0.8750\n",
      "Epoch 392: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.3327 - accuracy: 0.8750 - val_loss: 0.9790 - val_accuracy: 0.5000\n",
      "Epoch 393/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3325 - accuracy: 0.8750\n",
      "Epoch 393: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.3325 - accuracy: 0.8750 - val_loss: 0.9817 - val_accuracy: 0.5000\n",
      "Epoch 394/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3323 - accuracy: 0.8750\n",
      "Epoch 394: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.3323 - accuracy: 0.8750 - val_loss: 0.9803 - val_accuracy: 0.5000\n",
      "Epoch 395/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3320 - accuracy: 0.8750\n",
      "Epoch 395: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.3320 - accuracy: 0.8750 - val_loss: 0.9827 - val_accuracy: 0.5000\n",
      "Epoch 396/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3319 - accuracy: 0.8750\n",
      "Epoch 396: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.3319 - accuracy: 0.8750 - val_loss: 0.9810 - val_accuracy: 0.5000\n",
      "Epoch 397/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3316 - accuracy: 0.8750\n",
      "Epoch 397: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.3316 - accuracy: 0.8750 - val_loss: 0.9840 - val_accuracy: 0.5000\n",
      "Epoch 398/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3314 - accuracy: 0.8750\n",
      "Epoch 398: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.3314 - accuracy: 0.8750 - val_loss: 0.9821 - val_accuracy: 0.5000\n",
      "Epoch 399/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3312 - accuracy: 0.8750\n",
      "Epoch 399: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.3312 - accuracy: 0.8750 - val_loss: 0.9850 - val_accuracy: 0.5000\n",
      "Epoch 400/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3310 - accuracy: 0.8750\n",
      "Epoch 400: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3310 - accuracy: 0.8750 - val_loss: 0.9832 - val_accuracy: 0.5000\n",
      "Epoch 401/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3308 - accuracy: 0.8750\n",
      "Epoch 401: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3308 - accuracy: 0.8750 - val_loss: 0.9861 - val_accuracy: 0.5000\n",
      "Epoch 402/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3307 - accuracy: 0.8750\n",
      "Epoch 402: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.3307 - accuracy: 0.8750 - val_loss: 0.9843 - val_accuracy: 0.5000\n",
      "Epoch 403/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3305 - accuracy: 0.8750\n",
      "Epoch 403: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.3305 - accuracy: 0.8750 - val_loss: 0.9870 - val_accuracy: 0.5000\n",
      "Epoch 404/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.8750\n",
      "Epoch 404: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.3303 - accuracy: 0.8750 - val_loss: 0.9851 - val_accuracy: 0.5000\n",
      "Epoch 405/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3301 - accuracy: 0.8750\n",
      "Epoch 405: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.3301 - accuracy: 0.8750 - val_loss: 0.9879 - val_accuracy: 0.5000\n",
      "Epoch 406/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3299 - accuracy: 0.8750\n",
      "Epoch 406: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.3299 - accuracy: 0.8750 - val_loss: 0.9859 - val_accuracy: 0.5000\n",
      "Epoch 407/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3297 - accuracy: 0.8750\n",
      "Epoch 407: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3297 - accuracy: 0.8750 - val_loss: 0.9891 - val_accuracy: 0.5000\n",
      "Epoch 408/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3295 - accuracy: 0.8750\n",
      "Epoch 408: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.3295 - accuracy: 0.8750 - val_loss: 0.9869 - val_accuracy: 0.5000\n",
      "Epoch 409/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3293 - accuracy: 0.8750\n",
      "Epoch 409: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.3293 - accuracy: 0.8750 - val_loss: 0.9899 - val_accuracy: 0.5000\n",
      "Epoch 410/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3291 - accuracy: 0.8750\n",
      "Epoch 410: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3291 - accuracy: 0.8750 - val_loss: 0.9882 - val_accuracy: 0.5000\n",
      "Epoch 411/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3289 - accuracy: 0.8750\n",
      "Epoch 411: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3289 - accuracy: 0.8750 - val_loss: 0.9910 - val_accuracy: 0.5000\n",
      "Epoch 412/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3287 - accuracy: 0.8750\n",
      "Epoch 412: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.3287 - accuracy: 0.8750 - val_loss: 0.9893 - val_accuracy: 0.5000\n",
      "Epoch 413/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3285 - accuracy: 0.8750\n",
      "Epoch 413: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.3285 - accuracy: 0.8750 - val_loss: 0.9920 - val_accuracy: 0.5000\n",
      "Epoch 414/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3283 - accuracy: 0.8750\n",
      "Epoch 414: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.3283 - accuracy: 0.8750 - val_loss: 0.9900 - val_accuracy: 0.5000\n",
      "Epoch 415/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3281 - accuracy: 0.8750\n",
      "Epoch 415: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.3281 - accuracy: 0.8750 - val_loss: 0.9933 - val_accuracy: 0.5000\n",
      "Epoch 416/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3280 - accuracy: 0.8750\n",
      "Epoch 416: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.3280 - accuracy: 0.8750 - val_loss: 0.9907 - val_accuracy: 0.5000\n",
      "Epoch 417/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3278 - accuracy: 0.8750\n",
      "Epoch 417: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.3278 - accuracy: 0.8750 - val_loss: 0.9942 - val_accuracy: 0.5000\n",
      "Epoch 418/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3276 - accuracy: 0.8750\n",
      "Epoch 418: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3276 - accuracy: 0.8750 - val_loss: 0.9919 - val_accuracy: 0.5000\n",
      "Epoch 419/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3274 - accuracy: 0.8750\n",
      "Epoch 419: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.3274 - accuracy: 0.8750 - val_loss: 0.9952 - val_accuracy: 0.5000\n",
      "Epoch 420/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3272 - accuracy: 0.8750\n",
      "Epoch 420: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.3272 - accuracy: 0.8750 - val_loss: 0.9927 - val_accuracy: 0.5000\n",
      "Epoch 421/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.8750\n",
      "Epoch 421: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.3270 - accuracy: 0.8750 - val_loss: 0.9960 - val_accuracy: 0.5000\n",
      "Epoch 422/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3268 - accuracy: 0.8750\n",
      "Epoch 422: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.3268 - accuracy: 0.8750 - val_loss: 0.9944 - val_accuracy: 0.5000\n",
      "Epoch 423/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3266 - accuracy: 0.8750\n",
      "Epoch 423: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.3266 - accuracy: 0.8750 - val_loss: 0.9970 - val_accuracy: 0.5000\n",
      "Epoch 424/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3264 - accuracy: 0.8750\n",
      "Epoch 424: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.3264 - accuracy: 0.8750 - val_loss: 0.9954 - val_accuracy: 0.5000\n",
      "Epoch 425/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3262 - accuracy: 0.8750\n",
      "Epoch 425: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.3262 - accuracy: 0.8750 - val_loss: 0.9979 - val_accuracy: 0.5000\n",
      "Epoch 426/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3260 - accuracy: 0.8750\n",
      "Epoch 426: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.3260 - accuracy: 0.8750 - val_loss: 0.9958 - val_accuracy: 0.5000\n",
      "Epoch 427/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3258 - accuracy: 0.8750\n",
      "Epoch 427: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.3258 - accuracy: 0.8750 - val_loss: 0.9991 - val_accuracy: 0.5000\n",
      "Epoch 428/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3256 - accuracy: 0.8750\n",
      "Epoch 428: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.3256 - accuracy: 0.8750 - val_loss: 0.9976 - val_accuracy: 0.5000\n",
      "Epoch 429/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3254 - accuracy: 0.8750\n",
      "Epoch 429: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 493ms/step - loss: 0.3254 - accuracy: 0.8750 - val_loss: 1.0001 - val_accuracy: 0.5000\n",
      "Epoch 430/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3253 - accuracy: 0.8750\n",
      "Epoch 430: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.3253 - accuracy: 0.8750 - val_loss: 0.9985 - val_accuracy: 0.5000\n",
      "Epoch 431/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3251 - accuracy: 0.8750\n",
      "Epoch 431: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.3251 - accuracy: 0.8750 - val_loss: 1.0014 - val_accuracy: 0.5000\n",
      "Epoch 432/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3249 - accuracy: 0.8750\n",
      "Epoch 432: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.3249 - accuracy: 0.8750 - val_loss: 0.9992 - val_accuracy: 0.5000\n",
      "Epoch 433/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3247 - accuracy: 0.8750\n",
      "Epoch 433: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.3247 - accuracy: 0.8750 - val_loss: 1.0021 - val_accuracy: 0.5000\n",
      "Epoch 434/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3245 - accuracy: 0.8750\n",
      "Epoch 434: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.3245 - accuracy: 0.8750 - val_loss: 1.0004 - val_accuracy: 0.5000\n",
      "Epoch 435/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3243 - accuracy: 0.8750\n",
      "Epoch 435: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.3243 - accuracy: 0.8750 - val_loss: 1.0035 - val_accuracy: 0.5000\n",
      "Epoch 436/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3242 - accuracy: 0.8750\n",
      "Epoch 436: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.3242 - accuracy: 0.8750 - val_loss: 1.0016 - val_accuracy: 0.5000\n",
      "Epoch 437/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3240 - accuracy: 0.8750\n",
      "Epoch 437: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.3240 - accuracy: 0.8750 - val_loss: 1.0043 - val_accuracy: 0.5000\n",
      "Epoch 438/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3238 - accuracy: 0.8750\n",
      "Epoch 438: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.3238 - accuracy: 0.8750 - val_loss: 1.0022 - val_accuracy: 0.5000\n",
      "Epoch 439/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3236 - accuracy: 0.8750\n",
      "Epoch 439: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.3236 - accuracy: 0.8750 - val_loss: 1.0056 - val_accuracy: 0.5000\n",
      "Epoch 440/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3234 - accuracy: 0.8750\n",
      "Epoch 440: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.3234 - accuracy: 0.8750 - val_loss: 1.0034 - val_accuracy: 0.5000\n",
      "Epoch 441/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3232 - accuracy: 0.8750\n",
      "Epoch 441: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.3232 - accuracy: 0.8750 - val_loss: 1.0063 - val_accuracy: 0.5000\n",
      "Epoch 442/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3231 - accuracy: 0.8750\n",
      "Epoch 442: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.3231 - accuracy: 0.8750 - val_loss: 1.0043 - val_accuracy: 0.5000\n",
      "Epoch 443/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3229 - accuracy: 0.8750\n",
      "Epoch 443: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.3229 - accuracy: 0.8750 - val_loss: 1.0074 - val_accuracy: 0.5000\n",
      "Epoch 444/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3227 - accuracy: 0.8750\n",
      "Epoch 444: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.3227 - accuracy: 0.8750 - val_loss: 1.0055 - val_accuracy: 0.5000\n",
      "Epoch 445/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3225 - accuracy: 0.8750\n",
      "Epoch 445: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.3225 - accuracy: 0.8750 - val_loss: 1.0082 - val_accuracy: 0.5000\n",
      "Epoch 446/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3223 - accuracy: 0.8750\n",
      "Epoch 446: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.3223 - accuracy: 0.8750 - val_loss: 1.0065 - val_accuracy: 0.5000\n",
      "Epoch 447/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3221 - accuracy: 0.8750\n",
      "Epoch 447: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.3221 - accuracy: 0.8750 - val_loss: 1.0094 - val_accuracy: 0.5000\n",
      "Epoch 448/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3220 - accuracy: 0.8750\n",
      "Epoch 448: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.3220 - accuracy: 0.8750 - val_loss: 1.0074 - val_accuracy: 0.5000\n",
      "Epoch 449/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3218 - accuracy: 0.8750\n",
      "Epoch 449: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.3218 - accuracy: 0.8750 - val_loss: 1.0106 - val_accuracy: 0.5000\n",
      "Epoch 450/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3216 - accuracy: 0.8750\n",
      "Epoch 450: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.3216 - accuracy: 0.8750 - val_loss: 1.0081 - val_accuracy: 0.5000\n",
      "Epoch 451/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3214 - accuracy: 0.8750\n",
      "Epoch 451: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.3214 - accuracy: 0.8750 - val_loss: 1.0115 - val_accuracy: 0.5000\n",
      "Epoch 452/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3212 - accuracy: 0.8750\n",
      "Epoch 452: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.3212 - accuracy: 0.8750 - val_loss: 1.0097 - val_accuracy: 0.5000\n",
      "Epoch 453/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3211 - accuracy: 0.8750\n",
      "Epoch 453: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.3211 - accuracy: 0.8750 - val_loss: 1.0125 - val_accuracy: 0.5000\n",
      "Epoch 454/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3209 - accuracy: 0.8750\n",
      "Epoch 454: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.3209 - accuracy: 0.8750 - val_loss: 1.0105 - val_accuracy: 0.5000\n",
      "Epoch 455/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3207 - accuracy: 0.8750\n",
      "Epoch 455: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3207 - accuracy: 0.8750 - val_loss: 1.0138 - val_accuracy: 0.5000\n",
      "Epoch 456/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3205 - accuracy: 0.8750\n",
      "Epoch 456: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.3205 - accuracy: 0.8750 - val_loss: 1.0114 - val_accuracy: 0.5000\n",
      "Epoch 457/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3204 - accuracy: 0.8750\n",
      "Epoch 457: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.3204 - accuracy: 0.8750 - val_loss: 1.0152 - val_accuracy: 0.5000\n",
      "Epoch 458/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3202 - accuracy: 0.8750\n",
      "Epoch 458: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.3202 - accuracy: 0.8750 - val_loss: 1.0125 - val_accuracy: 0.5000\n",
      "Epoch 459/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3200 - accuracy: 0.8750\n",
      "Epoch 459: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.3200 - accuracy: 0.8750 - val_loss: 1.0156 - val_accuracy: 0.5000\n",
      "Epoch 460/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3198 - accuracy: 0.8750\n",
      "Epoch 460: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.3198 - accuracy: 0.8750 - val_loss: 1.0136 - val_accuracy: 0.5000\n",
      "Epoch 461/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3196 - accuracy: 0.8750\n",
      "Epoch 461: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3196 - accuracy: 0.8750 - val_loss: 1.0168 - val_accuracy: 0.5000\n",
      "Epoch 462/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3194 - accuracy: 0.8750\n",
      "Epoch 462: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.3194 - accuracy: 0.8750 - val_loss: 1.0147 - val_accuracy: 0.5000\n",
      "Epoch 463/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3192 - accuracy: 0.8750\n",
      "Epoch 463: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3192 - accuracy: 0.8750 - val_loss: 1.0174 - val_accuracy: 0.5000\n",
      "Epoch 464/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3190 - accuracy: 0.8750\n",
      "Epoch 464: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.3190 - accuracy: 0.8750 - val_loss: 1.0157 - val_accuracy: 0.5000\n",
      "Epoch 465/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3188 - accuracy: 0.8750\n",
      "Epoch 465: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3188 - accuracy: 0.8750 - val_loss: 1.0185 - val_accuracy: 0.5000\n",
      "Epoch 466/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3187 - accuracy: 0.8750\n",
      "Epoch 466: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.3187 - accuracy: 0.8750 - val_loss: 1.0167 - val_accuracy: 0.5000\n",
      "Epoch 467/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3185 - accuracy: 0.8750\n",
      "Epoch 467: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3185 - accuracy: 0.8750 - val_loss: 1.0193 - val_accuracy: 0.5000\n",
      "Epoch 468/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3183 - accuracy: 0.8750\n",
      "Epoch 468: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.3183 - accuracy: 0.8750 - val_loss: 1.0175 - val_accuracy: 0.5000\n",
      "Epoch 469/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3181 - accuracy: 0.8750\n",
      "Epoch 469: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.3181 - accuracy: 0.8750 - val_loss: 1.0206 - val_accuracy: 0.5000\n",
      "Epoch 470/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3179 - accuracy: 0.8750\n",
      "Epoch 470: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3179 - accuracy: 0.8750 - val_loss: 1.0184 - val_accuracy: 0.5000\n",
      "Epoch 471/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3178 - accuracy: 0.8750\n",
      "Epoch 471: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.3178 - accuracy: 0.8750 - val_loss: 1.0215 - val_accuracy: 0.5000\n",
      "Epoch 472/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3176 - accuracy: 0.8750\n",
      "Epoch 472: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.3176 - accuracy: 0.8750 - val_loss: 1.0191 - val_accuracy: 0.5000\n",
      "Epoch 473/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3174 - accuracy: 0.8750\n",
      "Epoch 473: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.3174 - accuracy: 0.8750 - val_loss: 1.0223 - val_accuracy: 0.5000\n",
      "Epoch 474/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3172 - accuracy: 0.8750\n",
      "Epoch 474: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.3172 - accuracy: 0.8750 - val_loss: 1.0202 - val_accuracy: 0.5000\n",
      "Epoch 475/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3170 - accuracy: 0.8750\n",
      "Epoch 475: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3170 - accuracy: 0.8750 - val_loss: 1.0228 - val_accuracy: 0.5000\n",
      "Epoch 476/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3168 - accuracy: 0.8750\n",
      "Epoch 476: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.3168 - accuracy: 0.8750 - val_loss: 1.0210 - val_accuracy: 0.5000\n",
      "Epoch 477/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3166 - accuracy: 0.8750\n",
      "Epoch 477: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3166 - accuracy: 0.8750 - val_loss: 1.0238 - val_accuracy: 0.5000\n",
      "Epoch 478/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3165 - accuracy: 0.8750\n",
      "Epoch 478: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.3165 - accuracy: 0.8750 - val_loss: 1.0218 - val_accuracy: 0.5000\n",
      "Epoch 479/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3163 - accuracy: 0.8750\n",
      "Epoch 479: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3163 - accuracy: 0.8750 - val_loss: 1.0245 - val_accuracy: 0.5000\n",
      "Epoch 480/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3161 - accuracy: 0.8750\n",
      "Epoch 480: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.3161 - accuracy: 0.8750 - val_loss: 1.0229 - val_accuracy: 0.5000\n",
      "Epoch 481/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3159 - accuracy: 0.8750\n",
      "Epoch 481: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.3159 - accuracy: 0.8750 - val_loss: 1.0253 - val_accuracy: 0.5000\n",
      "Epoch 482/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3158 - accuracy: 0.8750\n",
      "Epoch 482: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.3158 - accuracy: 0.8750 - val_loss: 1.0232 - val_accuracy: 0.5000\n",
      "Epoch 483/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3156 - accuracy: 0.8750\n",
      "Epoch 483: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3156 - accuracy: 0.8750 - val_loss: 1.0267 - val_accuracy: 0.5000\n",
      "Epoch 484/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3155 - accuracy: 0.8958\n",
      "Epoch 484: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.3155 - accuracy: 0.8958 - val_loss: 1.0243 - val_accuracy: 0.5000\n",
      "Epoch 485/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3153 - accuracy: 0.8750\n",
      "Epoch 485: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3153 - accuracy: 0.8750 - val_loss: 1.0279 - val_accuracy: 0.5000\n",
      "Epoch 486/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3152 - accuracy: 0.8958\n",
      "Epoch 486: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3152 - accuracy: 0.8958 - val_loss: 1.0252 - val_accuracy: 0.5000\n",
      "Epoch 487/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3150 - accuracy: 0.8750\n",
      "Epoch 487: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.3150 - accuracy: 0.8750 - val_loss: 1.0285 - val_accuracy: 0.5000\n",
      "Epoch 488/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3148 - accuracy: 0.8958\n",
      "Epoch 488: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.3148 - accuracy: 0.8958 - val_loss: 1.0257 - val_accuracy: 0.5000\n",
      "Epoch 489/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3146 - accuracy: 0.8750\n",
      "Epoch 489: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.3146 - accuracy: 0.8750 - val_loss: 1.0297 - val_accuracy: 0.5000\n",
      "Epoch 490/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3145 - accuracy: 0.8958\n",
      "Epoch 490: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.3145 - accuracy: 0.8958 - val_loss: 1.0269 - val_accuracy: 0.5000\n",
      "Epoch 491/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3143 - accuracy: 0.8750\n",
      "Epoch 491: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.3143 - accuracy: 0.8750 - val_loss: 1.0306 - val_accuracy: 0.5000\n",
      "Epoch 492/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3141 - accuracy: 0.8958\n",
      "Epoch 492: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.3141 - accuracy: 0.8958 - val_loss: 1.0281 - val_accuracy: 0.5000\n",
      "Epoch 493/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3139 - accuracy: 0.8750\n",
      "Epoch 493: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3139 - accuracy: 0.8750 - val_loss: 1.0312 - val_accuracy: 0.5000\n",
      "Epoch 494/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3138 - accuracy: 0.8958\n",
      "Epoch 494: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.3138 - accuracy: 0.8958 - val_loss: 1.0289 - val_accuracy: 0.5000\n",
      "Epoch 495/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3136 - accuracy: 0.8750\n",
      "Epoch 495: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.3136 - accuracy: 0.8750 - val_loss: 1.0320 - val_accuracy: 0.5000\n",
      "Epoch 496/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3135 - accuracy: 0.8958\n",
      "Epoch 496: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.3135 - accuracy: 0.8958 - val_loss: 1.0298 - val_accuracy: 0.5000\n",
      "Epoch 497/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3133 - accuracy: 0.8750\n",
      "Epoch 497: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.3133 - accuracy: 0.8750 - val_loss: 1.0333 - val_accuracy: 0.5000\n",
      "Epoch 498/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3132 - accuracy: 0.8958\n",
      "Epoch 498: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.3132 - accuracy: 0.8958 - val_loss: 1.0305 - val_accuracy: 0.5000\n",
      "Epoch 499/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3130 - accuracy: 0.8750\n",
      "Epoch 499: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.3130 - accuracy: 0.8750 - val_loss: 1.0339 - val_accuracy: 0.5000\n",
      "Epoch 500/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3128 - accuracy: 0.8958\n",
      "Epoch 500: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.3128 - accuracy: 0.8958 - val_loss: 1.0314 - val_accuracy: 0.5000\n",
      "Epoch 501/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3126 - accuracy: 0.8750\n",
      "Epoch 501: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3126 - accuracy: 0.8750 - val_loss: 1.0349 - val_accuracy: 0.5000\n",
      "Epoch 502/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3125 - accuracy: 0.8958\n",
      "Epoch 502: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.3125 - accuracy: 0.8958 - val_loss: 1.0326 - val_accuracy: 0.5000\n",
      "Epoch 503/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3123 - accuracy: 0.8750\n",
      "Epoch 503: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.3123 - accuracy: 0.8750 - val_loss: 1.0359 - val_accuracy: 0.5000\n",
      "Epoch 504/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3121 - accuracy: 0.8958\n",
      "Epoch 504: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 0.3121 - accuracy: 0.8958 - val_loss: 1.0333 - val_accuracy: 0.5000\n",
      "Epoch 505/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3120 - accuracy: 0.8750\n",
      "Epoch 505: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3120 - accuracy: 0.8750 - val_loss: 1.0364 - val_accuracy: 0.5000\n",
      "Epoch 506/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3118 - accuracy: 0.8958\n",
      "Epoch 506: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3118 - accuracy: 0.8958 - val_loss: 1.0345 - val_accuracy: 0.5000\n",
      "Epoch 507/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3116 - accuracy: 0.8750\n",
      "Epoch 507: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.3116 - accuracy: 0.8750 - val_loss: 1.0379 - val_accuracy: 0.5000\n",
      "Epoch 508/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3115 - accuracy: 0.8958\n",
      "Epoch 508: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.3115 - accuracy: 0.8958 - val_loss: 1.0351 - val_accuracy: 0.5000\n",
      "Epoch 509/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3113 - accuracy: 0.8750\n",
      "Epoch 509: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3113 - accuracy: 0.8750 - val_loss: 1.0384 - val_accuracy: 0.5000\n",
      "Epoch 510/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3112 - accuracy: 0.8958\n",
      "Epoch 510: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3112 - accuracy: 0.8958 - val_loss: 1.0362 - val_accuracy: 0.5000\n",
      "Epoch 511/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3110 - accuracy: 0.8750\n",
      "Epoch 511: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.3110 - accuracy: 0.8750 - val_loss: 1.0390 - val_accuracy: 0.5000\n",
      "Epoch 512/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3108 - accuracy: 0.8958\n",
      "Epoch 512: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.3108 - accuracy: 0.8958 - val_loss: 1.0368 - val_accuracy: 0.5000\n",
      "Epoch 513/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3107 - accuracy: 0.8958\n",
      "Epoch 513: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3107 - accuracy: 0.8958 - val_loss: 1.0401 - val_accuracy: 0.5000\n",
      "Epoch 514/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3105 - accuracy: 0.8958\n",
      "Epoch 514: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.3105 - accuracy: 0.8958 - val_loss: 1.0376 - val_accuracy: 0.5000\n",
      "Epoch 515/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3104 - accuracy: 0.8958\n",
      "Epoch 515: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.3104 - accuracy: 0.8958 - val_loss: 1.0412 - val_accuracy: 0.5000\n",
      "Epoch 516/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.8958\n",
      "Epoch 516: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.3102 - accuracy: 0.8958 - val_loss: 1.0388 - val_accuracy: 0.5000\n",
      "Epoch 517/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.8958\n",
      "Epoch 517: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.3100 - accuracy: 0.8958 - val_loss: 1.0417 - val_accuracy: 0.5000\n",
      "Epoch 518/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3099 - accuracy: 0.8958\n",
      "Epoch 518: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.3099 - accuracy: 0.8958 - val_loss: 1.0391 - val_accuracy: 0.5000\n",
      "Epoch 519/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3097 - accuracy: 0.8958\n",
      "Epoch 519: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.3097 - accuracy: 0.8958 - val_loss: 1.0429 - val_accuracy: 0.5000\n",
      "Epoch 520/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3096 - accuracy: 0.8958\n",
      "Epoch 520: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.3096 - accuracy: 0.8958 - val_loss: 1.0402 - val_accuracy: 0.5000\n",
      "Epoch 521/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.8958\n",
      "Epoch 521: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3094 - accuracy: 0.8958 - val_loss: 1.0437 - val_accuracy: 0.5000\n",
      "Epoch 522/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3092 - accuracy: 0.8958\n",
      "Epoch 522: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3092 - accuracy: 0.8958 - val_loss: 1.0413 - val_accuracy: 0.5000\n",
      "Epoch 523/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3090 - accuracy: 0.8958\n",
      "Epoch 523: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.3090 - accuracy: 0.8958 - val_loss: 1.0441 - val_accuracy: 0.5000\n",
      "Epoch 524/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3089 - accuracy: 0.8958\n",
      "Epoch 524: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.3089 - accuracy: 0.8958 - val_loss: 1.0419 - val_accuracy: 0.5000\n",
      "Epoch 525/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3087 - accuracy: 0.8958\n",
      "Epoch 525: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3087 - accuracy: 0.8958 - val_loss: 1.0452 - val_accuracy: 0.5000\n",
      "Epoch 526/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3086 - accuracy: 0.8958\n",
      "Epoch 526: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.3086 - accuracy: 0.8958 - val_loss: 1.0426 - val_accuracy: 0.5000\n",
      "Epoch 527/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3084 - accuracy: 0.8958\n",
      "Epoch 527: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.3084 - accuracy: 0.8958 - val_loss: 1.0463 - val_accuracy: 0.5000\n",
      "Epoch 528/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3082 - accuracy: 0.8958\n",
      "Epoch 528: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3082 - accuracy: 0.8958 - val_loss: 1.0440 - val_accuracy: 0.5000\n",
      "Epoch 529/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3080 - accuracy: 0.8958\n",
      "Epoch 529: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.3080 - accuracy: 0.8958 - val_loss: 1.0464 - val_accuracy: 0.5000\n",
      "Epoch 530/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3079 - accuracy: 0.8958\n",
      "Epoch 530: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.3079 - accuracy: 0.8958 - val_loss: 1.0448 - val_accuracy: 0.5000\n",
      "Epoch 531/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3077 - accuracy: 0.8958\n",
      "Epoch 531: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.3077 - accuracy: 0.8958 - val_loss: 1.0475 - val_accuracy: 0.5000\n",
      "Epoch 532/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3075 - accuracy: 0.8958\n",
      "Epoch 532: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.3075 - accuracy: 0.8958 - val_loss: 1.0456 - val_accuracy: 0.5000\n",
      "Epoch 533/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3073 - accuracy: 0.8958\n",
      "Epoch 533: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.3073 - accuracy: 0.8958 - val_loss: 1.0485 - val_accuracy: 0.5000\n",
      "Epoch 534/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.8958\n",
      "Epoch 534: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3072 - accuracy: 0.8958 - val_loss: 1.0463 - val_accuracy: 0.5000\n",
      "Epoch 535/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3070 - accuracy: 0.8958\n",
      "Epoch 535: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3070 - accuracy: 0.8958 - val_loss: 1.0495 - val_accuracy: 0.5000\n",
      "Epoch 536/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3069 - accuracy: 0.8958\n",
      "Epoch 536: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.3069 - accuracy: 0.8958 - val_loss: 1.0472 - val_accuracy: 0.5000\n",
      "Epoch 537/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3067 - accuracy: 0.8958\n",
      "Epoch 537: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.3067 - accuracy: 0.8958 - val_loss: 1.0502 - val_accuracy: 0.5000\n",
      "Epoch 538/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3066 - accuracy: 0.8958\n",
      "Epoch 538: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3066 - accuracy: 0.8958 - val_loss: 1.0479 - val_accuracy: 0.5000\n",
      "Epoch 539/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3064 - accuracy: 0.8958\n",
      "Epoch 539: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.3064 - accuracy: 0.8958 - val_loss: 1.0509 - val_accuracy: 0.5000\n",
      "Epoch 540/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3062 - accuracy: 0.8958\n",
      "Epoch 540: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.3062 - accuracy: 0.8958 - val_loss: 1.0487 - val_accuracy: 0.5000\n",
      "Epoch 541/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3060 - accuracy: 0.8958\n",
      "Epoch 541: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.3060 - accuracy: 0.8958 - val_loss: 1.0516 - val_accuracy: 0.5000\n",
      "Epoch 542/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3059 - accuracy: 0.8958\n",
      "Epoch 542: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.3059 - accuracy: 0.8958 - val_loss: 1.0498 - val_accuracy: 0.5000\n",
      "Epoch 543/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3057 - accuracy: 0.8958\n",
      "Epoch 543: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.3057 - accuracy: 0.8958 - val_loss: 1.0522 - val_accuracy: 0.5000\n",
      "Epoch 544/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3056 - accuracy: 0.8958\n",
      "Epoch 544: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3056 - accuracy: 0.8958 - val_loss: 1.0508 - val_accuracy: 0.5000\n",
      "Epoch 545/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3054 - accuracy: 0.8958\n",
      "Epoch 545: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.3054 - accuracy: 0.8958 - val_loss: 1.0534 - val_accuracy: 0.5000\n",
      "Epoch 546/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3053 - accuracy: 0.8958\n",
      "Epoch 546: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.3053 - accuracy: 0.8958 - val_loss: 1.0512 - val_accuracy: 0.5000\n",
      "Epoch 547/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3051 - accuracy: 0.8958\n",
      "Epoch 547: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.3051 - accuracy: 0.8958 - val_loss: 1.0538 - val_accuracy: 0.5000\n",
      "Epoch 548/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3049 - accuracy: 0.8958\n",
      "Epoch 548: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3049 - accuracy: 0.8958 - val_loss: 1.0520 - val_accuracy: 0.5000\n",
      "Epoch 549/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3048 - accuracy: 0.8958\n",
      "Epoch 549: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.3048 - accuracy: 0.8958 - val_loss: 1.0547 - val_accuracy: 0.5000\n",
      "Epoch 550/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3046 - accuracy: 0.8958\n",
      "Epoch 550: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3046 - accuracy: 0.8958 - val_loss: 1.0532 - val_accuracy: 0.5000\n",
      "Epoch 551/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3044 - accuracy: 0.8958\n",
      "Epoch 551: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3044 - accuracy: 0.8958 - val_loss: 1.0556 - val_accuracy: 0.5000\n",
      "Epoch 552/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3043 - accuracy: 0.8958\n",
      "Epoch 552: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.3043 - accuracy: 0.8958 - val_loss: 1.0542 - val_accuracy: 0.5000\n",
      "Epoch 553/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3041 - accuracy: 0.8958\n",
      "Epoch 553: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.3041 - accuracy: 0.8958 - val_loss: 1.0562 - val_accuracy: 0.5000\n",
      "Epoch 554/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3040 - accuracy: 0.8958\n",
      "Epoch 554: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.3040 - accuracy: 0.8958 - val_loss: 1.0545 - val_accuracy: 0.5000\n",
      "Epoch 555/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3038 - accuracy: 0.8958\n",
      "Epoch 555: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3038 - accuracy: 0.8958 - val_loss: 1.0568 - val_accuracy: 0.5000\n",
      "Epoch 556/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3037 - accuracy: 0.8958\n",
      "Epoch 556: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.3037 - accuracy: 0.8958 - val_loss: 1.0552 - val_accuracy: 0.5000\n",
      "Epoch 557/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3035 - accuracy: 0.8958\n",
      "Epoch 557: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.3035 - accuracy: 0.8958 - val_loss: 1.0583 - val_accuracy: 0.5000\n",
      "Epoch 558/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3033 - accuracy: 0.8958\n",
      "Epoch 558: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.3033 - accuracy: 0.8958 - val_loss: 1.0559 - val_accuracy: 0.5000\n",
      "Epoch 559/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3032 - accuracy: 0.8958\n",
      "Epoch 559: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3032 - accuracy: 0.8958 - val_loss: 1.0596 - val_accuracy: 0.5000\n",
      "Epoch 560/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3031 - accuracy: 0.8958\n",
      "Epoch 560: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.3031 - accuracy: 0.8958 - val_loss: 1.0564 - val_accuracy: 0.5000\n",
      "Epoch 561/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.8958\n",
      "Epoch 561: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.3029 - accuracy: 0.8958 - val_loss: 1.0599 - val_accuracy: 0.5000\n",
      "Epoch 562/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3028 - accuracy: 0.8958\n",
      "Epoch 562: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.3028 - accuracy: 0.8958 - val_loss: 1.0572 - val_accuracy: 0.5000\n",
      "Epoch 563/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3026 - accuracy: 0.8958\n",
      "Epoch 563: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.3026 - accuracy: 0.8958 - val_loss: 1.0608 - val_accuracy: 0.5000\n",
      "Epoch 564/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3024 - accuracy: 0.8958\n",
      "Epoch 564: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3024 - accuracy: 0.8958 - val_loss: 1.0580 - val_accuracy: 0.5000\n",
      "Epoch 565/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3023 - accuracy: 0.8958\n",
      "Epoch 565: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.3023 - accuracy: 0.8958 - val_loss: 1.0616 - val_accuracy: 0.5000\n",
      "Epoch 566/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3021 - accuracy: 0.8958\n",
      "Epoch 566: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.3021 - accuracy: 0.8958 - val_loss: 1.0591 - val_accuracy: 0.5000\n",
      "Epoch 567/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3020 - accuracy: 0.8958\n",
      "Epoch 567: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.3020 - accuracy: 0.8958 - val_loss: 1.0622 - val_accuracy: 0.5000\n",
      "Epoch 568/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3018 - accuracy: 0.8958\n",
      "Epoch 568: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.3018 - accuracy: 0.8958 - val_loss: 1.0599 - val_accuracy: 0.5000\n",
      "Epoch 569/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3016 - accuracy: 0.8958\n",
      "Epoch 569: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3016 - accuracy: 0.8958 - val_loss: 1.0631 - val_accuracy: 0.5000\n",
      "Epoch 570/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3015 - accuracy: 0.8958\n",
      "Epoch 570: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3015 - accuracy: 0.8958 - val_loss: 1.0604 - val_accuracy: 0.5000\n",
      "Epoch 571/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3014 - accuracy: 0.8958\n",
      "Epoch 571: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.3014 - accuracy: 0.8958 - val_loss: 1.0644 - val_accuracy: 0.5000\n",
      "Epoch 572/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3012 - accuracy: 0.8958\n",
      "Epoch 572: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.3012 - accuracy: 0.8958 - val_loss: 1.0611 - val_accuracy: 0.5000\n",
      "Epoch 573/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3011 - accuracy: 0.8958\n",
      "Epoch 573: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.3011 - accuracy: 0.8958 - val_loss: 1.0652 - val_accuracy: 0.5000\n",
      "Epoch 574/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3010 - accuracy: 0.8958\n",
      "Epoch 574: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.3010 - accuracy: 0.8958 - val_loss: 1.0622 - val_accuracy: 0.5000\n",
      "Epoch 575/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.8958\n",
      "Epoch 575: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.3008 - accuracy: 0.8958 - val_loss: 1.0661 - val_accuracy: 0.5000\n",
      "Epoch 576/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3007 - accuracy: 0.8958\n",
      "Epoch 576: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.3007 - accuracy: 0.8958 - val_loss: 1.0629 - val_accuracy: 0.5000\n",
      "Epoch 577/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3005 - accuracy: 0.8958\n",
      "Epoch 577: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3005 - accuracy: 0.8958 - val_loss: 1.0670 - val_accuracy: 0.5000\n",
      "Epoch 578/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3003 - accuracy: 0.8958\n",
      "Epoch 578: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.3003 - accuracy: 0.8958 - val_loss: 1.0639 - val_accuracy: 0.5000\n",
      "Epoch 579/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3002 - accuracy: 0.8958\n",
      "Epoch 579: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3002 - accuracy: 0.8958 - val_loss: 1.0675 - val_accuracy: 0.5000\n",
      "Epoch 580/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3000 - accuracy: 0.8958\n",
      "Epoch 580: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.3000 - accuracy: 0.8958 - val_loss: 1.0649 - val_accuracy: 0.5000\n",
      "Epoch 581/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2999 - accuracy: 0.8958\n",
      "Epoch 581: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.2999 - accuracy: 0.8958 - val_loss: 1.0681 - val_accuracy: 0.5000\n",
      "Epoch 582/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2997 - accuracy: 0.8958\n",
      "Epoch 582: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.2997 - accuracy: 0.8958 - val_loss: 1.0650 - val_accuracy: 0.5000\n",
      "Epoch 583/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2996 - accuracy: 0.8958\n",
      "Epoch 583: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2996 - accuracy: 0.8958 - val_loss: 1.0693 - val_accuracy: 0.5000\n",
      "Epoch 584/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2994 - accuracy: 0.8958\n",
      "Epoch 584: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2994 - accuracy: 0.8958 - val_loss: 1.0663 - val_accuracy: 0.5000\n",
      "Epoch 585/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2993 - accuracy: 0.8958\n",
      "Epoch 585: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.2993 - accuracy: 0.8958 - val_loss: 1.0699 - val_accuracy: 0.5000\n",
      "Epoch 586/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2991 - accuracy: 0.8958\n",
      "Epoch 586: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.2991 - accuracy: 0.8958 - val_loss: 1.0667 - val_accuracy: 0.5000\n",
      "Epoch 587/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 0.8958\n",
      "Epoch 587: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.2990 - accuracy: 0.8958 - val_loss: 1.0707 - val_accuracy: 0.5000\n",
      "Epoch 588/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2988 - accuracy: 0.8958\n",
      "Epoch 588: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.2988 - accuracy: 0.8958 - val_loss: 1.0680 - val_accuracy: 0.5000\n",
      "Epoch 589/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2986 - accuracy: 0.8958\n",
      "Epoch 589: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.2986 - accuracy: 0.8958 - val_loss: 1.0715 - val_accuracy: 0.5000\n",
      "Epoch 590/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2985 - accuracy: 0.8958\n",
      "Epoch 590: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2985 - accuracy: 0.8958 - val_loss: 1.0686 - val_accuracy: 0.5000\n",
      "Epoch 591/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2983 - accuracy: 0.8958\n",
      "Epoch 591: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.2983 - accuracy: 0.8958 - val_loss: 1.0721 - val_accuracy: 0.5000\n",
      "Epoch 592/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2982 - accuracy: 0.8958\n",
      "Epoch 592: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.2982 - accuracy: 0.8958 - val_loss: 1.0698 - val_accuracy: 0.5000\n",
      "Epoch 593/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2980 - accuracy: 0.8958\n",
      "Epoch 593: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.2980 - accuracy: 0.8958 - val_loss: 1.0739 - val_accuracy: 0.5000\n",
      "Epoch 594/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2980 - accuracy: 0.8958\n",
      "Epoch 594: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.2980 - accuracy: 0.8958 - val_loss: 1.0702 - val_accuracy: 0.5000\n",
      "Epoch 595/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2978 - accuracy: 0.8958\n",
      "Epoch 595: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2978 - accuracy: 0.8958 - val_loss: 1.0737 - val_accuracy: 0.5000\n",
      "Epoch 596/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2976 - accuracy: 0.8958\n",
      "Epoch 596: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.2976 - accuracy: 0.8958 - val_loss: 1.0708 - val_accuracy: 0.5000\n",
      "Epoch 597/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2975 - accuracy: 0.8958\n",
      "Epoch 597: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2975 - accuracy: 0.8958 - val_loss: 1.0749 - val_accuracy: 0.5000\n",
      "Epoch 598/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2973 - accuracy: 0.8958\n",
      "Epoch 598: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.2973 - accuracy: 0.8958 - val_loss: 1.0720 - val_accuracy: 0.5000\n",
      "Epoch 599/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2971 - accuracy: 0.8958\n",
      "Epoch 599: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.2971 - accuracy: 0.8958 - val_loss: 1.0755 - val_accuracy: 0.5000\n",
      "Epoch 600/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2970 - accuracy: 0.8958\n",
      "Epoch 600: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.2970 - accuracy: 0.8958 - val_loss: 1.0724 - val_accuracy: 0.5000\n",
      "Epoch 601/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2969 - accuracy: 0.8958\n",
      "Epoch 601: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.2969 - accuracy: 0.8958 - val_loss: 1.0759 - val_accuracy: 0.5000\n",
      "Epoch 602/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2967 - accuracy: 0.8958\n",
      "Epoch 602: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.2967 - accuracy: 0.8958 - val_loss: 1.0736 - val_accuracy: 0.5000\n",
      "Epoch 603/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2966 - accuracy: 0.8958\n",
      "Epoch 603: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.2966 - accuracy: 0.8958 - val_loss: 1.0771 - val_accuracy: 0.5000\n",
      "Epoch 604/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2964 - accuracy: 0.8958\n",
      "Epoch 604: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.2964 - accuracy: 0.8958 - val_loss: 1.0743 - val_accuracy: 0.5000\n",
      "Epoch 605/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2962 - accuracy: 0.8958\n",
      "Epoch 605: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2962 - accuracy: 0.8958 - val_loss: 1.0779 - val_accuracy: 0.5000\n",
      "Epoch 606/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2961 - accuracy: 0.8958\n",
      "Epoch 606: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.2961 - accuracy: 0.8958 - val_loss: 1.0745 - val_accuracy: 0.5000\n",
      "Epoch 607/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2960 - accuracy: 0.8958\n",
      "Epoch 607: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.2960 - accuracy: 0.8958 - val_loss: 1.0788 - val_accuracy: 0.5000\n",
      "Epoch 608/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2958 - accuracy: 0.8958\n",
      "Epoch 608: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.2958 - accuracy: 0.8958 - val_loss: 1.0756 - val_accuracy: 0.5000\n",
      "Epoch 609/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2957 - accuracy: 0.8958\n",
      "Epoch 609: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.2957 - accuracy: 0.8958 - val_loss: 1.0794 - val_accuracy: 0.5000\n",
      "Epoch 610/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2955 - accuracy: 0.8958\n",
      "Epoch 610: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.2955 - accuracy: 0.8958 - val_loss: 1.0766 - val_accuracy: 0.5000\n",
      "Epoch 611/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2954 - accuracy: 0.8958\n",
      "Epoch 611: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.2954 - accuracy: 0.8958 - val_loss: 1.0799 - val_accuracy: 0.5000\n",
      "Epoch 612/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2953 - accuracy: 0.8958\n",
      "Epoch 612: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2953 - accuracy: 0.8958 - val_loss: 1.0769 - val_accuracy: 0.5000\n",
      "Epoch 613/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2951 - accuracy: 0.8958\n",
      "Epoch 613: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.2951 - accuracy: 0.8958 - val_loss: 1.0806 - val_accuracy: 0.5000\n",
      "Epoch 614/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2950 - accuracy: 0.8958\n",
      "Epoch 614: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.2950 - accuracy: 0.8958 - val_loss: 1.0776 - val_accuracy: 0.5000\n",
      "Epoch 615/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2948 - accuracy: 0.8958\n",
      "Epoch 615: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2948 - accuracy: 0.8958 - val_loss: 1.0819 - val_accuracy: 0.5000\n",
      "Epoch 616/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2947 - accuracy: 0.8958\n",
      "Epoch 616: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.2947 - accuracy: 0.8958 - val_loss: 1.0787 - val_accuracy: 0.5000\n",
      "Epoch 617/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2945 - accuracy: 0.8958\n",
      "Epoch 617: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.2945 - accuracy: 0.8958 - val_loss: 1.0822 - val_accuracy: 0.5000\n",
      "Epoch 618/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2944 - accuracy: 0.8958\n",
      "Epoch 618: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2944 - accuracy: 0.8958 - val_loss: 1.0792 - val_accuracy: 0.5000\n",
      "Epoch 619/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.8958\n",
      "Epoch 619: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.2942 - accuracy: 0.8958 - val_loss: 1.0834 - val_accuracy: 0.5000\n",
      "Epoch 620/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.8958\n",
      "Epoch 620: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.2941 - accuracy: 0.8958 - val_loss: 1.0802 - val_accuracy: 0.5000\n",
      "Epoch 621/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2939 - accuracy: 0.8958\n",
      "Epoch 621: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.2939 - accuracy: 0.8958 - val_loss: 1.0840 - val_accuracy: 0.5000\n",
      "Epoch 622/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2938 - accuracy: 0.8958\n",
      "Epoch 622: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.2938 - accuracy: 0.8958 - val_loss: 1.0812 - val_accuracy: 0.5000\n",
      "Epoch 623/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2936 - accuracy: 0.8958\n",
      "Epoch 623: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.2936 - accuracy: 0.8958 - val_loss: 1.0846 - val_accuracy: 0.5000\n",
      "Epoch 624/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2934 - accuracy: 0.8958\n",
      "Epoch 624: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2934 - accuracy: 0.8958 - val_loss: 1.0820 - val_accuracy: 0.5000\n",
      "Epoch 625/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2933 - accuracy: 0.8958\n",
      "Epoch 625: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.2933 - accuracy: 0.8958 - val_loss: 1.0856 - val_accuracy: 0.5000\n",
      "Epoch 626/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.8958\n",
      "Epoch 626: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.2932 - accuracy: 0.8958 - val_loss: 1.0827 - val_accuracy: 0.5000\n",
      "Epoch 627/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2930 - accuracy: 0.8958\n",
      "Epoch 627: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2930 - accuracy: 0.8958 - val_loss: 1.0861 - val_accuracy: 0.5000\n",
      "Epoch 628/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2929 - accuracy: 0.8958\n",
      "Epoch 628: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2929 - accuracy: 0.8958 - val_loss: 1.0833 - val_accuracy: 0.5000\n",
      "Epoch 629/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2927 - accuracy: 0.8958\n",
      "Epoch 629: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.2927 - accuracy: 0.8958 - val_loss: 1.0866 - val_accuracy: 0.5000\n",
      "Epoch 630/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2926 - accuracy: 0.8958\n",
      "Epoch 630: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.2926 - accuracy: 0.8958 - val_loss: 1.0839 - val_accuracy: 0.5000\n",
      "Epoch 631/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2924 - accuracy: 0.8958\n",
      "Epoch 631: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.2924 - accuracy: 0.8958 - val_loss: 1.0882 - val_accuracy: 0.5000\n",
      "Epoch 632/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.8958\n",
      "Epoch 632: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.2923 - accuracy: 0.8958 - val_loss: 1.0849 - val_accuracy: 0.5000\n",
      "Epoch 633/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2922 - accuracy: 0.8958\n",
      "Epoch 633: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.2922 - accuracy: 0.8958 - val_loss: 1.0887 - val_accuracy: 0.5000\n",
      "Epoch 634/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2920 - accuracy: 0.8958\n",
      "Epoch 634: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.2920 - accuracy: 0.8958 - val_loss: 1.0859 - val_accuracy: 0.5000\n",
      "Epoch 635/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2918 - accuracy: 0.8958\n",
      "Epoch 635: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2918 - accuracy: 0.8958 - val_loss: 1.0890 - val_accuracy: 0.5000\n",
      "Epoch 636/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2917 - accuracy: 0.8958\n",
      "Epoch 636: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2917 - accuracy: 0.8958 - val_loss: 1.0863 - val_accuracy: 0.5000\n",
      "Epoch 637/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2915 - accuracy: 0.8958\n",
      "Epoch 637: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.2915 - accuracy: 0.8958 - val_loss: 1.0904 - val_accuracy: 0.5000\n",
      "Epoch 638/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.8958\n",
      "Epoch 638: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2914 - accuracy: 0.8958 - val_loss: 1.0874 - val_accuracy: 0.5000\n",
      "Epoch 639/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.8958\n",
      "Epoch 639: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2913 - accuracy: 0.8958 - val_loss: 1.0914 - val_accuracy: 0.5000\n",
      "Epoch 640/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2912 - accuracy: 0.8958\n",
      "Epoch 640: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.2912 - accuracy: 0.8958 - val_loss: 1.0881 - val_accuracy: 0.5000\n",
      "Epoch 641/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2910 - accuracy: 0.8958\n",
      "Epoch 641: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.2910 - accuracy: 0.8958 - val_loss: 1.0919 - val_accuracy: 0.5000\n",
      "Epoch 642/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2908 - accuracy: 0.8958\n",
      "Epoch 642: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2908 - accuracy: 0.8958 - val_loss: 1.0885 - val_accuracy: 0.5000\n",
      "Epoch 643/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2907 - accuracy: 0.8958\n",
      "Epoch 643: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2907 - accuracy: 0.8958 - val_loss: 1.0928 - val_accuracy: 0.5000\n",
      "Epoch 644/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.8958\n",
      "Epoch 644: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.2905 - accuracy: 0.8958 - val_loss: 1.0898 - val_accuracy: 0.5000\n",
      "Epoch 645/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2904 - accuracy: 0.8958\n",
      "Epoch 645: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.2904 - accuracy: 0.8958 - val_loss: 1.0934 - val_accuracy: 0.5000\n",
      "Epoch 646/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2902 - accuracy: 0.8958\n",
      "Epoch 646: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2902 - accuracy: 0.8958 - val_loss: 1.0907 - val_accuracy: 0.5000\n",
      "Epoch 647/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2901 - accuracy: 0.8958\n",
      "Epoch 647: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2901 - accuracy: 0.8958 - val_loss: 1.0938 - val_accuracy: 0.5000\n",
      "Epoch 648/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2899 - accuracy: 0.8958\n",
      "Epoch 648: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.2899 - accuracy: 0.8958 - val_loss: 1.0915 - val_accuracy: 0.5000\n",
      "Epoch 649/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2898 - accuracy: 0.8958\n",
      "Epoch 649: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.2898 - accuracy: 0.8958 - val_loss: 1.0950 - val_accuracy: 0.5000\n",
      "Epoch 650/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.8958\n",
      "Epoch 650: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.2897 - accuracy: 0.8958 - val_loss: 1.0923 - val_accuracy: 0.5000\n",
      "Epoch 651/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.8958\n",
      "Epoch 651: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.2895 - accuracy: 0.8958 - val_loss: 1.0955 - val_accuracy: 0.5000\n",
      "Epoch 652/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2894 - accuracy: 0.8958\n",
      "Epoch 652: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.2894 - accuracy: 0.8958 - val_loss: 1.0931 - val_accuracy: 0.5000\n",
      "Epoch 653/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.8958\n",
      "Epoch 653: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.2892 - accuracy: 0.8958 - val_loss: 1.0966 - val_accuracy: 0.5000\n",
      "Epoch 654/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.8958\n",
      "Epoch 654: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.2891 - accuracy: 0.8958 - val_loss: 1.0938 - val_accuracy: 0.5000\n",
      "Epoch 655/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2889 - accuracy: 0.8958\n",
      "Epoch 655: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.2889 - accuracy: 0.8958 - val_loss: 1.0975 - val_accuracy: 0.5000\n",
      "Epoch 656/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2888 - accuracy: 0.8958\n",
      "Epoch 656: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.2888 - accuracy: 0.8958 - val_loss: 1.0950 - val_accuracy: 0.5000\n",
      "Epoch 657/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2887 - accuracy: 0.8958\n",
      "Epoch 657: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2887 - accuracy: 0.8958 - val_loss: 1.0984 - val_accuracy: 0.5000\n",
      "Epoch 658/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.8958\n",
      "Epoch 658: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.2885 - accuracy: 0.8958 - val_loss: 1.0951 - val_accuracy: 0.5000\n",
      "Epoch 659/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2884 - accuracy: 0.8958\n",
      "Epoch 659: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2884 - accuracy: 0.8958 - val_loss: 1.0989 - val_accuracy: 0.5000\n",
      "Epoch 660/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.8958\n",
      "Epoch 660: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2883 - accuracy: 0.8958 - val_loss: 1.0964 - val_accuracy: 0.5000\n",
      "Epoch 661/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.8958\n",
      "Epoch 661: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2881 - accuracy: 0.8958 - val_loss: 1.1002 - val_accuracy: 0.5000\n",
      "Epoch 662/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2880 - accuracy: 0.8958\n",
      "Epoch 662: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.2880 - accuracy: 0.8958 - val_loss: 1.0969 - val_accuracy: 0.5000\n",
      "Epoch 663/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.8958\n",
      "Epoch 663: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2878 - accuracy: 0.8958 - val_loss: 1.1007 - val_accuracy: 0.5000\n",
      "Epoch 664/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.8958\n",
      "Epoch 664: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.2877 - accuracy: 0.8958 - val_loss: 1.0977 - val_accuracy: 0.5000\n",
      "Epoch 665/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2875 - accuracy: 0.8958\n",
      "Epoch 665: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.2875 - accuracy: 0.8958 - val_loss: 1.1014 - val_accuracy: 0.5000\n",
      "Epoch 666/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2874 - accuracy: 0.8958\n",
      "Epoch 666: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.2874 - accuracy: 0.8958 - val_loss: 1.0985 - val_accuracy: 0.5000\n",
      "Epoch 667/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2873 - accuracy: 0.8958\n",
      "Epoch 667: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2873 - accuracy: 0.8958 - val_loss: 1.1024 - val_accuracy: 0.5000\n",
      "Epoch 668/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2871 - accuracy: 0.8958\n",
      "Epoch 668: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.2871 - accuracy: 0.8958 - val_loss: 1.0993 - val_accuracy: 0.5000\n",
      "Epoch 669/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2870 - accuracy: 0.8958\n",
      "Epoch 669: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.2870 - accuracy: 0.8958 - val_loss: 1.1033 - val_accuracy: 0.5000\n",
      "Epoch 670/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2868 - accuracy: 0.8958\n",
      "Epoch 670: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.2868 - accuracy: 0.8958 - val_loss: 1.1002 - val_accuracy: 0.5000\n",
      "Epoch 671/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2867 - accuracy: 0.8958\n",
      "Epoch 671: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.2867 - accuracy: 0.8958 - val_loss: 1.1042 - val_accuracy: 0.5000\n",
      "Epoch 672/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2866 - accuracy: 0.8958\n",
      "Epoch 672: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.2866 - accuracy: 0.8958 - val_loss: 1.1008 - val_accuracy: 0.5000\n",
      "Epoch 673/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.8958\n",
      "Epoch 673: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2864 - accuracy: 0.8958 - val_loss: 1.1048 - val_accuracy: 0.5000\n",
      "Epoch 674/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.8958\n",
      "Epoch 674: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2863 - accuracy: 0.8958 - val_loss: 1.1015 - val_accuracy: 0.5000\n",
      "Epoch 675/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.8958\n",
      "Epoch 675: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.2861 - accuracy: 0.8958 - val_loss: 1.1059 - val_accuracy: 0.5000\n",
      "Epoch 676/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2860 - accuracy: 0.8958\n",
      "Epoch 676: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.2860 - accuracy: 0.8958 - val_loss: 1.1025 - val_accuracy: 0.5000\n",
      "Epoch 677/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2859 - accuracy: 0.8958\n",
      "Epoch 677: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.2859 - accuracy: 0.8958 - val_loss: 1.1067 - val_accuracy: 0.5000\n",
      "Epoch 678/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.8958\n",
      "Epoch 678: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.2857 - accuracy: 0.8958 - val_loss: 1.1034 - val_accuracy: 0.5000\n",
      "Epoch 679/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.9167\n",
      "Epoch 679: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.2856 - accuracy: 0.9167 - val_loss: 1.1072 - val_accuracy: 0.5000\n",
      "Epoch 680/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.8958\n",
      "Epoch 680: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.2854 - accuracy: 0.8958 - val_loss: 1.1044 - val_accuracy: 0.5000\n",
      "Epoch 681/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.8958\n",
      "Epoch 681: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.2852 - accuracy: 0.8958 - val_loss: 1.1081 - val_accuracy: 0.5000\n",
      "Epoch 682/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2851 - accuracy: 0.8958\n",
      "Epoch 682: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.2851 - accuracy: 0.8958 - val_loss: 1.1051 - val_accuracy: 0.5000\n",
      "Epoch 683/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.9167\n",
      "Epoch 683: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.2850 - accuracy: 0.9167 - val_loss: 1.1086 - val_accuracy: 0.5000\n",
      "Epoch 684/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.8958\n",
      "Epoch 684: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.2848 - accuracy: 0.8958 - val_loss: 1.1060 - val_accuracy: 0.5000\n",
      "Epoch 685/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.8958\n",
      "Epoch 685: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.2847 - accuracy: 0.8958 - val_loss: 1.1096 - val_accuracy: 0.5000\n",
      "Epoch 686/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2845 - accuracy: 0.8958\n",
      "Epoch 686: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.2845 - accuracy: 0.8958 - val_loss: 1.1072 - val_accuracy: 0.5000\n",
      "Epoch 687/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.8958\n",
      "Epoch 687: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.2844 - accuracy: 0.8958 - val_loss: 1.1103 - val_accuracy: 0.5000\n",
      "Epoch 688/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2842 - accuracy: 0.8958\n",
      "Epoch 688: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2842 - accuracy: 0.8958 - val_loss: 1.1082 - val_accuracy: 0.5000\n",
      "Epoch 689/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2840 - accuracy: 0.8958\n",
      "Epoch 689: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.2840 - accuracy: 0.8958 - val_loss: 1.1112 - val_accuracy: 0.5000\n",
      "Epoch 690/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2839 - accuracy: 0.8958\n",
      "Epoch 690: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.2839 - accuracy: 0.8958 - val_loss: 1.1093 - val_accuracy: 0.5000\n",
      "Epoch 691/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2838 - accuracy: 0.8958\n",
      "Epoch 691: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.2838 - accuracy: 0.8958 - val_loss: 1.1115 - val_accuracy: 0.5000\n",
      "Epoch 692/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2836 - accuracy: 0.8958\n",
      "Epoch 692: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.2836 - accuracy: 0.8958 - val_loss: 1.1102 - val_accuracy: 0.5000\n",
      "Epoch 693/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2835 - accuracy: 0.8958\n",
      "Epoch 693: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.2835 - accuracy: 0.8958 - val_loss: 1.1124 - val_accuracy: 0.5000\n",
      "Epoch 694/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2833 - accuracy: 0.8958\n",
      "Epoch 694: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.2833 - accuracy: 0.8958 - val_loss: 1.1106 - val_accuracy: 0.5000\n",
      "Epoch 695/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2832 - accuracy: 0.8958\n",
      "Epoch 695: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.2832 - accuracy: 0.8958 - val_loss: 1.1139 - val_accuracy: 0.5000\n",
      "Epoch 696/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2831 - accuracy: 0.8958\n",
      "Epoch 696: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.2831 - accuracy: 0.8958 - val_loss: 1.1113 - val_accuracy: 0.5000\n",
      "Epoch 697/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2829 - accuracy: 0.8958\n",
      "Epoch 697: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 0.2829 - accuracy: 0.8958 - val_loss: 1.1139 - val_accuracy: 0.5000\n",
      "Epoch 698/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2828 - accuracy: 0.8958\n",
      "Epoch 698: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.2828 - accuracy: 0.8958 - val_loss: 1.1123 - val_accuracy: 0.5000\n",
      "Epoch 699/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2826 - accuracy: 0.8958\n",
      "Epoch 699: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.2826 - accuracy: 0.8958 - val_loss: 1.1146 - val_accuracy: 0.5000\n",
      "Epoch 700/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2825 - accuracy: 0.8958\n",
      "Epoch 700: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.2825 - accuracy: 0.8958 - val_loss: 1.1130 - val_accuracy: 0.5000\n",
      "Epoch 701/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2824 - accuracy: 0.8958\n",
      "Epoch 701: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.2824 - accuracy: 0.8958 - val_loss: 1.1161 - val_accuracy: 0.5000\n",
      "Epoch 702/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.8958\n",
      "Epoch 702: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.2822 - accuracy: 0.8958 - val_loss: 1.1134 - val_accuracy: 0.5000\n",
      "Epoch 703/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2821 - accuracy: 0.9167\n",
      "Epoch 703: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.2821 - accuracy: 0.9167 - val_loss: 1.1167 - val_accuracy: 0.5000\n",
      "Epoch 704/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2819 - accuracy: 0.8958\n",
      "Epoch 704: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.2819 - accuracy: 0.8958 - val_loss: 1.1141 - val_accuracy: 0.5000\n",
      "Epoch 705/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2818 - accuracy: 0.9167\n",
      "Epoch 705: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.2818 - accuracy: 0.9167 - val_loss: 1.1174 - val_accuracy: 0.5000\n",
      "Epoch 706/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2817 - accuracy: 0.8958\n",
      "Epoch 706: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2817 - accuracy: 0.8958 - val_loss: 1.1150 - val_accuracy: 0.5000\n",
      "Epoch 707/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2815 - accuracy: 0.9167\n",
      "Epoch 707: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.2815 - accuracy: 0.9167 - val_loss: 1.1189 - val_accuracy: 0.5000\n",
      "Epoch 708/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2814 - accuracy: 0.8958\n",
      "Epoch 708: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.2814 - accuracy: 0.8958 - val_loss: 1.1155 - val_accuracy: 0.5000\n",
      "Epoch 709/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2813 - accuracy: 0.9167\n",
      "Epoch 709: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.2813 - accuracy: 0.9167 - val_loss: 1.1194 - val_accuracy: 0.5000\n",
      "Epoch 710/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2812 - accuracy: 0.8958\n",
      "Epoch 710: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 0.2812 - accuracy: 0.8958 - val_loss: 1.1159 - val_accuracy: 0.5000\n",
      "Epoch 711/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2810 - accuracy: 0.9167\n",
      "Epoch 711: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.2810 - accuracy: 0.9167 - val_loss: 1.1204 - val_accuracy: 0.5000\n",
      "Epoch 712/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2809 - accuracy: 0.8958\n",
      "Epoch 712: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 487ms/step - loss: 0.2809 - accuracy: 0.8958 - val_loss: 1.1168 - val_accuracy: 0.5000\n",
      "Epoch 713/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2807 - accuracy: 0.9167\n",
      "Epoch 713: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 0.2807 - accuracy: 0.9167 - val_loss: 1.1208 - val_accuracy: 0.5000\n",
      "Epoch 714/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2806 - accuracy: 0.8958\n",
      "Epoch 714: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 0.2806 - accuracy: 0.8958 - val_loss: 1.1176 - val_accuracy: 0.5000\n",
      "Epoch 715/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2804 - accuracy: 0.9167\n",
      "Epoch 715: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 1s 620ms/step - loss: 0.2804 - accuracy: 0.9167 - val_loss: 1.1217 - val_accuracy: 0.5000\n",
      "Epoch 716/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2803 - accuracy: 0.8958\n",
      "Epoch 716: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.2803 - accuracy: 0.8958 - val_loss: 1.1190 - val_accuracy: 0.5000\n",
      "Epoch 717/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2801 - accuracy: 0.9167\n",
      "Epoch 717: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2801 - accuracy: 0.9167 - val_loss: 1.1218 - val_accuracy: 0.5000\n",
      "Epoch 718/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2800 - accuracy: 0.8958\n",
      "Epoch 718: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.2800 - accuracy: 0.8958 - val_loss: 1.1195 - val_accuracy: 0.5000\n",
      "Epoch 719/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2799 - accuracy: 0.9167\n",
      "Epoch 719: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.2799 - accuracy: 0.9167 - val_loss: 1.1228 - val_accuracy: 0.5000\n",
      "Epoch 720/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2797 - accuracy: 0.8958\n",
      "Epoch 720: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.2797 - accuracy: 0.8958 - val_loss: 1.1198 - val_accuracy: 0.5000\n",
      "Epoch 721/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2796 - accuracy: 0.9167\n",
      "Epoch 721: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.2796 - accuracy: 0.9167 - val_loss: 1.1233 - val_accuracy: 0.5000\n",
      "Epoch 722/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2795 - accuracy: 0.8958\n",
      "Epoch 722: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.2795 - accuracy: 0.8958 - val_loss: 1.1210 - val_accuracy: 0.5000\n",
      "Epoch 723/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2793 - accuracy: 0.9167\n",
      "Epoch 723: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.2793 - accuracy: 0.9167 - val_loss: 1.1242 - val_accuracy: 0.5000\n",
      "Epoch 724/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2792 - accuracy: 0.8958\n",
      "Epoch 724: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.2792 - accuracy: 0.8958 - val_loss: 1.1218 - val_accuracy: 0.5000\n",
      "Epoch 725/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2790 - accuracy: 0.9167\n",
      "Epoch 725: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.2790 - accuracy: 0.9167 - val_loss: 1.1250 - val_accuracy: 0.5000\n",
      "Epoch 726/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2789 - accuracy: 0.8958\n",
      "Epoch 726: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.2789 - accuracy: 0.8958 - val_loss: 1.1225 - val_accuracy: 0.5000\n",
      "Epoch 727/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2787 - accuracy: 0.9167\n",
      "Epoch 727: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2787 - accuracy: 0.9167 - val_loss: 1.1260 - val_accuracy: 0.5000\n",
      "Epoch 728/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2786 - accuracy: 0.8958\n",
      "Epoch 728: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.2786 - accuracy: 0.8958 - val_loss: 1.1233 - val_accuracy: 0.5000\n",
      "Epoch 729/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2785 - accuracy: 0.9167\n",
      "Epoch 729: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.2785 - accuracy: 0.9167 - val_loss: 1.1268 - val_accuracy: 0.5000\n",
      "Epoch 730/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2783 - accuracy: 0.8958\n",
      "Epoch 730: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.2783 - accuracy: 0.8958 - val_loss: 1.1244 - val_accuracy: 0.5000\n",
      "Epoch 731/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2782 - accuracy: 0.9167\n",
      "Epoch 731: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2782 - accuracy: 0.9167 - val_loss: 1.1273 - val_accuracy: 0.5000\n",
      "Epoch 732/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2781 - accuracy: 0.8958\n",
      "Epoch 732: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.2781 - accuracy: 0.8958 - val_loss: 1.1246 - val_accuracy: 0.5000\n",
      "Epoch 733/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.9167\n",
      "Epoch 733: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.2779 - accuracy: 0.9167 - val_loss: 1.1282 - val_accuracy: 0.5000\n",
      "Epoch 734/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2778 - accuracy: 0.8958\n",
      "Epoch 734: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.2778 - accuracy: 0.8958 - val_loss: 1.1257 - val_accuracy: 0.5000\n",
      "Epoch 735/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2777 - accuracy: 0.9167\n",
      "Epoch 735: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.2777 - accuracy: 0.9167 - val_loss: 1.1288 - val_accuracy: 0.5000\n",
      "Epoch 736/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2776 - accuracy: 0.8958\n",
      "Epoch 736: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.2776 - accuracy: 0.8958 - val_loss: 1.1264 - val_accuracy: 0.5000\n",
      "Epoch 737/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2774 - accuracy: 0.9167\n",
      "Epoch 737: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.2774 - accuracy: 0.9167 - val_loss: 1.1300 - val_accuracy: 0.5000\n",
      "Epoch 738/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2773 - accuracy: 0.8958\n",
      "Epoch 738: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.2773 - accuracy: 0.8958 - val_loss: 1.1268 - val_accuracy: 0.5000\n",
      "Epoch 739/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2772 - accuracy: 0.9167\n",
      "Epoch 739: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.2772 - accuracy: 0.9167 - val_loss: 1.1308 - val_accuracy: 0.5000\n",
      "Epoch 740/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2770 - accuracy: 0.8958\n",
      "Epoch 740: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.2770 - accuracy: 0.8958 - val_loss: 1.1276 - val_accuracy: 0.5000\n",
      "Epoch 741/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.9167\n",
      "Epoch 741: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.2769 - accuracy: 0.9167 - val_loss: 1.1319 - val_accuracy: 0.5000\n",
      "Epoch 742/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2767 - accuracy: 0.8958\n",
      "Epoch 742: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2767 - accuracy: 0.8958 - val_loss: 1.1288 - val_accuracy: 0.5000\n",
      "Epoch 743/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2766 - accuracy: 0.9167\n",
      "Epoch 743: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.2766 - accuracy: 0.9167 - val_loss: 1.1326 - val_accuracy: 0.5000\n",
      "Epoch 744/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2765 - accuracy: 0.8958\n",
      "Epoch 744: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.2765 - accuracy: 0.8958 - val_loss: 1.1297 - val_accuracy: 0.5000\n",
      "Epoch 745/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.9167\n",
      "Epoch 745: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.2763 - accuracy: 0.9167 - val_loss: 1.1331 - val_accuracy: 0.5000\n",
      "Epoch 746/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2762 - accuracy: 0.8958\n",
      "Epoch 746: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.2762 - accuracy: 0.8958 - val_loss: 1.1305 - val_accuracy: 0.5000\n",
      "Epoch 747/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2760 - accuracy: 0.9167\n",
      "Epoch 747: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2760 - accuracy: 0.9167 - val_loss: 1.1340 - val_accuracy: 0.5000\n",
      "Epoch 748/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2759 - accuracy: 0.8958\n",
      "Epoch 748: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.2759 - accuracy: 0.8958 - val_loss: 1.1314 - val_accuracy: 0.5000\n",
      "Epoch 749/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2758 - accuracy: 0.9167\n",
      "Epoch 749: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.2758 - accuracy: 0.9167 - val_loss: 1.1349 - val_accuracy: 0.5000\n",
      "Epoch 750/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2756 - accuracy: 0.8958\n",
      "Epoch 750: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.2756 - accuracy: 0.8958 - val_loss: 1.1320 - val_accuracy: 0.5000\n",
      "Epoch 751/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2755 - accuracy: 0.9167\n",
      "Epoch 751: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.2755 - accuracy: 0.9167 - val_loss: 1.1359 - val_accuracy: 0.5000\n",
      "Epoch 752/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2754 - accuracy: 0.8958\n",
      "Epoch 752: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.2754 - accuracy: 0.8958 - val_loss: 1.1326 - val_accuracy: 0.5000\n",
      "Epoch 753/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2753 - accuracy: 0.9167\n",
      "Epoch 753: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.2753 - accuracy: 0.9167 - val_loss: 1.1367 - val_accuracy: 0.5000\n",
      "Epoch 754/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2751 - accuracy: 0.8958\n",
      "Epoch 754: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 450ms/step - loss: 0.2751 - accuracy: 0.8958 - val_loss: 1.1337 - val_accuracy: 0.5000\n",
      "Epoch 755/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2750 - accuracy: 0.9167\n",
      "Epoch 755: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.2750 - accuracy: 0.9167 - val_loss: 1.1374 - val_accuracy: 0.5000\n",
      "Epoch 756/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2749 - accuracy: 0.8958\n",
      "Epoch 756: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.2749 - accuracy: 0.8958 - val_loss: 1.1346 - val_accuracy: 0.5000\n",
      "Epoch 757/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2747 - accuracy: 0.9167\n",
      "Epoch 757: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.2747 - accuracy: 0.9167 - val_loss: 1.1376 - val_accuracy: 0.5000\n",
      "Epoch 758/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2746 - accuracy: 0.8958\n",
      "Epoch 758: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2746 - accuracy: 0.8958 - val_loss: 1.1358 - val_accuracy: 0.5000\n",
      "Epoch 759/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2744 - accuracy: 0.9167\n",
      "Epoch 759: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.2744 - accuracy: 0.9167 - val_loss: 1.1391 - val_accuracy: 0.5000\n",
      "Epoch 760/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2743 - accuracy: 0.8958\n",
      "Epoch 760: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.2743 - accuracy: 0.8958 - val_loss: 1.1360 - val_accuracy: 0.5000\n",
      "Epoch 761/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2742 - accuracy: 0.9167\n",
      "Epoch 761: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.2742 - accuracy: 0.9167 - val_loss: 1.1397 - val_accuracy: 0.5000\n",
      "Epoch 762/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2740 - accuracy: 0.8958\n",
      "Epoch 762: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.2740 - accuracy: 0.8958 - val_loss: 1.1366 - val_accuracy: 0.5000\n",
      "Epoch 763/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2739 - accuracy: 0.9167\n",
      "Epoch 763: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.2739 - accuracy: 0.9167 - val_loss: 1.1406 - val_accuracy: 0.5000\n",
      "Epoch 764/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2738 - accuracy: 0.8958\n",
      "Epoch 764: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 0.2738 - accuracy: 0.8958 - val_loss: 1.1378 - val_accuracy: 0.5000\n",
      "Epoch 765/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2737 - accuracy: 0.9167\n",
      "Epoch 765: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.2737 - accuracy: 0.9167 - val_loss: 1.1413 - val_accuracy: 0.5000\n",
      "Epoch 766/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2735 - accuracy: 0.8958\n",
      "Epoch 766: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.2735 - accuracy: 0.8958 - val_loss: 1.1385 - val_accuracy: 0.5000\n",
      "Epoch 767/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2734 - accuracy: 0.9167\n",
      "Epoch 767: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.2734 - accuracy: 0.9167 - val_loss: 1.1420 - val_accuracy: 0.5000\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2733 - accuracy: 0.8958\n",
      "Epoch 768: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.2733 - accuracy: 0.8958 - val_loss: 1.1395 - val_accuracy: 0.5000\n",
      "Epoch 769/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2731 - accuracy: 0.9167\n",
      "Epoch 769: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.2731 - accuracy: 0.9167 - val_loss: 1.1431 - val_accuracy: 0.5000\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2730 - accuracy: 0.8958\n",
      "Epoch 770: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.2730 - accuracy: 0.8958 - val_loss: 1.1400 - val_accuracy: 0.5000\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2728 - accuracy: 0.9167\n",
      "Epoch 771: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.2728 - accuracy: 0.9167 - val_loss: 1.1439 - val_accuracy: 0.5000\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2727 - accuracy: 0.8958\n",
      "Epoch 772: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.2727 - accuracy: 0.8958 - val_loss: 1.1412 - val_accuracy: 0.5000\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2726 - accuracy: 0.9167\n",
      "Epoch 773: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.2726 - accuracy: 0.9167 - val_loss: 1.1443 - val_accuracy: 0.5000\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2724 - accuracy: 0.8958\n",
      "Epoch 774: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2724 - accuracy: 0.8958 - val_loss: 1.1421 - val_accuracy: 0.5000\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2723 - accuracy: 0.9167\n",
      "Epoch 775: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.2723 - accuracy: 0.9167 - val_loss: 1.1454 - val_accuracy: 0.5000\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.8958\n",
      "Epoch 776: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.2721 - accuracy: 0.8958 - val_loss: 1.1430 - val_accuracy: 0.5000\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2720 - accuracy: 0.9167\n",
      "Epoch 777: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2720 - accuracy: 0.9167 - val_loss: 1.1464 - val_accuracy: 0.5000\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2719 - accuracy: 0.8958\n",
      "Epoch 778: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2719 - accuracy: 0.8958 - val_loss: 1.1436 - val_accuracy: 0.5000\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2717 - accuracy: 0.9167\n",
      "Epoch 779: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.2717 - accuracy: 0.9167 - val_loss: 1.1473 - val_accuracy: 0.5000\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2716 - accuracy: 0.8958\n",
      "Epoch 780: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.2716 - accuracy: 0.8958 - val_loss: 1.1441 - val_accuracy: 0.5000\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2715 - accuracy: 0.9167\n",
      "Epoch 781: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.2715 - accuracy: 0.9167 - val_loss: 1.1480 - val_accuracy: 0.5000\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2714 - accuracy: 0.8958\n",
      "Epoch 782: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 0.2714 - accuracy: 0.8958 - val_loss: 1.1450 - val_accuracy: 0.5000\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.9167\n",
      "Epoch 783: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.2712 - accuracy: 0.9167 - val_loss: 1.1491 - val_accuracy: 0.5000\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2711 - accuracy: 0.8958\n",
      "Epoch 784: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.2711 - accuracy: 0.8958 - val_loss: 1.1461 - val_accuracy: 0.5000\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.9167\n",
      "Epoch 785: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.2709 - accuracy: 0.9167 - val_loss: 1.1496 - val_accuracy: 0.5000\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2708 - accuracy: 0.8958\n",
      "Epoch 786: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2708 - accuracy: 0.8958 - val_loss: 1.1468 - val_accuracy: 0.5000\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2707 - accuracy: 0.9167\n",
      "Epoch 787: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2707 - accuracy: 0.9167 - val_loss: 1.1505 - val_accuracy: 0.5000\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.8958\n",
      "Epoch 788: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.2705 - accuracy: 0.8958 - val_loss: 1.1477 - val_accuracy: 0.5000\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2704 - accuracy: 0.9167\n",
      "Epoch 789: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 0.2704 - accuracy: 0.9167 - val_loss: 1.1515 - val_accuracy: 0.5000\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.8958\n",
      "Epoch 790: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.2703 - accuracy: 0.8958 - val_loss: 1.1484 - val_accuracy: 0.5000\n",
      "Epoch 791/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.9167\n",
      "Epoch 791: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.2702 - accuracy: 0.9167 - val_loss: 1.1522 - val_accuracy: 0.5000\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.8958\n",
      "Epoch 792: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.2700 - accuracy: 0.8958 - val_loss: 1.1494 - val_accuracy: 0.5000\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.9167\n",
      "Epoch 793: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.2699 - accuracy: 0.9167 - val_loss: 1.1526 - val_accuracy: 0.5000\n",
      "Epoch 794/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2698 - accuracy: 0.8958\n",
      "Epoch 794: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.2698 - accuracy: 0.8958 - val_loss: 1.1500 - val_accuracy: 0.5000\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.9167\n",
      "Epoch 795: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.2696 - accuracy: 0.9167 - val_loss: 1.1536 - val_accuracy: 0.5000\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.8958\n",
      "Epoch 796: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.2695 - accuracy: 0.8958 - val_loss: 1.1508 - val_accuracy: 0.5000\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2694 - accuracy: 0.9167\n",
      "Epoch 797: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.2694 - accuracy: 0.9167 - val_loss: 1.1544 - val_accuracy: 0.5000\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2693 - accuracy: 0.8958\n",
      "Epoch 798: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.2693 - accuracy: 0.8958 - val_loss: 1.1518 - val_accuracy: 0.5000\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.9167\n",
      "Epoch 799: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.2691 - accuracy: 0.9167 - val_loss: 1.1547 - val_accuracy: 0.5000\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2690 - accuracy: 0.8958\n",
      "Epoch 800: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.2690 - accuracy: 0.8958 - val_loss: 1.1520 - val_accuracy: 0.5000\n",
      "Epoch 801/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2689 - accuracy: 0.9167\n",
      "Epoch 801: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.2689 - accuracy: 0.9167 - val_loss: 1.1564 - val_accuracy: 0.5000\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2688 - accuracy: 0.8958\n",
      "Epoch 802: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.2688 - accuracy: 0.8958 - val_loss: 1.1525 - val_accuracy: 0.5000\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2687 - accuracy: 0.9167\n",
      "Epoch 803: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 401ms/step - loss: 0.2687 - accuracy: 0.9167 - val_loss: 1.1571 - val_accuracy: 0.5000\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2686 - accuracy: 0.8958\n",
      "Epoch 804: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.2686 - accuracy: 0.8958 - val_loss: 1.1536 - val_accuracy: 0.5000\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.9167\n",
      "Epoch 805: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.2684 - accuracy: 0.9167 - val_loss: 1.1573 - val_accuracy: 0.5000\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2683 - accuracy: 0.8958\n",
      "Epoch 806: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.2683 - accuracy: 0.8958 - val_loss: 1.1543 - val_accuracy: 0.5000\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.9167\n",
      "Epoch 807: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 0.2681 - accuracy: 0.9167 - val_loss: 1.1579 - val_accuracy: 0.5000\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2680 - accuracy: 0.8958\n",
      "Epoch 808: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.2680 - accuracy: 0.8958 - val_loss: 1.1549 - val_accuracy: 0.5000\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2679 - accuracy: 0.9167\n",
      "Epoch 809: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.2679 - accuracy: 0.9167 - val_loss: 1.1587 - val_accuracy: 0.5000\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2678 - accuracy: 0.8958\n",
      "Epoch 810: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.2678 - accuracy: 0.8958 - val_loss: 1.1557 - val_accuracy: 0.5000\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.9167\n",
      "Epoch 811: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.2677 - accuracy: 0.9167 - val_loss: 1.1596 - val_accuracy: 0.5000\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 0.8958\n",
      "Epoch 812: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.2676 - accuracy: 0.8958 - val_loss: 1.1564 - val_accuracy: 0.5000\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2674 - accuracy: 0.9167\n",
      "Epoch 813: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.2674 - accuracy: 0.9167 - val_loss: 1.1605 - val_accuracy: 0.5000\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2673 - accuracy: 0.8958\n",
      "Epoch 814: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.2673 - accuracy: 0.8958 - val_loss: 1.1572 - val_accuracy: 0.5000\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2671 - accuracy: 0.9167\n",
      "Epoch 815: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.2671 - accuracy: 0.9167 - val_loss: 1.1611 - val_accuracy: 0.5000\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2670 - accuracy: 0.8958\n",
      "Epoch 816: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 0.2670 - accuracy: 0.8958 - val_loss: 1.1579 - val_accuracy: 0.5000\n",
      "Epoch 817/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2669 - accuracy: 0.9167\n",
      "Epoch 817: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.2669 - accuracy: 0.9167 - val_loss: 1.1616 - val_accuracy: 0.5000\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2668 - accuracy: 0.8958\n",
      "Epoch 818: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.2668 - accuracy: 0.8958 - val_loss: 1.1592 - val_accuracy: 0.5000\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.9167\n",
      "Epoch 819: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.2666 - accuracy: 0.9167 - val_loss: 1.1627 - val_accuracy: 0.5000\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.8958\n",
      "Epoch 820: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.2665 - accuracy: 0.8958 - val_loss: 1.1596 - val_accuracy: 0.5000\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2664 - accuracy: 0.9167\n",
      "Epoch 821: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 0.2664 - accuracy: 0.9167 - val_loss: 1.1632 - val_accuracy: 0.5000\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2663 - accuracy: 0.8958\n",
      "Epoch 822: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.2663 - accuracy: 0.8958 - val_loss: 1.1605 - val_accuracy: 0.5000\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.9167\n",
      "Epoch 823: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.2662 - accuracy: 0.9167 - val_loss: 1.1642 - val_accuracy: 0.5000\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2660 - accuracy: 0.8958\n",
      "Epoch 824: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.2660 - accuracy: 0.8958 - val_loss: 1.1613 - val_accuracy: 0.5000\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2659 - accuracy: 0.9167\n",
      "Epoch 825: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.2659 - accuracy: 0.9167 - val_loss: 1.1649 - val_accuracy: 0.5000\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 0.8958\n",
      "Epoch 826: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 0.2658 - accuracy: 0.8958 - val_loss: 1.1622 - val_accuracy: 0.5000\n",
      "Epoch 827/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2656 - accuracy: 0.9167\n",
      "Epoch 827: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.2656 - accuracy: 0.9167 - val_loss: 1.1654 - val_accuracy: 0.5000\n",
      "Epoch 828/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2656 - accuracy: 0.8958\n",
      "Epoch 828: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.2656 - accuracy: 0.8958 - val_loss: 1.1631 - val_accuracy: 0.5000\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.9167\n",
      "Epoch 829: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.2654 - accuracy: 0.9167 - val_loss: 1.1661 - val_accuracy: 0.5000\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.8958\n",
      "Epoch 830: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.2653 - accuracy: 0.8958 - val_loss: 1.1637 - val_accuracy: 0.5000\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2652 - accuracy: 0.9167\n",
      "Epoch 831: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.2652 - accuracy: 0.9167 - val_loss: 1.1667 - val_accuracy: 0.5000\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2651 - accuracy: 0.8958\n",
      "Epoch 832: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.2651 - accuracy: 0.8958 - val_loss: 1.1644 - val_accuracy: 0.5000\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2649 - accuracy: 0.9167\n",
      "Epoch 833: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 0.2649 - accuracy: 0.9167 - val_loss: 1.1682 - val_accuracy: 0.5000\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.8958\n",
      "Epoch 834: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 0.2648 - accuracy: 0.8958 - val_loss: 1.1648 - val_accuracy: 0.5000\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.9167\n",
      "Epoch 835: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.2647 - accuracy: 0.9167 - val_loss: 1.1686 - val_accuracy: 0.5000\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2646 - accuracy: 0.8958\n",
      "Epoch 836: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 0.2646 - accuracy: 0.8958 - val_loss: 1.1662 - val_accuracy: 0.5000\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2645 - accuracy: 0.9167\n",
      "Epoch 837: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.2645 - accuracy: 0.9167 - val_loss: 1.1698 - val_accuracy: 0.5000\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2644 - accuracy: 0.8958\n",
      "Epoch 838: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.2644 - accuracy: 0.8958 - val_loss: 1.1664 - val_accuracy: 0.5000\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2643 - accuracy: 0.9167\n",
      "Epoch 839: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.2643 - accuracy: 0.9167 - val_loss: 1.1705 - val_accuracy: 0.5000\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2642 - accuracy: 0.8958\n",
      "Epoch 840: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.2642 - accuracy: 0.8958 - val_loss: 1.1668 - val_accuracy: 0.5000\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9167\n",
      "Epoch 841: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.2640 - accuracy: 0.9167 - val_loss: 1.1713 - val_accuracy: 0.5000\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2639 - accuracy: 0.8958\n",
      "Epoch 842: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.2639 - accuracy: 0.8958 - val_loss: 1.1676 - val_accuracy: 0.5000\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2638 - accuracy: 0.9167\n",
      "Epoch 843: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.2638 - accuracy: 0.9167 - val_loss: 1.1718 - val_accuracy: 0.5000\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2637 - accuracy: 0.8958\n",
      "Epoch 844: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.2637 - accuracy: 0.8958 - val_loss: 1.1685 - val_accuracy: 0.5000\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.9167\n",
      "Epoch 845: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 0.2635 - accuracy: 0.9167 - val_loss: 1.1724 - val_accuracy: 0.5000\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2634 - accuracy: 0.8958\n",
      "Epoch 846: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.2634 - accuracy: 0.8958 - val_loss: 1.1694 - val_accuracy: 0.5000\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2633 - accuracy: 0.9167\n",
      "Epoch 847: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.2633 - accuracy: 0.9167 - val_loss: 1.1729 - val_accuracy: 0.5000\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2632 - accuracy: 0.8958\n",
      "Epoch 848: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.2632 - accuracy: 0.8958 - val_loss: 1.1704 - val_accuracy: 0.5000\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2631 - accuracy: 0.9167\n",
      "Epoch 849: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.2631 - accuracy: 0.9167 - val_loss: 1.1737 - val_accuracy: 0.5000\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2629 - accuracy: 0.9167\n",
      "Epoch 850: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.2629 - accuracy: 0.9167 - val_loss: 1.1710 - val_accuracy: 0.5000\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.9167\n",
      "Epoch 851: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.2628 - accuracy: 0.9167 - val_loss: 1.1746 - val_accuracy: 0.5000\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2627 - accuracy: 0.9167\n",
      "Epoch 852: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.2627 - accuracy: 0.9167 - val_loss: 1.1715 - val_accuracy: 0.5000\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2626 - accuracy: 0.9167\n",
      "Epoch 853: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.2626 - accuracy: 0.9167 - val_loss: 1.1753 - val_accuracy: 0.5000\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.9167\n",
      "Epoch 854: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.2625 - accuracy: 0.9167 - val_loss: 1.1726 - val_accuracy: 0.5000\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2624 - accuracy: 0.9167\n",
      "Epoch 855: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.2624 - accuracy: 0.9167 - val_loss: 1.1760 - val_accuracy: 0.5000\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2623 - accuracy: 0.9167\n",
      "Epoch 856: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.2623 - accuracy: 0.9167 - val_loss: 1.1728 - val_accuracy: 0.5000\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2622 - accuracy: 0.9167\n",
      "Epoch 857: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.2622 - accuracy: 0.9167 - val_loss: 1.1772 - val_accuracy: 0.5000\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2621 - accuracy: 0.8958\n",
      "Epoch 858: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 0.2621 - accuracy: 0.8958 - val_loss: 1.1734 - val_accuracy: 0.5000\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2619 - accuracy: 0.9167\n",
      "Epoch 859: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.2619 - accuracy: 0.9167 - val_loss: 1.1781 - val_accuracy: 0.5000\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.9167\n",
      "Epoch 860: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.2618 - accuracy: 0.9167 - val_loss: 1.1742 - val_accuracy: 0.5000\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2617 - accuracy: 0.9167\n",
      "Epoch 861: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.2617 - accuracy: 0.9167 - val_loss: 1.1782 - val_accuracy: 0.5000\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2616 - accuracy: 0.9167\n",
      "Epoch 862: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.2616 - accuracy: 0.9167 - val_loss: 1.1746 - val_accuracy: 0.5000\n",
      "Epoch 863/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.9167\n",
      "Epoch 863: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.2615 - accuracy: 0.9167 - val_loss: 1.1792 - val_accuracy: 0.5000\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2613 - accuracy: 0.9167\n",
      "Epoch 864: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.2613 - accuracy: 0.9167 - val_loss: 1.1757 - val_accuracy: 0.5000\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2612 - accuracy: 0.9167\n",
      "Epoch 865: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.2612 - accuracy: 0.9167 - val_loss: 1.1794 - val_accuracy: 0.5000\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2611 - accuracy: 0.9167\n",
      "Epoch 866: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.2611 - accuracy: 0.9167 - val_loss: 1.1767 - val_accuracy: 0.5000\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2610 - accuracy: 0.9167\n",
      "Epoch 867: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.2610 - accuracy: 0.9167 - val_loss: 1.1804 - val_accuracy: 0.5000\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2609 - accuracy: 0.9167\n",
      "Epoch 868: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.2609 - accuracy: 0.9167 - val_loss: 1.1770 - val_accuracy: 0.5000\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2608 - accuracy: 0.9167\n",
      "Epoch 869: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.2608 - accuracy: 0.9167 - val_loss: 1.1814 - val_accuracy: 0.5000\n",
      "Epoch 870/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.9167\n",
      "Epoch 870: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.2607 - accuracy: 0.9167 - val_loss: 1.1779 - val_accuracy: 0.5000\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2605 - accuracy: 0.9167\n",
      "Epoch 871: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.2605 - accuracy: 0.9167 - val_loss: 1.1817 - val_accuracy: 0.5000\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2604 - accuracy: 0.9167\n",
      "Epoch 872: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.2604 - accuracy: 0.9167 - val_loss: 1.1785 - val_accuracy: 0.5000\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2603 - accuracy: 0.9167\n",
      "Epoch 873: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.2603 - accuracy: 0.9167 - val_loss: 1.1824 - val_accuracy: 0.5000\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2602 - accuracy: 0.9167\n",
      "Epoch 874: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.2602 - accuracy: 0.9167 - val_loss: 1.1792 - val_accuracy: 0.5000\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2601 - accuracy: 0.9167\n",
      "Epoch 875: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 0.2601 - accuracy: 0.9167 - val_loss: 1.1829 - val_accuracy: 0.5000\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2599 - accuracy: 0.9167\n",
      "Epoch 876: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.2599 - accuracy: 0.9167 - val_loss: 1.1799 - val_accuracy: 0.5000\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2598 - accuracy: 0.9167\n",
      "Epoch 877: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.2598 - accuracy: 0.9167 - val_loss: 1.1847 - val_accuracy: 0.5000\n",
      "Epoch 878/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2598 - accuracy: 0.9167\n",
      "Epoch 878: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.2598 - accuracy: 0.9167 - val_loss: 1.1806 - val_accuracy: 0.5000\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2596 - accuracy: 0.9167\n",
      "Epoch 879: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 0.2596 - accuracy: 0.9167 - val_loss: 1.1848 - val_accuracy: 0.5000\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2595 - accuracy: 0.9167\n",
      "Epoch 880: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2595 - accuracy: 0.9167 - val_loss: 1.1812 - val_accuracy: 0.5000\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.9167\n",
      "Epoch 881: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.2594 - accuracy: 0.9167 - val_loss: 1.1855 - val_accuracy: 0.5000\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2593 - accuracy: 0.9167\n",
      "Epoch 882: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.2593 - accuracy: 0.9167 - val_loss: 1.1823 - val_accuracy: 0.5000\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2592 - accuracy: 0.9167\n",
      "Epoch 883: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.2592 - accuracy: 0.9167 - val_loss: 1.1864 - val_accuracy: 0.5000\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2591 - accuracy: 0.9167\n",
      "Epoch 884: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.2591 - accuracy: 0.9167 - val_loss: 1.1830 - val_accuracy: 0.5000\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2589 - accuracy: 0.9167\n",
      "Epoch 885: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.2589 - accuracy: 0.9167 - val_loss: 1.1874 - val_accuracy: 0.5000\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2589 - accuracy: 0.9167\n",
      "Epoch 886: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.2589 - accuracy: 0.9167 - val_loss: 1.1834 - val_accuracy: 0.5000\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2587 - accuracy: 0.9167\n",
      "Epoch 887: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.2587 - accuracy: 0.9167 - val_loss: 1.1875 - val_accuracy: 0.5000\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2586 - accuracy: 0.9167\n",
      "Epoch 888: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.2586 - accuracy: 0.9167 - val_loss: 1.1843 - val_accuracy: 0.5000\n",
      "Epoch 889/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2585 - accuracy: 0.9167\n",
      "Epoch 889: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.2585 - accuracy: 0.9167 - val_loss: 1.1885 - val_accuracy: 0.5000\n",
      "Epoch 890/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 0.9167\n",
      "Epoch 890: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.2584 - accuracy: 0.9167 - val_loss: 1.1853 - val_accuracy: 0.5000\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2583 - accuracy: 0.9167\n",
      "Epoch 891: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.2583 - accuracy: 0.9167 - val_loss: 1.1891 - val_accuracy: 0.5000\n",
      "Epoch 892/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2582 - accuracy: 0.9167\n",
      "Epoch 892: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.2582 - accuracy: 0.9167 - val_loss: 1.1855 - val_accuracy: 0.5000\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9167\n",
      "Epoch 893: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.2580 - accuracy: 0.9167 - val_loss: 1.1901 - val_accuracy: 0.5000\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.9167\n",
      "Epoch 894: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2579 - accuracy: 0.9167 - val_loss: 1.1869 - val_accuracy: 0.5000\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.9167\n",
      "Epoch 895: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.2578 - accuracy: 0.9167 - val_loss: 1.1906 - val_accuracy: 0.5000\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.9167\n",
      "Epoch 896: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.2577 - accuracy: 0.9167 - val_loss: 1.1876 - val_accuracy: 0.5000\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.9167\n",
      "Epoch 897: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 0.2576 - accuracy: 0.9167 - val_loss: 1.1911 - val_accuracy: 0.5000\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.9167\n",
      "Epoch 898: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.2574 - accuracy: 0.9167 - val_loss: 1.1883 - val_accuracy: 0.5000\n",
      "Epoch 899/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.9167\n",
      "Epoch 899: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.2573 - accuracy: 0.9167 - val_loss: 1.1923 - val_accuracy: 0.5000\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.9167\n",
      "Epoch 900: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.2573 - accuracy: 0.9167 - val_loss: 1.1887 - val_accuracy: 0.5000\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2572 - accuracy: 0.9167\n",
      "Epoch 901: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.2572 - accuracy: 0.9167 - val_loss: 1.1929 - val_accuracy: 0.5000\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2571 - accuracy: 0.9167\n",
      "Epoch 902: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.2571 - accuracy: 0.9167 - val_loss: 1.1897 - val_accuracy: 0.5000\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.9167\n",
      "Epoch 903: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.2569 - accuracy: 0.9167 - val_loss: 1.1941 - val_accuracy: 0.5000\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 0.9167\n",
      "Epoch 904: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.2568 - accuracy: 0.9167 - val_loss: 1.1904 - val_accuracy: 0.5000\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2567 - accuracy: 0.9167\n",
      "Epoch 905: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.2567 - accuracy: 0.9167 - val_loss: 1.1950 - val_accuracy: 0.5000\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2566 - accuracy: 0.9167\n",
      "Epoch 906: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.2566 - accuracy: 0.9167 - val_loss: 1.1909 - val_accuracy: 0.5000\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.9167\n",
      "Epoch 907: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.2565 - accuracy: 0.9167 - val_loss: 1.1955 - val_accuracy: 0.5000\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2564 - accuracy: 0.9167\n",
      "Epoch 908: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.2564 - accuracy: 0.9167 - val_loss: 1.1916 - val_accuracy: 0.5000\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2563 - accuracy: 0.9167\n",
      "Epoch 909: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.2563 - accuracy: 0.9167 - val_loss: 1.1965 - val_accuracy: 0.5000\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2562 - accuracy: 0.9167\n",
      "Epoch 910: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.2562 - accuracy: 0.9167 - val_loss: 1.1925 - val_accuracy: 0.5000\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.9167\n",
      "Epoch 911: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 0.2560 - accuracy: 0.9167 - val_loss: 1.1970 - val_accuracy: 0.5000\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2559 - accuracy: 0.9167\n",
      "Epoch 912: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.2559 - accuracy: 0.9167 - val_loss: 1.1934 - val_accuracy: 0.5000\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.9167\n",
      "Epoch 913: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.2558 - accuracy: 0.9167 - val_loss: 1.1970 - val_accuracy: 0.5000\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.9167\n",
      "Epoch 914: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.2557 - accuracy: 0.9167 - val_loss: 1.1943 - val_accuracy: 0.5000\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.9167\n",
      "Epoch 915: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.2556 - accuracy: 0.9167 - val_loss: 1.1981 - val_accuracy: 0.5000\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.9167\n",
      "Epoch 916: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.2554 - accuracy: 0.9167 - val_loss: 1.1952 - val_accuracy: 0.5000\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2553 - accuracy: 0.9167\n",
      "Epoch 917: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.2553 - accuracy: 0.9167 - val_loss: 1.1980 - val_accuracy: 0.5000\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2552 - accuracy: 0.9167\n",
      "Epoch 918: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.2552 - accuracy: 0.9167 - val_loss: 1.1959 - val_accuracy: 0.5000\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2551 - accuracy: 0.9167\n",
      "Epoch 919: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.2551 - accuracy: 0.9167 - val_loss: 1.1994 - val_accuracy: 0.5000\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2550 - accuracy: 0.9167\n",
      "Epoch 920: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.2550 - accuracy: 0.9167 - val_loss: 1.1965 - val_accuracy: 0.5000\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.9167\n",
      "Epoch 921: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.2549 - accuracy: 0.9167 - val_loss: 1.2003 - val_accuracy: 0.5000\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.9167\n",
      "Epoch 922: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.2548 - accuracy: 0.9167 - val_loss: 1.1968 - val_accuracy: 0.5000\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.9167\n",
      "Epoch 923: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.2547 - accuracy: 0.9167 - val_loss: 1.2011 - val_accuracy: 0.5000\n",
      "Epoch 924/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2546 - accuracy: 0.9167\n",
      "Epoch 924: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.2546 - accuracy: 0.9167 - val_loss: 1.1973 - val_accuracy: 0.5000\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.9167\n",
      "Epoch 925: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.2545 - accuracy: 0.9167 - val_loss: 1.2016 - val_accuracy: 0.5000\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2544 - accuracy: 0.9167\n",
      "Epoch 926: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.2544 - accuracy: 0.9167 - val_loss: 1.1984 - val_accuracy: 0.5000\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9167\n",
      "Epoch 927: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.2543 - accuracy: 0.9167 - val_loss: 1.2023 - val_accuracy: 0.5000\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2541 - accuracy: 0.9167\n",
      "Epoch 928: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.2541 - accuracy: 0.9167 - val_loss: 1.1991 - val_accuracy: 0.5000\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2541 - accuracy: 0.9167\n",
      "Epoch 929: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.2541 - accuracy: 0.9167 - val_loss: 1.2034 - val_accuracy: 0.5000\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2540 - accuracy: 0.9167\n",
      "Epoch 930: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.2540 - accuracy: 0.9167 - val_loss: 1.1997 - val_accuracy: 0.5000\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.9167\n",
      "Epoch 931: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 0.2539 - accuracy: 0.9167 - val_loss: 1.2041 - val_accuracy: 0.5000\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2538 - accuracy: 0.9167\n",
      "Epoch 932: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.2538 - accuracy: 0.9167 - val_loss: 1.2003 - val_accuracy: 0.5000\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.9167\n",
      "Epoch 933: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.2536 - accuracy: 0.9167 - val_loss: 1.2047 - val_accuracy: 0.5000\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.9167\n",
      "Epoch 934: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.2535 - accuracy: 0.9167 - val_loss: 1.2012 - val_accuracy: 0.5000\n",
      "Epoch 935/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.9167\n",
      "Epoch 935: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.2534 - accuracy: 0.9167 - val_loss: 1.2053 - val_accuracy: 0.5000\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 0.9167\n",
      "Epoch 936: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.2533 - accuracy: 0.9167 - val_loss: 1.2018 - val_accuracy: 0.5000\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.9167\n",
      "Epoch 937: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.2532 - accuracy: 0.9167 - val_loss: 1.2059 - val_accuracy: 0.5000\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2531 - accuracy: 0.9167\n",
      "Epoch 938: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.2531 - accuracy: 0.9167 - val_loss: 1.2028 - val_accuracy: 0.5000\n",
      "Epoch 939/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.9167\n",
      "Epoch 939: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.2529 - accuracy: 0.9167 - val_loss: 1.2068 - val_accuracy: 0.5000\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.9167\n",
      "Epoch 940: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.2529 - accuracy: 0.9167 - val_loss: 1.2030 - val_accuracy: 0.5000\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2527 - accuracy: 0.9167\n",
      "Epoch 941: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 0.2527 - accuracy: 0.9167 - val_loss: 1.2072 - val_accuracy: 0.5000\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.9167\n",
      "Epoch 942: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.2526 - accuracy: 0.9167 - val_loss: 1.2041 - val_accuracy: 0.5000\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.9167\n",
      "Epoch 943: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.2525 - accuracy: 0.9167 - val_loss: 1.2076 - val_accuracy: 0.5000\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2524 - accuracy: 0.9167\n",
      "Epoch 944: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.2524 - accuracy: 0.9167 - val_loss: 1.2049 - val_accuracy: 0.5000\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2523 - accuracy: 0.9167\n",
      "Epoch 945: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.2523 - accuracy: 0.9167 - val_loss: 1.2086 - val_accuracy: 0.5000\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.9167\n",
      "Epoch 946: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.2522 - accuracy: 0.9167 - val_loss: 1.2053 - val_accuracy: 0.5000\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.9167\n",
      "Epoch 947: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.2521 - accuracy: 0.9167 - val_loss: 1.2096 - val_accuracy: 0.5000\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.9167\n",
      "Epoch 948: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.2519 - accuracy: 0.9167 - val_loss: 1.2064 - val_accuracy: 0.5000\n",
      "Epoch 949/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.9167\n",
      "Epoch 949: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2519 - accuracy: 0.9167 - val_loss: 1.2098 - val_accuracy: 0.5000\n",
      "Epoch 950/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.9167\n",
      "Epoch 950: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 469ms/step - loss: 0.2518 - accuracy: 0.9167 - val_loss: 1.2070 - val_accuracy: 0.5000\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2517 - accuracy: 0.9167\n",
      "Epoch 951: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.2517 - accuracy: 0.9167 - val_loss: 1.2107 - val_accuracy: 0.5000\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.9167\n",
      "Epoch 952: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.2516 - accuracy: 0.9167 - val_loss: 1.2077 - val_accuracy: 0.5000\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.9167\n",
      "Epoch 953: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.2514 - accuracy: 0.9167 - val_loss: 1.2117 - val_accuracy: 0.5000\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.9167\n",
      "Epoch 954: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.2514 - accuracy: 0.9167 - val_loss: 1.2080 - val_accuracy: 0.5000\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.9167\n",
      "Epoch 955: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 0.2513 - accuracy: 0.9167 - val_loss: 1.2127 - val_accuracy: 0.5000\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.9167\n",
      "Epoch 956: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.2512 - accuracy: 0.9167 - val_loss: 1.2090 - val_accuracy: 0.5000\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.9167\n",
      "Epoch 957: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.2510 - accuracy: 0.9167 - val_loss: 1.2128 - val_accuracy: 0.5000\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.9167\n",
      "Epoch 958: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 0.2509 - accuracy: 0.9167 - val_loss: 1.2095 - val_accuracy: 0.5000\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.9167\n",
      "Epoch 959: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.2509 - accuracy: 0.9167 - val_loss: 1.2140 - val_accuracy: 0.5000\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.9167\n",
      "Epoch 960: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.2508 - accuracy: 0.9167 - val_loss: 1.2097 - val_accuracy: 0.5000\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 0.9167\n",
      "Epoch 961: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.2507 - accuracy: 0.9167 - val_loss: 1.2147 - val_accuracy: 0.5000\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9167\n",
      "Epoch 962: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.2506 - accuracy: 0.9167 - val_loss: 1.2105 - val_accuracy: 0.5000\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.9167\n",
      "Epoch 963: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 0.2505 - accuracy: 0.9167 - val_loss: 1.2156 - val_accuracy: 0.5000\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.9167\n",
      "Epoch 964: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.2504 - accuracy: 0.9167 - val_loss: 1.2115 - val_accuracy: 0.5000\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.9167\n",
      "Epoch 965: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2502 - accuracy: 0.9167 - val_loss: 1.2158 - val_accuracy: 0.5000\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.9167\n",
      "Epoch 966: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.2501 - accuracy: 0.9167 - val_loss: 1.2118 - val_accuracy: 0.5000\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.9167\n",
      "Epoch 967: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.2500 - accuracy: 0.9167 - val_loss: 1.2165 - val_accuracy: 0.5000\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.9167\n",
      "Epoch 968: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.2499 - accuracy: 0.9167 - val_loss: 1.2129 - val_accuracy: 0.5000\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.9167\n",
      "Epoch 969: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 447ms/step - loss: 0.2498 - accuracy: 0.9167 - val_loss: 1.2173 - val_accuracy: 0.5000\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.9167\n",
      "Epoch 970: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2497 - accuracy: 0.9167 - val_loss: 1.2139 - val_accuracy: 0.5000\n",
      "Epoch 971/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.9167\n",
      "Epoch 971: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.2496 - accuracy: 0.9167 - val_loss: 1.2178 - val_accuracy: 0.5000\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.9167\n",
      "Epoch 972: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.2494 - accuracy: 0.9167 - val_loss: 1.2142 - val_accuracy: 0.5000\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.9167\n",
      "Epoch 973: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.2494 - accuracy: 0.9167 - val_loss: 1.2188 - val_accuracy: 0.5000\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.9167\n",
      "Epoch 974: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.2493 - accuracy: 0.9167 - val_loss: 1.2149 - val_accuracy: 0.5000\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.9167\n",
      "Epoch 975: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.2491 - accuracy: 0.9167 - val_loss: 1.2192 - val_accuracy: 0.5000\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.9167\n",
      "Epoch 976: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.2491 - accuracy: 0.9167 - val_loss: 1.2156 - val_accuracy: 0.5000\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.9167\n",
      "Epoch 977: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.2489 - accuracy: 0.9167 - val_loss: 1.2194 - val_accuracy: 0.5000\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.9167\n",
      "Epoch 978: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2488 - accuracy: 0.9167 - val_loss: 1.2164 - val_accuracy: 0.5000\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.9167\n",
      "Epoch 979: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.2487 - accuracy: 0.9167 - val_loss: 1.2203 - val_accuracy: 0.5000\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.9167\n",
      "Epoch 980: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.2486 - accuracy: 0.9167 - val_loss: 1.2173 - val_accuracy: 0.5000\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.9167\n",
      "Epoch 981: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.2485 - accuracy: 0.9167 - val_loss: 1.2211 - val_accuracy: 0.5000\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.9167\n",
      "Epoch 982: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 0.2484 - accuracy: 0.9167 - val_loss: 1.2177 - val_accuracy: 0.5000\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.9167\n",
      "Epoch 983: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.2483 - accuracy: 0.9167 - val_loss: 1.2215 - val_accuracy: 0.5000\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.9167\n",
      "Epoch 984: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2482 - accuracy: 0.9167 - val_loss: 1.2184 - val_accuracy: 0.5000\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.9167\n",
      "Epoch 985: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.2480 - accuracy: 0.9167 - val_loss: 1.2220 - val_accuracy: 0.5000\n",
      "Epoch 986/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.9167\n",
      "Epoch 986: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.2480 - accuracy: 0.9167 - val_loss: 1.2193 - val_accuracy: 0.5000\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.9167\n",
      "Epoch 987: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.2478 - accuracy: 0.9167 - val_loss: 1.2230 - val_accuracy: 0.5000\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.9167\n",
      "Epoch 988: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.2477 - accuracy: 0.9167 - val_loss: 1.2202 - val_accuracy: 0.5000\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2476 - accuracy: 0.9167\n",
      "Epoch 989: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.2476 - accuracy: 0.9167 - val_loss: 1.2231 - val_accuracy: 0.5000\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.9167\n",
      "Epoch 990: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.2475 - accuracy: 0.9167 - val_loss: 1.2204 - val_accuracy: 0.5000\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.9167\n",
      "Epoch 991: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.2475 - accuracy: 0.9167 - val_loss: 1.2248 - val_accuracy: 0.5000\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.9167\n",
      "Epoch 992: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.2474 - accuracy: 0.9167 - val_loss: 1.2213 - val_accuracy: 0.5000\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.9167\n",
      "Epoch 993: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 0.2473 - accuracy: 0.9167 - val_loss: 1.2256 - val_accuracy: 0.5000\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.9167\n",
      "Epoch 994: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2472 - accuracy: 0.9167 - val_loss: 1.2215 - val_accuracy: 0.5000\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.9167\n",
      "Epoch 995: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.2471 - accuracy: 0.9167 - val_loss: 1.2264 - val_accuracy: 0.5000\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2470 - accuracy: 0.9167\n",
      "Epoch 996: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.2470 - accuracy: 0.9167 - val_loss: 1.2225 - val_accuracy: 0.5000\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.9167\n",
      "Epoch 997: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.2468 - accuracy: 0.9167 - val_loss: 1.2266 - val_accuracy: 0.5000\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.9167\n",
      "Epoch 998: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.2468 - accuracy: 0.9167 - val_loss: 1.2229 - val_accuracy: 0.5000\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.9167\n",
      "Epoch 999: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.2467 - accuracy: 0.9167 - val_loss: 1.2280 - val_accuracy: 0.5000\n",
      "Epoch 1000/1000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.9167\n",
      "Epoch 1000: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.2466 - accuracy: 0.9167 - val_loss: 1.2235 - val_accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "Best Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = SGD(learning_rate = learning_rate)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(512, activation='relu', input_dim=8))\n",
    "model1.add(Dense(512, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Create a ModelCheckpoint callback to save the best model\n",
    "checkpoint = ModelCheckpoint('best_model_T2.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "# Train the model with the callback\n",
    "model1.fit(X_train, y_train, epochs=1000, batch_size=62, validation_data=(X_test, y_test), callbacks=[checkpoint])\n",
    "\n",
    "# Load the best model\n",
    "model1 = load_model('best_model_T2.h5')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model1.predict(X_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "ann_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Best Accuracy:\", ann_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "Precision: 0.7777777777777778\n",
      "Recall: 0.875\n",
      "F1 score: 0.823529411764706\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming you have already obtained the predictions for your test dataset using your model1\n",
    "# Replace `y_test` with the true labels and `y_pred_probs` and `y_pred_classes` with your model's \n",
    "\n",
    "# Assuming you have obtained the predicted probabilities and class labels as follows\n",
    "y_pred_probs = model1.predict(X_test, verbose=0)\n",
    "\n",
    "# Convert probabilities to binary class predictions using a threshold (e.g., 0.5)\n",
    "threshold = 0.5\n",
    "y_pred_classes = (y_pred_probs > threshold).astype(int)\n",
    "\n",
    "\n",
    "# Reduce the 2D prediction arrays to 1D arrays\n",
    "#y_pred_probs = y_pred_probs[:, 0]\n",
    "#y_pred_classes = y_pred_classes[:, 0]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "precision = precision_score(y_test, y_pred_classes)\n",
    "recall = recall_score(y_test, y_pred_classes)\n",
    "f1 = f1_score(y_test, y_pred_classes)\n",
    "\n",
    "# Print the results\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance scores for Random Forest:\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.75\n",
      "F1-score: 0.7058823529411765\n",
      "Accuracy 0.5833333333333334\n",
      "\n",
      "Performance scores for SVM:\n",
      "Precision: 0.6\n",
      "Recall: 0.75\n",
      "F1-score: 0.6666666666666665\n",
      "Accuracy 0.75\n",
      "\n",
      "Performance scores for Logistic Regression:\n",
      "Precision: 0.5555555555555556\n",
      "Recall: 0.625\n",
      "F1-score: 0.5882352941176471\n",
      "Accuracy 0.4166666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Make predictions on the test set\n",
    "predicted_labels_rf = rf_classifier.predict(X_test)\n",
    "predicted_labels_svm = svm_classifier.predict(X_test)\n",
    "predicted_labels_lr = logreg_classifier.predict(X_test)\n",
    "# Calculate precision for each model\n",
    "precision_rf = precision_score(y_test, predicted_labels_rf)\n",
    "precision_svm = precision_score(y_test, predicted_labels_svm)\n",
    "precision_lr = precision_score(y_test, predicted_labels_lr)\n",
    "# Calculate recall for each model\n",
    "recall_rf = recall_score(y_test, predicted_labels_rf)\n",
    "recall_svm = recall_score(y_test, predicted_labels_svm)\n",
    "recall_lr = recall_score(y_test, predicted_labels_lr)\n",
    "F1_rf = f1_score(y_test, predicted_labels_rf)\n",
    "F1_svm = f1_score(y_test, predicted_labels_svm)\n",
    "F1_lr = f1_score(y_test, predicted_labels_lr)\n",
    "models = ['Random Forest', 'SVM', 'Logistic Regression']\n",
    "precision_scores = [precision_rf,  precision_svm, precision_lr]\n",
    "recall_scores = [recall_rf, recall_svm, recall_lr]\n",
    "accuracy_score = [rf_accuracy, svm_accuracy, lr_accuracy]\n",
    "f1_scores = [F1_rf, F1_svm, F1_lr]\n",
    "# Print the performance scores\n",
    "for i in range(len(models)):\n",
    "    print(f\"Performance scores for {models[i]}:\")\n",
    "    print(\"Precision:\", precision_scores[i])\n",
    "    print(\"Recall:\", recall_scores[i])\n",
    "    print(\"F1-score:\", f1_scores[i])\n",
    "    print(\"Accuracy\", accuracy_score[i])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the weights of the first layer\n",
    "weights = model1.layers[0].get_weights()[0]\n",
    "\n",
    "# Calculate the absolute sum of weights for each feature\n",
    "feature_importances = np.abs(weights).sum(axis=1)\n",
    "\n",
    "# Create a list of feature names\n",
    "feature_names = [\"femaleTeamMembersPercent\", \"meetingHoursTotal\", \"codingDeliverablesHoursTotal\", \"globalLeadAdminHoursTotal\", \"commitCount\", \"issueCount\", \"teamLeadGender\",\"teamDistribution_encoded\"]\n",
    "\n",
    "# Sort the feature importances and feature names in descending order\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "sorted_feature_importances = feature_importances[sorted_indices]\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(sorted_feature_importances)), sorted_feature_importances, align='center')\n",
    "plt.xticks(range(len(sorted_feature_importances)), sorted_feature_names, rotation=45)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Feature Importance')\n",
    "plt.title('Feature Importances based on Weights')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 126 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n126 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py\", line 1491, in fit\n    super().fit(X=X, y=y, sample_weight=sample_weight, **kwargs)\n  File \"c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py\", line 760, in fit\n    self._fit(\n  File \"c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py\", line 915, in _fit\n    X, y = self._initialize(X, y)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py\", line 852, in _initialize\n    self.model_ = self._build_keras_model()\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py\", line 429, in _build_keras_model\n    model = final_build_fn(**build_params)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\surface\\AppData\\Local\\Temp\\ipykernel_13828\\1968642639.py\", line 22, in create_model\n    opt = Adam(learning_rate=learning_rate)\n                             ^^^^^^^^^^^^^\nNameError: name 'learning_rate' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m optimizer_model \u001b[39m=\u001b[39m KerasClassifier(build_fn\u001b[39m=\u001b[39mcreate_model, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     61\u001b[0m optimizer_halving_grid \u001b[39m=\u001b[39m HalvingGridSearchCV(estimator\u001b[39m=\u001b[39moptimizer_model, param_grid\u001b[39m=\u001b[39moptimizer_grid, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m optimizer_halving_result \u001b[39m=\u001b[39m optimizer_halving_grid\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     64\u001b[0m \u001b[39m# Print the best parameters and the corresponding accuracy for optimizers\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Parameters (Optimizers): \u001b[39m\u001b[39m\"\u001b[39m, optimizer_halving_result\u001b[39m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py:261\u001b[0m, in \u001b[0;36mBaseSuccessiveHalving.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_input_parameters(\n\u001b[0;32m    254\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m    255\u001b[0m     y\u001b[39m=\u001b[39my,\n\u001b[0;32m    256\u001b[0m     groups\u001b[39m=\u001b[39mgroups,\n\u001b[0;32m    257\u001b[0m )\n\u001b[0;32m    259\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_samples_orig \u001b[39m=\u001b[39m _num_samples(X)\n\u001b[1;32m--> 261\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y\u001b[39m=\u001b[39;49my, groups\u001b[39m=\u001b[39;49mgroups, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    263\u001b[0m \u001b[39m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_score_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv_results_[\u001b[39m\"\u001b[39m\u001b[39mmean_test_score\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_index_]\n",
      "File \u001b[1;32mc:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py:366\u001b[0m, in \u001b[0;36mBaseSuccessiveHalving._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m    359\u001b[0m     cv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checked_cv_orig\n\u001b[0;32m    361\u001b[0m more_results \u001b[39m=\u001b[39m {\n\u001b[0;32m    362\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39miter\u001b[39m\u001b[39m\"\u001b[39m: [itr] \u001b[39m*\u001b[39m n_candidates,\n\u001b[0;32m    363\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mn_resources\u001b[39m\u001b[39m\"\u001b[39m: [n_resources] \u001b[39m*\u001b[39m n_candidates,\n\u001b[0;32m    364\u001b[0m }\n\u001b[1;32m--> 366\u001b[0m results \u001b[39m=\u001b[39m evaluate_candidates(\n\u001b[0;32m    367\u001b[0m     candidate_params, cv, more_results\u001b[39m=\u001b[39;49mmore_results\n\u001b[0;32m    368\u001b[0m )\n\u001b[0;32m    370\u001b[0m n_candidates_to_keep \u001b[39m=\u001b[39m ceil(n_candidates \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfactor)\n\u001b[0;32m    371\u001b[0m candidate_params \u001b[39m=\u001b[39m _top_k(results, n_candidates_to_keep, itr)\n",
      "File \u001b[1;32mc:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:852\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[0;32m    846\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    849\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[0;32m    850\u001b[0m     )\n\u001b[1;32m--> 852\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[0;32m    854\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 126 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n126 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py\", line 1491, in fit\n    super().fit(X=X, y=y, sample_weight=sample_weight, **kwargs)\n  File \"c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py\", line 760, in fit\n    self._fit(\n  File \"c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py\", line 915, in _fit\n    X, y = self._initialize(X, y)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py\", line 852, in _initialize\n    self.model_ = self._build_keras_model()\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\surface\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py\", line 429, in _build_keras_model\n    model = final_build_fn(**build_params)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\surface\\AppData\\Local\\Temp\\ipykernel_13828\\1968642639.py\", line 22, in create_model\n    opt = Adam(learning_rate=learning_rate)\n                             ^^^^^^^^^^^^^\nNameError: name 'learning_rate' is not defined\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import Adam, RMSprop, SGD, Adagrad, Adadelta, Adamax, Nadam\n",
    "from keras.activations import relu, sigmoid, softmax, tanh, linear, elu, selu, softplus, softsign, hard_sigmoid, exponential\n",
    "\n",
    "# Define a function to create your model (to be used by GridSearchCV)\n",
    "def create_model(optimizer='adam', activation='relu', dropout_rate=0.0, hidden_layers=1, neurons=256, kernel_regularizer=None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation=activation, input_dim=8, kernel_regularizer=kernel_regularizer))\n",
    "    \n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(Dense(neurons, activation=activation, kernel_regularizer=kernel_regularizer))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adagrad':\n",
    "        opt = Adagrad(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adadelta':\n",
    "        opt = Adadelta(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adamax':\n",
    "        opt = Adamax(learning_rate=learning_rate)\n",
    "    elif optimizer == 'nadam':\n",
    "        opt = Nadam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer.\")\n",
    "\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "optimizer_grid = {\n",
    "    'optimizer': ['adam', 'rmsprop', 'sgd', 'adagrad', 'adadelta', 'adamax', 'nadam'],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [100, 200, 300],\n",
    "}\n",
    "\n",
    "regularization_grid = {\n",
    "    'kernel_regularizer': [None, l1(0.01), l2(0.01)]\n",
    "}\n",
    "\n",
    "other_hyperparameters_grid = {\n",
    "    'activation': ['relu', 'sigmoid', 'softmax', 'tanh', 'linear', 'elu', 'selu', 'softplus', 'softsign', 'hard_sigmoid', 'exponential'],\n",
    "    'dropout_rate': [0.0, 0.2, 0.4],\n",
    "    'hidden_layers': [1, 2, 3],\n",
    "    'neurons': [256, 512, 1024]\n",
    "}\n",
    "\n",
    "# Create the KerasClassifier wrapper for optimizer grid search\n",
    "optimizer_model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "optimizer_halving_grid = HalvingGridSearchCV(estimator=optimizer_model, param_grid=optimizer_grid, cv=3)\n",
    "optimizer_halving_result = optimizer_halving_grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding accuracy for optimizers\n",
    "print(\"Best Parameters (Optimizers): \", optimizer_halving_result.best_params_)\n",
    "print(\"Best Accuracy (Optimizers): \", optimizer_halving_result.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the model with the best optimizer found\n",
    "model.set_params(**optimizer_halving_result.best_params_)\n",
    "\n",
    "# Perform halving grid search for regularization\n",
    "regularization_halving_grid = HalvingGridSearchCV(estimator=model, param_grid=regularization_grid, cv=3)\n",
    "regularization_halving_result = regularization_halving_grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding accuracy for regularization\n",
    "print(\"Best Parameters (Regularization): \", regularization_halving_result.best_params_)\n",
    "print(\"Best Accuracy (Regularization): \", regularization_halving_result.best_score_)\n",
    "\n",
    "# Update the model with the best regularization found\n",
    "model.set_params(**regularization_halving_result.best_params_)\n",
    "\n",
    "# Perform halving grid search for other hyperparameters\n",
    "other_hyperparameters_halving_grid = HalvingGridSearchCV(estimator=model, param_grid=other_hyperparameters_grid, cv=3)\n",
    "other_hyperparameters_halving_result = other_hyperparameters_halving_grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding accuracy for other hyperparameters\n",
    "print(\"Best Parameters (Other Hyperparameters): \", other_hyperparameters_halving_result.best_params_)\n",
    "print(\"Best Accuracy (Other Hyperparameters): \", other_hyperparameters_halving_result.best_score_)\n",
    "\n",
    "# Final best parameters and corresponding accuracy\n",
    "best_params = {**optimizer_halving_result.best_params_, **regularization_halving_result.best_params_, **other_hyperparameters_halving_result.best_params_}\n",
    "best_accuracy = other_hyperparameters_halving_result.best_score_\n",
    "\n",
    "# Print the final best parameters and the corresponding accuracy\n",
    "print(\"Final Best Parameters: \", best_params)\n",
    "print(\"Final Best Accuracy: \", best_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAHLCAYAAABVgQ5/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAACpNElEQVR4nOzdd1gU1/4/8PdSRYogKFIsWGJBvBYQC2IvodkAewEDYo9d06y5MV5jsKAiURRQo2ABbBEVUDRg19hQI1hAVBSQDgv7+8Pfztd1FwRXA5r363l4rnvmzJnPzM7mzmfOOTMiiUQiARERERERkRJUqjoAIiIiIiL69DGxICIiIiIipTGxICIiIiIipTGxICIiIiIipTGxICIiIiIipTGxICIiIiIipTGxICIiIiIipTGxICIiIiIipTGxIKJPwubNm1FcXFzVYRAREVEZmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSRBKJRFLVQRARvYtolbiqQyAiIqq2JHPUqjoE9lgQEREREZHymFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSPpvEwtraGosXL35n2acsNTUV1tbW8Pf3lyn/FPdz8eLFsLa2rnD9T3EfSbHIyEhYW1vjwoULVR0KERERfUBV/+7vz0RqaipcXFyEzyKRCDVr1oShoSGaN2+O3r17o0ePHlBT4yGvriIjI7FkyRKsWLECffr0kVsu/Y7d3Nwwf/78Koiw4qT7UhHt27fH5s2b31kvJiYGiYmJmDhxorLhERER0Wfos77KPXPmDFRVVf/Rbdra2sLR0REAkJeXh5SUFMTFxSEqKgotWrTAqlWrUK9evQ+6zarYT6re2rVrh6VLl8qUbd26FcnJyXLltWvXrlCbMTExOHjwIBMLIiIiUuizTiw0NTX/8W02aNAADg4OMmUzZszAzp07sXr1asyYMQM7duz4oD0XVbGfUhKJBPn5+ahZs2aVxfBvUlBQADU1tXeeP+bm5jA3N5cpO3DgAJKTk+XOTyIiIqIP4aMmFsXFxdi5cyf++OMPPHjwAGpqamjQoAGcnJwwbNgwoV5qaio2btyIhIQEZGdno27duujXrx8mTJiAGjVqyLT5999/w9fXF5cvX4aGhga6dOmCWbNmKdy+tbU1nJycZMbmS8uGDBmC9evX4+bNm9DU1ESPHj0we/ZsuQvkixcvYv369bhz5w50dHTQt29fDB48GMOGDYOXl1eF796OHDkSt27dwpEjR3Ds2DGZi7uioiKEhITg6NGjePz4MTQ0NNCuXTtMnDgRLVq0eGfbb+5nSUkJHB0dYWhoiB07dsjV3bt3L3766SesWrUKPXr0qNT2L1y4AB8fHyxatAj5+fkIDQ3F48ePMX78eEycOBHXr19HWFgYrl27hqdPn0JVVRVNmzbFmDFj0LNnT4WxZ2Rk4Ndff8WZM2dQWFgIKysrzJgxo0L7DQAJCQkICgrCjRs3UFRUhAYNGsDV1RWurq4y9a5evYotW7YgMTER2dnZqFWrFpo1awYvLy9YWVlVaFvlOXDgAEJDQ5GcnAw1NTW0bt0aXl5eaNu2rVBHOpRK0Xnj7++PgIAAREREwNTUFMDreSgHDx5EVFQU1q5dizNnziAjIwPh4eEwNTXFwYMHsWfPHjx8+BBisRiGhoawsrLC7NmzYWBgUOHY7969C39/f1y+fBn5+fkwMzODk5MTRo8eLfSEeXt749KlSwAgMzdm0aJFcHZ2RnJyMn7//XdcunQJaWlpKCkpgYWFBVxdXTFo0KD3PKpERET0KfloiUVxcTGmTp2KixcvolOnTvjyyy+hoaGBe/fuITo6Wkgsnjx5gnHjxiEnJweurq5o0KABLl68iMDAQFy9ehUbNmwQ7s6mpKTAy8sLRUVFcHd3h7GxMU6fPo1p06ZVKrY7d+5g5syZcHZ2Rv/+/XHx4kWEh4dDRUUF3377rVDvypUrmDp1KvT09DBu3Djo6uoiKioKV69efa9jMnDgQBw5cgRxcXFCYiEWizFt2jRcu3YNDg4OcHd3R05ODvbv348JEyYgICAArVq1qvA2VFVV8eWXXyI4OBh///03mjRpIrP80KFD0NfXh52d3Xtvf9euXcjKysKgQYNgaGgIY2NjAK+HyiQnJ6NPnz4wMTFBVlYWDh48iLlz52L58uUYMGCAXLzTpk2Dnp4evLy88OLFC+zZswfe3t7YunUrmjZtWu6+7tu3Dz/99BOsrKzg6ekJLS0tJCQkYMWKFUhJScGMGTMAAMnJyZgyZQoMDQ0xfPhw1K5dGy9fvsSVK1dw584ducQiLy8PmZmZctt79eqVwjjWrl2LoKAgWFpaYvLkycjLy8P+/fsxceJE/PLLL8Kxfl/S2CdMmCD0Dh06dAiLFy9Gu3bt4OPjA01NTTx9+hRnzpzBy5cvK5xY3Lx5E97e3lBTU4ObmxsMDQ1x+vRprFu3Dnfv3sXy5csBAJ6enpBIJLh8+bLMUKo2bdoAeJ10Xrp0CXZ2djA1NUVBQQGOHz+O5cuXIyMjAx4eHkodAyIiIqr+PlpisXPnTly8eBEeHh6YMmWKzLLS0lLh335+fsjIyICvr69wAebm5oY1a9YgODgYBw8eFO54btiwAa9evcKmTZuEu6bu7u6YO3cuEhMTKxzb3bt3ERgYiNatWwMAhg4ditzcXERERGDmzJlCr8Xq1ashEomwZcsWYViJm5sbvL293+uYNGvWDADw8OFDoWz37t24ePEi1q1bh86dOwvlrq6uGDZsGHx9fSs0sfZNTk5OwrGTXlwDwOPHj3Ht2jUMGzZMSNbeZ/tpaWkICwuTG5s/YcIETJ06VaZs+PDhGDlyJLZs2aIwsTAxMcHKlSshEokAAL169cLYsWOxZs0arFu3rsx9TE9Px6pVq9CvXz/8+OOPQrmbmxtWrVqFHTt2YOjQoTA3N0d8fDwKCgrw448/Ct95ed6eg1Ce5ORkBAcH4z//+Q82bdoEdXV1AMCgQYPg5uaGn3/+GZ07d1ZqDkyTJk2wbNkymbKYmBhoa2tj48aNMsOifHx8KtX2qlWrUFxcjMDAQOH8HDZsGBYuXIijR4/CxcUFHTt2RKdOnXD06FFcvnxZ4VAqR0dHuV6ikSNHwsfHB9u2bcOYMWP44AIiIqLP3Ed73OzRo0ehp6eHr776Sn6jKq83W1pailOnTqF58+Zyd3XHjx8PFRUVxMTECHVPnz6NVq1ayQzFEIlEGDt2bKVis7KykrvAtLGxQUlJCVJTUwEAL168wM2bN9G9e3eZsepqamoYMWJEpbYnpa2tDQDIzc0Vyo4cOYJGjRqhZcuWyMzMFP7EYjFsbW1x9epVFBQUVGo7TZo0QcuWLXH06FGZJO7QoUMAXiceymzf0dFR4YRfLS0t4d8FBQXIzMxEQUEBbGxskJSUhJycHLl1xo4dKyQVANCyZUvY2tri3LlzyMvLK3Mfjx8/jqKiIgwcOFAm7szMTHTr1g2lpaU4d+4cAEBHRwcAEBsbi8LCwnKPHQB4eXnBz89P7u/ti3tpmxKJBGPHjhWSCgCoU6cOnJ2d8eTJk0olvYqMHj1arkxHRwcFBQWIi4uDRCJ5r3ZfvnyJa9euwd7eXkgqgNe/KU9PTwBAdHR0hdp687svLCxEZmYmXr16hU6dOiE3NxfJycnvFSMRERF9Oj7aLcSHDx+iefPm5U4szsjIQF5eHho3biy3rFatWjAyMkJKSgqA1xdBeXl5aNiwoVxdReuXx8zMTOH2ACArKwsAhARD0fYUlVWENKGQJhgAkJSUhMLCQoWPN5XKzMys9JOkHB0dsWrVKpw7dw6dOnWCRCLB4cOH0bhxY7Rs2VKp7Tdo0EBhvZcvX2Ljxo2IjY3Fy5cv5Zbn5OQIF/lSFhYWcvUsLCwQHx+PJ0+eyA3lkpJeqE6ePLnMuKUx9OvXD4cPH0ZgYCB27twJKysrdOrUCf3794eJiYncek2aNIGtra1cufScUFSmKE5pWUpKSqWGs71N0fnm4eGBS5cuYc6cOahVqxbat2+Prl27om/fvjLnV3mksSv6/VhYWEBFRUX4/b1LXl4eNm/ejKioKDx9+lRueVnDyIiIiOjz8a8cm1DesJT3vftbEXfv3gUgf6HYtGlTzJw5s8z1KjMRV2rAgAHw9fXFoUOH0KlTJ1y5cgUpKSkK56NUdvtvT6gHXh+3qVOnIikpCcOHD0erVq2go6MDFRUVREZGyvWeKEv6PS1ZsgRGRkYK60gTSA0NDWzYsAHXr19HfHw8Ll26JEyWXr58eZkTyz+0N3tm3lZSUlLmMkXHu0GDBggNDcW5c+dw/vx5XLp0CcuXLxf26+0nQn1s3377LeLi4jB48GC0b98etWrVgoqKCs6cOYOdO3d+0O+eiIiIqqePllg0bNgQycnJKCoqgoaGhsI6BgYG0NbWxv379+WWvXr1Cunp6fjiiy+EujVr1sSDBw/k6ipaX1nSO9mKtqeorCLCw8MBQGbYV/369ZGRkQEbGxthiNiHoK+vj65duyI6Ohp5eXk4dOgQVFRU5MbHf6jt3717F3fu3FH4xKMDBw6UuV5SUpLc5OmkpCSoqqoq7E14M27g9X4q6l1QpHXr1sIQuLS0NIwaNQobN25UKrGQJi9///233MW89LyU1tHT0wOg+O59RXsG3qShoQE7OzvhfIqLi8PXX3+NHTt2VOgFftKnTyn6/SQnJ6O0tFSmd6+sxCg7O1t4IME333wjs0w6HI2IiIg+fx9tjsWAAQPw6tUrbNmyRW6Z9G6ziooKunXrhsTERJw9e1amzrZt21BaWio8ElVVVRV2dna4efMmLly4INNWUFDQB4/fyMgIrVq1QmxsLB4/fiyUi8Vi7Nq1q9Lt7dq1C0eOHEGzZs3Qt29fodzR0REvXrxQ+GhY4PVcj/fl5OSEgoICHD58GMePH4etrS3q1KkjU+dDbV+alLzd43Pv3j1hnowiQUFBMuvcvn0b586dg42NTbnvxujbty80NDTg7++vcA5KTk4OioqKAEDhE56MjY1hYGAgDH17X/b29hCJRAgODoZYLBbK09PTERkZCRMTEzRv3hzA6yFwhoaGOH/+vMw+P378uNxjpIiifZI+orei+1S7dm20adMGp06dwr1794RyiUSCwMBAAJBJuqTzKN5uv6zvPj09vdykkoiIiD4vH63HYsSIETh9+jS2bNmCmzdvwtbWFpqamrh//z4ePHiADRs2AHj9KM2EhATMmTMHrq6uqF+/Pi5duoSoqCi0b99eZqLx5MmTcfbsWXz99dcYNmwY6tati9OnTyMjI+Oj7MOMGTMwZcoUTJgwAa6urtDR0UFUVJRwAanoDu7Dhw9x+PBhAK8nMD9+/BhxcXG4f/8+WrZsiVWrVsk8HWfEiBFISEjAmjVrcP78edjY2EBbWxtpaWk4f/68cPH8Puzs7FCrVi2sW7cOubm5whvB3/Shtm9hYYHGjRsjKCgIBQUFaNiwIR4+fIh9+/ahadOmuHXrlsL1njx5gqlTp8Le3h7p6enYs2cPNDU1ZZ5mpYixsTEWLFiA5cuXw83NDQ4ODjAxMUFGRoaQzISGhsLU1BRbtmxBfHw87OzsYGZmBolEgtOnTyM5ObnSE//f1qhRI4wZMwZBQUHw8vJC3759hcfN5uXlYdmyZTJD79zd3bFx40ZMnz4d3bt3R3p6Ovbu3YsmTZrg5s2bFd7ulClToKuri3bt2sHY2BjZ2dmIjIyESCSq1Avw5syZA29vb3h5eQmPm42Li8Off/6JAQMGoGPHjkJdKysr7NmzBytWrICdnZ3wvg4zMzN06tQJR44cgaamJiwtLfHkyRPs27cPZmZmSidvRERE9Gn4aImFuro61q9fj5CQEPzxxx/YsGEDNDQ00KBBAzg7Owv1TExMsG3bNmzatAlHjhxBdnY2jI2N4eHhgQkTJshchJubm+O3337Dr7/+it27dwsvyFu6dCn69ev3wfehQ4cOWLduHfz8/BAYGAhdXV307dsXAwYMwPjx4xVOTE9ISEBCQgJEIhFq1qwJQ0NDNG/eHN7e3ujRo4fcIzfV1NTg6+uLsLAwHD58WLiIr1OnDiwtLWUSq8pSV1dH//79sWfPHmhrawu9Px9j+6qqqlizZg18fX1x8OBB5Ofno0mTJli8eDHu3LlTZmKxbt06rF69Gps3b0ZBQYHwgrw3n1JUFhcXFzRo0AAhISHYt28fsrOzoa+vj4YNG2LSpEkwNDQEAOEC/vjx43j58iU0NTVRv359fPfddxg4cGCF9q8806dPR/369REaGor169dDXV0dlpaWWL58Odq1aydTV/rOlsOHD+PixYuwsLDA999/j1u3blUqsXB1dUVUVBT27duHrKws1KpVC82bN8e8efNknpr2Lq1atcLWrVvh7++PsLAw4QV506ZNk3saVf/+/ZGYmIhjx47hxIkTKC0txaJFi2BmZoZly5Zh3bp1OH36NA4dOoT69etj8uTJUFNTw5IlSyocDxEREX26RJKPOVv5M3XixAnMnz8fP/74I/r371/V4RD9K4hWid9diYiI6F9KMqfqn8n00eZYfA4kEoncew/EYjF27NgBVVVVdOjQoYoiIyIiIiKqXqo+tanGioqK4OzsjAEDBqBhw4bIyspCVFQU7t69i3HjxpX5mFMiIiIion8bJhblUFNTQ9euXREbG4v09HQArx+jO3/+fLi5uVVxdERERERE1QfnWBDRJ4FzLIiIiMrGORZERERERPRZYGJBRERERERKq/o+EyKiCvDX2woPDw+oq6tXdShERESkAHssiIiIiIhIaUwsiIiIiIhIaUwsiIiIiIhIaUwsiIiIiIhIaUwsiIiIiIhIaUwsiIiIiIhIaUwsiIiIiIhIaUwsiIiIiIhIaUwsiIiIiIhIaUwsiIiIiIhIaUwsiIiIiIhIaUwsiIiIiIhIaSKJRCKp6iCIiN5FtEpc1SEQEVUbkjlqVR0CkRz2WBARERERkdKYWBARERERkdKYWBARERERkdKYWBARERERkdKYWBARERERkdKYWBARERERkdKYWBARERERkdKYWBARERERkdKqTWLh7+8Pa2trpKamVnUo/2rW1tZYvHhxVYdBRERERJ8YvraxEp49e4Zdu3bhzz//RGpqKoqLi2FkZIS2bdvC2dkZHTt2rOoQPwp/f380b94cPXr0kFvm7OyMJ0+eoFatWjhy5Ag0NDTk6syaNQunTp0CAERERMDU1PRjh/xJkB47KTU1NdSpUwcdO3aEl5cX6tWrV4XRfXjZ2dnYuXMnOnToAGtr66oOh4iIiD6wapNYTJgwAePHj1d4YVodxMXF4dtvv0VRURH69OmDwYMHQ1NTE0+ePEFMTAwmT54MX19f2NnZVXWoSjlz5gxUVVVlygICAuDk5KQwsQAATU1NZGVl4dSpU+jTp4/MshcvXuDMmTPQ1NREYWHhxwr7k2VsbIwpU6YAAPLy8nDx4kVERETgzJkz+P3336Gvr1+1AX5A2dnZCAgIAAAmFkRERJ+hapNYqKmpQU2t2oQj4++//8b8+fNRq1YtbNu2DRYWFjLLfXx8cOTIEWhqalZRhB/O++yDmZkZVFRUEBERIZdYHDp0CADQrVs3HD9+/IPE+DnR1taGg4OD8NnV1RUGBgbYs2cPIiIiMHbsWKW3UVBQUK1/X0RERPR5qPQci/LmQjg7O8Pb21v4LB2vf+3aNXh7e8POzg69e/fGsmXLkJeXV6F2//77b0ybNg12dnbo1asXvvvuO7x8+VJuLsCFCxdgbW2NyMhIubgWL16s8A7pw4cP8f3336N///7o1KkTnJ2dsWbNGuTn58vU27RpEwoLC/Hdd9/JJRUAIBKJ4ODgABsbG6FMLBZj27ZtcHNzQ5cuXdC7d2/MmTMH9+7dk1k3NTUV1tbW8Pf3R1RUFEaOHImuXbti0KBBiIiIAACkpaVh3rx56NWrF+zt7fH9998jNzdX4T5mZmZi8eLF6N27N+zt7TF79mykp6cDAPbt2wdXV1d06dIFQ4cORUxMjNy+vHlcpbEBwMGDB2FtbS38vc3Z2RkJCQl4/vy5THlkZCTs7OxgYGAgtw4A5OTkYO3atRg0aBA6d+6MPn364JtvvsHjx49l6hUWFsLf3x9DhgxB165d0aNHDwwbNgxr1qyRqRcXFwdvb2/07t0bXbt2haOjI+bOnYsHDx4IdZKTk7FixQq4u7vD3t4eXbt2xejRo3HgwAGFMd69exdTpkwRzt9FixYhMzOzzPkox44dw4QJE4S2x40bV6mkqnPnzgCAR48eCWUVPVel50FGRgaWLFmCfv36oVu3bnj27JlwvP38/ITzoHfv3pgwYQL++OMPmXbS09Px008/wdHREZ06dcKAAQPw448/4uXLlzL1pL/b5ORk+Pn5wcHBAZ07d8aIESMQFxcn1Ltw4QJcXFwAvO4Bk55Hzs7OFT4uREREVL199FuYd+7cwcyZM+Hs7Iz+/fvj4sWLCA8Ph4qKCr799tty101JSYGXlxeKiorg7u4OY2NjnD59GtOmTVM6rlu3bsHHxwe6uroYMmQI6tatizt37uD333/H1atXsXnzZqipqaGwsBBnzpyBsbExunTpUuH2v//+e0RFRcHW1hZDhw7FixcvEBoaCg8PDwQEBKBFixYy9ePi4oQLfz09PYSHh2Pp0qVQV1eHn58fbGxsMHnyZNy8eRMRERHQ0NDA999/L7fd6dOno27duvDx8cGjR4+we/duzJ07Fz179sT+/fsxcOBAaGhoYPfu3Zg/fz727dsHMzMzhftgYGCApUuX4ocffkC7du0wePDgMvfXwcEB69atw8GDB+Hh4QEA+Ouvv5CUlISpU6ciPj5ebp2cnBx4enoiLS0NLi4uaNy4MdLT0xEWFobx48cjODgYJiYmAICff/4ZERERcHR0xKhRo1BSUoJHjx7h/PnzQnsXL17ErFmz0KRJE3h4eEBHRwfp6ek4d+4cHj16hIYNGwJ4fZF76dIl2NnZwdTUFAUFBTh+/DiWL1+OjIwMIX7g9QX9V199BYlEguHDh6NOnTo4c+ZMmefghg0bsHXrVnTp0gU+Pj5QUVFBdHQ0FixYgHnz5sHd3b3MY/jmNgEIw6Aqeq6+acqUKTA0NMSECROQn5+PmjVrIjs7GxMmTMD9+/fRu3dvuLq6oqSkBImJiYiLi0P//v0BvE5kPTw8UFxcjIEDB8Lc3ByPHj3C3r17ceHCBQQHB0NHR0dme4sXL4aamhpGjx6N4uJi7Nq1C3PmzMG+fftgamoKCwsLzJo1C6tXr0bPnj3Rs2dPAEDNmjXfeTyIiIjo0/DRE4u7d+8iMDAQrVu3BgAMHToUubm5iIiIwMyZM8u9sNiwYQNevXqFTZs2CXfJ3d3dMXfuXCQmJioV19KlS2FkZISgoCBoa2sL5R07dsTcuXNx5MgRODs749GjRygqKsIXX3xR4bbj4+MRFRWFvn374r///S9EIhEAoG/fvhgzZgxWrVqF3377TWadpKQkhIaGChfS/fr1g6OjI3744QfMmDEDo0ePFupmZ2fj0KFDmD17ttzxs7S0xPz582XKdu7ciWfPnmH37t3CBaGNjQ1GjBiB/fv3Y+rUqQr3Q0tLCw4ODvjhhx9gZmYmM2TnbQYGBujWrRsiIyOFC/OIiAgYGhqia9euChOLTZs2ISUlBYGBgTLH19nZGcOHD4e/v7/QIxATE4MuXbpgyZIlZcYQGxuL0tJS+Pn5oXbt2kL5V199JVPP0dERrq6uMmUjR46Ej48Ptm3bhjFjxggX6hs2bEBubi5+++03tG3bFgAwbNgwLFy4ELdu3ZJp4/bt29i6dSs8PDyEeRMAMHz4cMyePRt+fn5wdHSUOd9KS0uRmZkJAMjNzcXFixcREBAAVVVV4UK/oufqm5o0aYJly5bJlK1YsQL379/HN998gyFDhsgsKy0tFf69cuVKiMVi7NixA8bGxkJ5nz594OHhgR07dmDixIky6+vr6+PXX38VznVra2uMGzcO+/btw9SpU2FoaIgePXpg9erVaNq0abnnEhEREX2aPvrjZq2srISkQsrGxgYlJSXlPlq2tLQUp0+fRqtWrWSG3ohEIqXHnd+7dw93797FgAEDUFxcjMzMTOGvbdu20NLSEi6Ec3JyAEDuDm15pEOMPD09hQstAPjiiy/QrVs3XLlyBRkZGTLr9OjRQ0gqgNcX6g0bNoSKiorcXe62bdtCLBYrPH4jRoyQ+dyuXTsAry+m39yHZs2aQVtbW7g7/iEMHDgQDx8+xJUrV1BQUIBjx47BwcFB4dh+iUSCI0eOoF27dqhbt67Md6ClpYXWrVvLJCM6Ojq4f/++3FCyN0n37+TJkxCLxWXW09LSEv5dWFiIzMxMvHr1Cp06dUJubi6Sk5MBACUlJThz5gwsLS2FpEJq1KhRcu0eOXIEIpEIjo6OMvuTmZkJe3t75Obm4q+//pJZJzk5GX369EGfPn0wcOBALF26FPr6+vjll1/QtGnTSp2rb3ozEQVe/56OHTsGCwsLuaQCAFRUXv+nICcnB3FxcbC3t4empqbM9kxNTWFubo6EhAS59YcPHy5zrltaWqJmzZof9PwiIiKi6u2j91goGmZTq1YtAEBWVlaZ6718+RJ5eXnC8JU3NW7cWKmYkpKSALweH+7v71/m9oH/u1h9e05DeVJTU6GioqJwPkbjxo0RExODlJQUmXkHio6Trq4ujIyM5J6UpaenB0Dx8Xu7HV1dXQBQ+IhXPT29cr+DyurcuTOMjIwQGRmJlJQU5ObmCuPq35aRkYGsrCzEx8fLTfiWkl7sAq8fWbto0SIMHz4cZmZmsLa2Rrdu3WBvby/Uc3d3R2xsLFasWIF169bhP//5D7p06YL+/fvLHOu8vDxs3rwZUVFRePr0qdx2X716JcSYn5+v8Bxs1KiRXFlSUhIkEolcb8ibXrx4IfPZ1NRUGBKorq6OOnXqoH79+jJtAhU7V9/0dszS5Ek6f6MsycnJKC0tRXh4OMLDwxXWUXSumpuby5XVqlXrg55fREREVL1VOrF4867k20pKSuTK3n506ZskEkllN1+mysQl3e7o0aPLvNCSXrzXr18fGhoauHPnzgeKVLE3L6IrUg4oPn5lHe+yyj/kd6CqqgpHR0eEhYXh/v37sLKyUphcvbndjh07Yty4ce9su0ePHsJjWC9duoRz584hPDwc7dq1w4YNG6Curg59fX0EBQXh8uXLSEhIwOXLl7F69Wr4+/tjzZo1aNOmDQDg22+/RVxcHAYPHoz27dujVq1aUFFRwZkzZ7Bz506ZYUGVJRKJsHbt2jK/tyZNmsh8rlGjBmxtbctsrzLn6tvtKuPLL7+Ek5OTwmWKnhxW1v5+yPOLiIiIqrdKJxbSi5hXr17J3AUvLCxEenq6wjuX78PAwAA1a9aUeZqP1P379+XKyusFSUlJkfncoEEDAK8vhsq7qANeX0R17doV0dHRiI+PR6dOnd4Zu5mZGUpLS5GUlIRmzZrJLJPegS5rwvSnzsXFBdu3b8dff/1V7uR8AwMD6OrqIjc3953fgVStWrXg4OAABwcHSCQSrFu3DkFBQYiNjRV6PVRVVWWeXHX37l2MHj0aW7ZswZo1a5CdnY24uDg4ODjgm2++kWn/3LlzcjFqaWkpPAelw6XeVL9+fZw9exb16tUrM6GqrMqcq+XR19eHnp4e7t69W249c3NziEQiiMVipbanSHnJPxEREX36Kj3HQjrE4u1x1sre6X2bqqoq7OzscPPmTVy4cEEol0gkCAoKkqtvamoKVVVVuYvDq1evyo1rb968OZo0aYK9e/fKPdYUeP2o2DcTlIkTJ0JTUxPLli1TeEEJAEePHhWeUNS9e3cAQGBgoMwd23v37uHUqVNo27ZtmY9frY5q1qxZ4SEtDRs2xJw5c+Dl5YW+ffuWWU9FRQUDBgzAjRs3ynwUq3SIT0lJCbKzs2WWiUQiNG/eHMD/JZPSSdBvatSoEWrUqCEMb5LeWX/7Tnp6errc42ZVVVXRpUsX3LhxA1euXJFZtmPHDrltSSck+/n5Key9e3sYVEVU9lwti4qKCvr374/79+8rfKyu9Hjo6+uja9euOHnypNzvRlrv7flBFSWd2yL9LoiIiOjzUukei44dO6Jhw4bw9/dHVlYWTE1NhYv3D/2W4MmTJ+Ps2bP4+uuvMWzYMNStWxenT59WeGFTs2ZNODs748CBA/jmm2/QoUMHPHr0CJGRkWjWrJnMUCaRSISlS5di0qRJGDFihPCo04KCAjx+/BgnT57E1KlThSftNG3aFD///DO+/fZbjBw5En369EHr1q2hqamJtLQ0xMbG4s6dO1i7di0AoFOnTujbty+OHTuG7Oxs2NnZCY+b1dDQwJw5cz7ocfrYWrdujXPnzmHbtm2oV68eRCKR8MQiRYYPH16hdqdMmYKrV69i4cKFOHHiBKysrKCuro4nT57gzJkzaNmyJRYvXoy8vDwMGDAA9vb2aN68OQwMDJCamoqwsDDo6enB3t4eALB8+XI8e/YMtra2MDExQWFhIaKiopCbmwtHR0cAr19I16lTJ+GFhpaWlnjy5Inw2N23L9InTZqE+Ph4TJ8+He7u7qhbty7i4uKEJObtCcve3t7YvHmzcJ7UqVMH6enpuHXrFs6cOaNwonV5KnuulmfSpEk4f/48li9fjoSEBPznP/8BACQmJkIsFgtPkVqwYAG++uoreHl5wdHREc2bN0dpaSlSUlJw6tQpODg4yD0VqiL09fVRv359HDt2DObm5qhduza0tLSE74+IiIg+bZVOLFRVVbF69WqsWrUKu3fvhrq6Ojp16oTNmzdjwoQJHzQ4c3Nz/Pbbb/j111+xe/duaGhooEuXLli6dCn69esnV3/WrFmQSCSIiYlBbGwsWrZsidWrV2P//v1ycySaN2+OHTt2IDAwEKdOncLevXuhra0NExMTODs7y7zsDgDs7OwQGhqKXbt24ezZs4iOjoZYLEadOnXwn//8B7NmzZJ5etWyZcvQvHlzHDx4EL6+vtDS0kL79u0xadIkNG3a9IMep49twYIF+PnnnxEYGChMYi8vsagoHR0dbN26FSEhIYiKisKpU6egqqqKunXrom3bthg0aBCA1/MFRowYgXPnzuHcuXPIy8uDkZER7O3t4eHhgTp16gB43WMQGRmJQ4cOISMjA9ra2mjcuDF+/vln9O7dW9jusmXLsG7dOpw+fRqHDh1C/fr1MXnyZKipqck9zrZRo0bYvHkz1qxZg127dkFTUxN2dnaYP38+Bg4cKDffwNvbG61atcLvv/+OXbt2IT8/H7Vr10aTJk3eO6Gs7LlaFj09PQQGBmLr1q2Ijo5GdHQ0tLW1YWFhgWHDhgn16tWrh5CQEGzfvh2xsbE4cuQINDQ0YGxsjG7dupXbE/Uuy5Ytw+rVq+Hn54eCggKYmJgwsSAiIvpMiCSf6OxKa2trODk5KXzzMdHHduvWLYwZMwZTp07F+PHjqzqcfwXRqrIfIUxE9G8jmfPRH+xJVGkf/T0WRJ+6goICmc9vzvP50BOciYiIiD5VTHeJ3mHkyJGwsbFB06ZNkZ+fj9OnT+Py5cvo27cvWrZsWdXhEREREVULTCyI3qF79+44ffo0Dh8+jJKSEpiamsLHx4dDoIiIiIje8MnOsSCifxfOsSAi+j+cY0HVEedYEBERERGR0phYEBERERGR0tiPRkSfBH+9rfDw8IC6unpVh0JEREQKsMeCiIiIiIiUxsSCiIiIiIiUxsSCiIiIiIiUxsSCiIiIiIiUxsSCiIiIiIiUxsSCiIiIiIiUxsSCiIiIiIiUxsSCiIiIiIiUxsSCiIiIiIiUxsSCiIiIiIiUxsSCiIiIiIiUxsSCiIiIiIiUJpJIJJKqDoKI6F1Eq8RVHQIRfaIkc9SqOgSifwX2WBARERERkdKYWBARERERkdKYWBARERERkdKYWBARERERkdKYWBARERERkdKYWBARERERkdKYWBARERERkdKYWBARERERkdKYWBARERERkdKYWNA/LjIyEtbW1rhw4cIHbdfb2xvOzs4ftE368FJTU2FtbQ1/f/+qDoWIiIg+IL7jnuj/S01NhYuLC9zc3DB//nyFdZydnaGlpYU9e/b8w9FVjnRfKqoiSV5iYiJiYmLg7OwMU1NTZcIjIiKizxATC6LPkIGBAZYuXSpTFh0djejoaHh4eMDCwqLSbd65cwcBAQHo0KEDEwsiIiKSw8SC6BNSUlKC4uJi1KhRo9x6WlpacHBwkCl79OgRoqOjYWtrC2tr648ZJhEREf0LMbGgKiORSBAcHIywsDA8e/YMJiYm8PT0hJOTk0y9hIQEBAUF4caNGygqKkKDBg3g6uoKV1fXd27D29sbT548wcaNG7F69WpcvHgRAGBjY4Ovv/4a5ubmH2RfLl26hN9++w03btyAWCxGo0aN4ObmhkGDBsnUc3Z2homJCTZv3ixTfuHCBfj4+GDRokXCPJHIyEgsWbIEfn5++OuvvxAZGYm0tDR89913cHZ2xtWrV7FlyxYkJiYiOzsbtWrVQrNmzeDl5QUrK6sKx56ZmQl/f3+cOnUKL168gKGhIezt7TFx4kTo6+sDAPz9/REQEAAA8PHxEdZ1cnLC4sWLkZubi+3btyMhIQGPHz9GXl4ejI2N0bt3b3h5eb0zESIiIqJPHxMLqjJ+fn4oLCzEkCFDoKGhgbCwMCxevBjm5uZo27YtAGDfvn346aefYGVlBU9PT2hpaSEhIQErVqxASkoKZsyY8c7t5OfnY+LEiWjdujWmTp2Khw8fIiwsDH/99Rd27NgBIyMjmfpFRUXIzMxU2FZpaalc2alTpzB37lwYGhpi9OjRqFmzJo4dO4bly5cjJSUFU6ZMqfSxedOaNWsgFosxePBgaGtro2HDhkhOTsaUKVNgaGiI4cOHo3bt2nj58iWuXLmCO3fuVDixyMnJgaenJx49egQXFxe0aNECiYmJCAsLw/nz57F9+3Zoa2ujV69eSE9Px/79+2WGUkkTs+fPnyM8PBy9evXCgAEDoKqqikuXLiEoKAiJiYlYv369UseAiIiIqj8mFlRlioqKEBQUBHV1dQBA7969MXDgQOzZswdt27ZFeno6Vq1ahX79+uHHH38U1nNzc8OqVauwY8cODB069J29DpmZmRgxYgRmz54tlLVv3x5z587F5s2b8c0338jUDw8PR3h4eJntNW7cWPh3SUkJVq5cCS0tLWzfvh116tQBALi7u2PixInYvn07nJ2d0aBBg4ofmLcUFBRg586dMnf9f//9dxQUFODHH39E69at37vt7du34+HDh5g/fz7c3NyE8i+++AIrV65EUFAQJk2ahGbNmqFNmzbYv3+/wqFUZmZmOHToENTU/u8/Ke7u7ti4cSO2bNmC69evKxUnERERVX983CxVGTc3NyGpAIC6deuiQYMGePToEQDg+PHjKCoqwsCBA5GZmSnz161bN5SWluLcuXMV2ta4ceNkPvfs2RMNGzZEbGysXN3u3bvDz89P4Z+hoaFM3Vu3biEtLQ0uLi5CUgEA6urqGDt2LEpLSxVuozJcXV3lhhLp6OgAAGJjY1FYWPjebcfExMDAwACDBw+WKR8yZAgMDAwQHR1doXbU1dWFpEIsFuPVq1fIzMxEx44dAQDXr19/7xiJiIjo08AeC6oyZmZmcmW1atVCWloaACA5ORkAMHny5DLbePny5Tu3o6urKzfcCQAsLCwQExOD/Px8aGlpCeV169aFra2twrY0NDRkPqempgKQ7cWQatKkCQAgJSXlnTGWR1FvR79+/XD48GEEBgZi586dsLKyQqdOndC/f3+YmJhUuO3U1FS0bNlSpqcBANTU1NCgQQPcvn27wm2FhoZi7969uH//vtyQsezs7Aq3Q0RERJ8mJhZUZVRUFHeYSSQSmf9dsmSJwsQAUJycVGcikUhheUlJSZnrKJr4rKGhgQ0bNuD69euIj4/HpUuXhAnWy5cvR8+ePT9YzBUREhICX19fdOrUCcOHD4eRkRHU1dXx/PlzLF68WOHcFCIiIvq8MLGgaqt+/foAAH19/TJ7ECoiOzsb6enpcslJUlISateuLdNbUVnSxOb+/ftyy6RlbyY/enp6ePXqlVzd9+3VaN26tTB3IS0tDaNGjcLGjRsrnFiYmZnhwYMHEIvFMr0WYrEYDx8+lIm9rKQIAA4fPgxTU1OsXbtWJmE8e/ZsZXeJiIiIPlGcY0HVVt++faGhoQF/f38UFBTILc/JyUFRUVGF2tq+fbvM5+joaDx48ADdu3dXKsYWLVqgXr16iIyMRHp6ulAuFosRHBwMkUgks40GDRogOTkZz549E8qKiooQGhpaqe0qemqVsbExDAwMkJWVVeF2unfvjoyMDBw4cECm/MCBA8jIyJBJUKQJmKLESFVVFSKRSOhlAl4fg23btlU4FiIiIvq0sceCqi1jY2MsWLAAy5cvh5ubGxwcHGBiYoKMjAzcu3cPMTExCA0NfedboPX19XHy5Ek8f/4cHTp0EB43a2hoiIkTJyoVo6qqKubNm4e5c+di3LhxGDx4MGrWrImoqCj89ddf8PDwkJkj4e7ujmPHjmHy5MkYOnQoiouLcfjw4Uq/52HLli2Ij4+HnZ0dzMzMIJFIcPr0aSQnJ2Ps2LEVbmfcuHE4ceIEVq5cicTERDRv3hyJiYkIDw9Hw4YNZdqytLSEiooKtm7dilevXkFLSwtmZmZo3bo1evfujfXr12P69Ono2bMncnNz8ccff8jN3SAiIqLPF/9fn6o1FxcXNGjQACEhIdi3bx+ys7Ohr6+Phg0bYtKkSXJPaVJES0tLeEHe+vXrIZFI0LlzZ8ycObPMuRuVYW9vjw0bNmDLli0IDg5GcXExGjVqhO+++07uBXlt27bF4sWLsXXrVqxZswZ169bF0KFD0apVK0yaNKnC2+zevTvS09Nx/PhxvHz5Epqamqhfvz6+++47DBw4sMLt6OjoYMuWLcIL8iIiImBoaIihQ4di4sSJ0NbWFurWq1cPP/zwA7Zv344VK1ZALBbDyckJrVu3xpgxYyCRSBAeHo5ffvkFhoaG6Nu3L1xcXGQeY0tERESfL5HkzbELRJ8Z6Zu3IyMjqzoUUpJolbiqQyCiT5RkDu+jEv0TOMeCiIiIiIiUxsSCiIiIiIiUxsSCiIiIiIiUxjkWRPRJ4BwLInpfnGNB9M9gjwURERERESmNiQURERERESmNfYNE9Enw19sKDw8PqKurV3UoREREpAB7LIiIiIiISGlMLIiIiIiISGlMLIiIiIiISGlMLIiIiIiISGlMLIiIiIiISGlMLIiIiIiISGlMLIiIiIiISGlMLIiIiIiISGlMLIiIiIiISGlMLIiIiIiISGlMLIiIiIiISGlMLIiIiIiISGkiiUQiqeogiIjeRbRKXNUhEFU5yRy1qg6BiKhM7LEgIiIiIiKlMbEgIiIiIiKlMbEgIiIiIiKlMbEgIiIiIiKlMbEgIiIiIiKlMbEgIiIiIiKlMbEgIiIiIiKlMbEgIiIiIiKlVVlikZKSgtmzZ6NPnz6wtrbG4sWLqyqUCrlw4QKsra0RGRlZ1aFQBaSmpsLa2hr+/v5VHQoRERHRv0KVvcJzyZIluHv3Ljw9PWFoaAhzc/OqCuWj8/b2xqVLlypUd9GiRXB2dv7IEVVeamoqXFxcAAB2dnbw9fWVqyMWizFgwABkZmbCxMSESVg5/P39ERAQIHwWiUTQ1dVFixYtMGLECHTr1q0Ko/s4IiMjkZ2djZEjR1Z1KERERPQRVEliUVRUhMuXL8Pd3R1jxoypihD+UZ6enhg0aJDwOTMzE6tXr0a7du0wePBgmbpt2rT5h6OrHE1NTfz5559IT0+HkZGRzLLY2FhkZmZCU1OziqL79Pj4+MDU1BQlJSV4+PAh9u3bh5kzZ2L58uUYMGBAVYf3QUVGRuLJkydMLIiIiD5TVZJYvHz5EhKJBHp6elWx+X9cp06dZD6npqZi9erVMDMzg4ODQxVF9X7s7OwQExODQ4cOYdy4cTLLIiIi0KxZM5SUlCA/P7+KIqwaYrEYJSUllU6qunTpglatWgmfe/XqhTFjxmDr1q0fJLGQSCTIz89HzZo1lW6LiIiIqDz/eGKxePFiHDx4EAAQEBAgDAfZtGkTrK2tcezYMezevRt3795FSUkJmjZtijFjxqBPnz4y7VhbW8PJyQmOjo7YsGED7ty5g1q1asHd3R3jx4/Hq1ev4Ovri9OnTyMvLw82Njb49ttvUadOHaGN58+fIyQkBOfPn8eTJ09QWFgIMzMzODo6YsyYMVBVVX3n/kgkEuzduxcHDhxAUlISVFRU0KpVK3h5ecHa2rrSxychIQFBQUG4ceMGioqK0KBBA7i6usLV1VWmXnx8PMLDw3Hz5k2kp6dDXV0dlpaW8PT0RIcOHWTqent748mTJ/D398fq1atx4cIFiEQidO/eHfPmzUONGjWwbds2HDhwAOnp6bCwsMDcuXPRtm1bufhq166Nrl27IjIyUiaxSE9PR3x8PGbMmIEDBw4o3LeHDx8iICAA586dQ1ZWFurUqYM+ffrA29sbWlpaQj3pOXL8+HHhOywuLoaNjQ0WLlwIIyMj7Nu3Dzt37kRqaipMTEwwbdo09OjRQ+F2jx49im3btuHhw4cwMDCAi4sLJkyYADU12dM/PT0dAQEBiIuLw4sXL6Cvr49u3bph0qRJqF27tlBPOoxp9+7dCA8Px/Hjx5Geno4NGzbA2toacXFxCAoKwt9//42CggLo6+ujVatWmDp1Kho2bKgwRqmWLVuiVq1aePTokVBWVFSEkJAQHD16FI8fP4aGhgbatWuHiRMnokWLFkK9CxcuwMfHB4sWLUJ+fj5CQ0Px+PFjjB8/HhMnTgQAnDhxArt378adO3dQXFwMY2NjdO7cGV9//TXU1dUBVPyclg6P8/LyQqtWrRAQEIB79+5BV1cXDg4OmDJlinCMnZ2d8eTJEwCQaUP6uyciIqJP3z+eWAwZMgRffPEFVq9ejZ49e6Jnz54AAAsLC2zYsAFbt25Fly5d4OPjAxUVFURHR2PBggWYN28e3N3dZdpKTEzE6dOnMXjwYDg6OiIqKgrr16+HpqYmDh48CFNTU3h7e+PRo0fYvXs3Fi1ahA0bNgjr3717F9HR0ejRowfMzc0hFovx559/Yv369UhJScG33377zv354Ycf8Mcff6B3795wdnZGcXExjhw5gilTpmDlypXo3r17hY/Nvn378NNPP8HKygqenp7Q0tJCQkICVqxYgZSUFMyYMUOoGxkZiaysLDg4OMDY2BjPnj1DeHg4Jk+ejE2bNqFdu3Yybefn52PSpElo3749pk6dips3byIiIgKFhYXQ19fH9evX4e7uDrFYjJCQEMyaNQuRkZHQ1taWi9PFxQVz5szBtWvXhKFbBw8ehIqKChwcHBQmFrdu3YKPjw90dXUxZMgQ1K1bF3fu3MHvv/+Oq1evYvPmzXIX+tOnT0fdunXh4+MjfIdz585Fz549sX//fgwcOBAaGhrYvXs35s+fj3379sHMzEymjVOnTiElJQVubm4wNDTEqVOnEBAQgLS0NCxatEiol5aWBg8PDxQXF2PgwIEwNzfHo0ePsHfvXly4cAHBwcHQ0dGRafv777+HpqYmRo0aBZFIBCMjI1y8eBGzZs1CkyZN4OHhAR0dHaSnp+PcuXN49OjROxOLzMxMZGdnw9DQEMDrnpBp06bh2rVrcHBwgLu7O3JycrB//35MmDABAQEBMj0eALBr1y5kZWVh0KBBMDQ0hLGxMQDAz88PgYGBaNy4MUaOHAkjIyM8fvwYJ0+ehI+Pj5BYVPacPnPmDMLCwjB06FC4uLggNjYWwcHB0NXVhaenJwBg9uzZWL9+PTIzMzFr1ixhXQsLi3KPBxEREX06/vHEok2bNjAyMsLq1avRtGlTYSjQ7du3sXXrVnh4eGDKlClC/eHDh2P27Nnw8/ODo6OjzIXuvXv3EBgYiNatWwMABg4cCCcnJ6xevRru7u6YO3euzLZ37tyJ5ORkNGrUCADQvn17hIeHQyQSCXVGjhyJ77//HuHh4Zg4caLcPII3RUdH48iRI/jmm28wZMgQmZg9PDzwyy+/wN7eXqb9sqSnp2PVqlXo168ffvzxR6Hczc0Nq1atwo4dOzB06FBhkvt3330nc5cfAIYOHQp3d3cEBgbKJRaZmZkYO3Ysxo4dK5RlZ2fj+PHjaNGiBQIDA4ULewsLC8yePRtHjx7F0KFD5WK1s7ODoaEhIiMjhcQiIiIC3bp1g76+vsL9W7p0KYyMjBAUFCTzHXbs2BFz587FkSNH5CatW1paYv78+TJlO3fuxLNnz7B7927hQt/GxgYjRozA/v37MXXqVJn6d+/eRVBQkHBnf9iwYZg7dy4iIyMxZMgQWFlZAQBWrlwJsViMHTt2CBfiANCnTx94eHhgx44dwl1/KR0dHWzYsEEmIdq3bx9KS0vh5+cn08vx1VdfKTwuOTk5yMzMhFgsxsOHD+Hn54fS0lI4OjoCAHbv3o2LFy9i3bp16Ny5s7Ceq6srhg0bBl9fX2zevFmmzbS0NISFhcls//r16wgMDIS1tTXWrFkjM2Rr2rRpwr/f55y+f/8+9uzZA1NTUwCvz8Nhw4Zh9+7dQmLRo0cP7Ny5E4WFhZ/c8D8iIiKqmGrzHosjR45AJBLB0dERmZmZMn/29vbIzc3FX3/9JbOOlZWVkFQAEIYDSSQSDB8+XKau9EL7zSEmNWrUEC6QiouLkZWVhczMTHTu3BmlpaW4efNmuTEfPnwY2tra6NGjh0y8OTk56NatG1JTU/Hw4cMK7f/x48dRVFSEgQMHyu1/t27dUFpainPnzgn130wq8vLykJmZCVVVVbRu3Ro3btyQa19VVRXDhg2TKWvbti0kEgmGDh0qc3Gs6Fi9SU1NDQ4ODjh27BgKCgpw5coVPHz4UHhq1Nvu3buHu3fvYsCAASguLpbZt7Zt20JLSwvx8fFy640YMULmszQuR0dHmd6DZs2aQVtbW+GxtrW1lRkuJBKJhOQqOjoawOuL+7i4ONjb20NTU1MmPlNTU5ibmyMhIUGu7ZEjR8r1skjjOnnyJMRiscLj8abJkyejT58+GDBgALy9vZGYmIhRo0bBx8cHwOvfRaNGjdCyZUuZuMRiMWxtbXH16lUUFBTItOno6CiTVACvh4MBwNSpU+XmgYhEIuF38D7ndI8ePYSkQtqetbU1Xrx4gby8vHceAyIiIvo8VNnjZt+WlJQEiUQiN5fgTS9evJD5/PawFwDChPA3L3QAQFdXFwCQlZUllInFYmzbtg2HDx/Go0ePIJFIZNZ59epVuTEnJycjNzcX/fr1K7POy5cv3zn8RdoW8PpCs7y2pB4/fgw/Pz/Ex8cjOztbpp6iHhIjIyO5C8qyjpW0/M1j9TZnZ2cEBwfj5MmTuHDhAurUqSNzR/1NSUlJAF7PTSjrvRJv7pvU29+v9Dt8O15pzIrilfZOvalx48YAXr9LBXh97EtLSxEeHo7w8HCF8Sk61xo0aCBX5u7ujtjYWKxYsQLr1q3Df/7zH3Tp0gX9+/eHgYGBXP358+ejQYMGUFFRga6uLho1aoQaNWoIy5OSklBYWCg3x+hNmZmZqFevXrlxPXz4ECKRCM2aNSuzHeD9zmlFx6ZWrVoAXp9DnDhORET071BtEgvg9QXx2rVroaKiuCOlSZMmMp/Lm1xd1rI3k4dff/0Vu3fvRt++feHp6QkDAwOoqanh9u3bWLdunVyioagtAwMDLF++vMw6b8dcXlvA6/d7lDX8SnoBl5eXBy8vL+Tn52PEiBFo2rQptLW1IRKJsG3bNpw/f15u3bKOaXnLytv/xo0bo3Xr1ggNDcXff/8Nd3f3dx7z0aNHl5l8KHpCWFntVeS7fR9ffvklnJycFC5T9LSnNxMAKX19fQQFBeHy5ctISEjA5cuXsXr1avj7+2PNmjVyjxO2tLSUmyPxtqZNm2LmzJllLn87YVEUFyDbM1GW9zmnyzu3lP1OiIiI6NNRbRKL+vXr4+zZs6hXr94/NqHz8OHDaN++PX766SeZ8rKGAL2tfv36ePjwIaysrJS+K1u/fn0Ary9MbW1ty6177tw5PH/+HD/88IPc8KONGzcqFUdluLi44L///a/w77JI76CrqKi8c98+NGlP0Jvu378P4P8SNXNzc4hEImF4kbJUVVVhbW0tPO3o7t27GD16NLZs2YI1a9ZUqq369esjIyMDNjY25V7Av0vDhg1x9uxZ3LlzR2b4oKLtfahz+m0VmWtEREREn65qM8dCOqHTz88PJSUlcsvfHgb1IaioqMjdUc3Pz8fOnTsrtL6joyNKS0uxfv16hcsrE3Pfvn2hoaEBf39/uTHzwOt5AEVFRQD+747927HHx8fj+vXrFd6msvr16wcvLy/MmTNH4fAbqebNm6NJkybYu3cvHj9+LLdcLBaXO+xKGQkJCbh9+7bwWSKRICgoCACEx9Pq6+uja9euOHnypNw8Huk6GRkZFdpeZmamXJl0eNO7htYp4ujoiBcvXmDHjh0Kl1f0HOvfvz8AYMOGDSguLpZbLj2XPuQ5/baaNWvi1atX7MUgIiL6TFWbHgtLS0t4e3tj8+bNGDlyJPr06YM6deogPT0dt27dwpkzZxRO8FVG7969sW/fPixcuBAdO3bEixcvEBkZKYwPf5c+ffrA2dkZe/bswe3bt4WnIj179gzXrl3D48ePyxyz/zZjY2MsWLAAy5cvh5ubGxwcHGBiYoKMjAzcu3cPMTExCA0NhampKdq2bQtDQ0P4+vriyZMnwqNbDx8+jKZNm+LevXvKHJYK09HRkXtSkiIikQhLly7FpEmTMGLECLi4uKBx48YoKCgQHnc6depUuadCfQjNmjWDj48P3NzcYGRkhNjYWJw7dw4ODg4yw5IWLFiAr776Cl5eXnB0dETz5s1RWlqKlJQUnDp1Cg4ODhXa1+XLl+PZs2ewtbWFiYkJCgsLERUVhdzcXOFJT5UxYsQIJCQkYM2aNTh//jxsbGygra2NtLQ0nD9/XkhG36V169YYN24ctm/fjlGjRqFfv34wNDREamoqTpw4ge3bt0NXV/eDntOKYjh9+jRWrlyJNm3aQEVFBTY2NnITzYmIiOjTVG0SC+D1i9xatWqF33//Hbt27UJ+fj5q166NJk2aYM6cOR98e7NmzYK2tjaioqIQGxsLY2NjDB48GK1atSp3EvWbFi1aBGtra+zfvx/btm1DcXExDA0N0aJFC5nH5laEi4sLGjRogJCQEOzbtw/Z2dnQ19dHw4YNMWnSJOHdBrq6uli/fj3Wrl2L3bt3o6SkBC1atMCaNWsQHh7+jyUWldG8eXPs2LEDgYGBOHXqFPbu3QttbW2YmJjA2dkZNjY2H2W79vb2aNiwIbZt24YHDx6gdu3a+Oqrr+Qe/1qvXj2EhIRg+/btiI2NxZEjR6ChoQFjY2N069YNffv2rdD2HBwcEBkZiUOHDiEjIwPa2tpo3Lgxfv75Z/Tu3bvS8aupqcHX1xdhYWE4fPiwkETUqVMHlpaWZc4JUWTatGlo1qwZ9uzZg6CgIJSWlsLY2Bhdu3aVmZfxIc/pN40aNQopKSk4ceIE9u7di9LSUmzatImJBRER0WdCJOG4BCL6BIhWvfvxvUSfO8mcanU/kIhIRrWZY0FERERERJ8uJhZERERERKQ0JhZERERERKQ0JhZERERERKQ0JhZERERERKQ0JhZERERERKQ0PreOiD4J/npb4eHhAXV19aoOhYiIiBRgjwURERERESmNiQURERERESmNiQURERERESmNiQURERERESmNiQURERERESmNiQURERERESmNiQURERERESmNiQURERERESmNiQURERERESmNiQURERERESmNiQURERERESmNiQURERERESlNJJFIJFUdBBHRu4hWias6hConmaNW1SEQERGViT0WRERERESkNCYWRERERESkNCYWRERERESkNCYWRERERESkNCYWRERERESkNCYWRERERESkNCYWRERERESkNCYWRERERESkNCYWRERERESktM8qsXB2doa3t3eVrV+W1NRUWFtbw9/f/4O3/TF4e3vD2dn5o7QdGRkJa2trXLhw4YO3/bG+P/qwPuY5QERERFXns0osPjXW1tb4+uuvqzqMSlu7di2sra0xePDgqg7lg5Je8B4/flzhcmmC+PPPP//DkVWedF8q8lfRZCwmJuaTSY6JiIjon6dW1QHQp0UsFuPQoUMwNzfHo0ePcPHiRXTo0KGqwwIA7N27FyKRqKrDqBbatWuHpUuXypRt3boVycnJcuW1a9euUJsxMTE4ePAgJk6c+MHiJCIios8HEwuqlLi4OLx48QIbN27Et99+i4iIiGqTWGhoaFR1CB9dQUEB1NTUoKZW/k/X3Nwc5ubmMmUHDhxAcnIyHBwcPmaIRERE9C9V7ROL1NRU/Prrrzh37hwAoEOHDpg9ezZ8fHxgYmKCzZs3v7ONmJgYBAUF4c6dOxCJRGjWrBnGjh2LHj16KKx/+/Zt+Pr64saNG1BXV0e3bt0wY8YMmTu7ubm52L59OxISEvD48WPk5eXB2NgYvXv3hpeXF2rUqPFB9h8AcnJysHXrVpw8eRJPnz6FtrY2OnbsiMmTJ8tcPFY2plevXmHt2rWIjo5GYWEhWrVqhZkzZ5YbS3h4OMzMzGBtbY0BAwZg3759mDt3LnR0dOTq7t+/HyEhIUhNTYWxsTHc3d0V1vP390dAQAD27NmD/fv349ixY8jJyUGbNm0wf/58NGrUCCdPnsSWLVuQnJyM2rVrw8PDA0OGDJFpx9nZWe6ckJZ98803+PXXX3H58mWIRCLY2tpi3rx5MDIyqtB38C4HDhxAaGgokpOToaamhtatW8PLywtt27YV6qSmpsLFxQVeXl5yd/2lxyAiIgKmpqYAgMWLF+PgwYOIiorC2rVrcebMGWRkZCA8PBympqY4ePAg9uzZg4cPH0IsFsPQ0BBWVlaYPXs2DAwMKhz73bt34e/vj8uXLyM/Px9mZmZwcnLC6NGjoaqqCuD1vJtLly4BeD2ET2rRokVwdnZGcnIyfv/9d1y6dAlpaWkoKSmBhYUFXF1dMWjQoPc8qkRERPQpqdaJRWZmJry8vPDixQsMHToUFhYWuHz5Mnx8fJCfn1+hNkJDQ/Hzzz+jUaNG+OqrrwAABw8exJw5c/DNN9/IXZw+e/YMkyZNQq9evdC7d2/cvn0bERERuHXrFoKCgoSL8+fPnyM8PBy9evXCgAEDoKqqikuXLiEoKAiJiYlYv379BzkGOTk58PT0RFpaGlxcXNC4cWOkp6cjLCwM48ePR3BwMExMTCodk1gsxtSpU3Hz5k04ODjAysoKd+7cweTJk1GrVi2FsaSnp+Ps2bOYMGECRCIRnJ2dsXPnThw7dkzuOO7cuROrV6/GF198gSlTpqCgoAAhISHlXvAuXrwYWlpa8PDwQGZmJkJCQjBt2jT4+Phg7dq1cHV1hZ6eHsLDw/Hf//4XjRs3lrlwL8vz588xceJE9OjRA9OnT8fdu3exb98+5Obmws/PT65+Xl4eMjMz5cpfvXqlsP21a9ciKCgIlpaWmDx5MvLy8rB//35MnDgRv/zyC+zs7N4ZY3mmTJkCQ0NDTJgwAfn5+ahZsyYOHTqExYsXo127dvDx8YGmpiaePn2KM2fO4OXLlxVOLG7evAlvb2+oqanBzc0NhoaGOH36NNatW4e7d+9i+fLlAABPT09IJBJcvnxZZihVmzZtAAAXLlzApUuXYGdnB1NTUxQUFOD48eNYvnw5MjIy4OHhodQxICIiouqvWicW27dvx9OnT7Fs2TJ8+eWXAABXV1esWbMGwcHB71xfekfe3Nwc27ZtE+6Wu7q6YtSoUfD19UXfvn2hq6srrPP48WPMmjULI0eOFMoaN26MX3/9Fb///jvGjx8PADAzM8OhQ4dkhqS4u7tj48aN2LJlC65fv47WrVsrfQw2bdqElJQUBAYG4osvvhDKnZ2dMXz4cPj7+2Px4sWVjikiIgI3b96Uu3tuYWGB1atXC8nKmw4ePIjS0lI4OjoCAJo1a4YvvvgC4eHhMolFdnY2NmzYAAsLC2zdulVIxpydneHq6lrmvhoaGmL16tXCPAl9fX2sWrUKK1euxO7du1GvXj0AQL9+/eDo6Ig9e/ZUKLF49OgRfvrpJ/Tt21coU1FREXoYGjVqJFP/7TkI5UlOTkZwcDD+85//YNOmTVBXVwcADBo0CG5ubvj555/RuXNn4c7/+2jSpAmWLVsmUxYTEwNtbW1s3LhR5vv28fGpVNurVq1CcXExAgMD0axZMwDAsGHDsHDhQhw9ehQuLi7o2LEjOnXqhKNHj+Ly5csKh1I5OjrKfbcjR46Ej48Ptm3bhjFjxrxz+BYRERF92qr1U6FOnz4NIyMj9O/fX6Z8zJgxFVo/ISEB+fn5GD58uMwQHB0dHQwfPhx5eXlISEiQWUdbWxtubm4yZW5ubtDW1kZ0dLRQpq6uLlwoicVivHr1CpmZmejYsSMA4Pr16xXf0TJIJBIcOXIE7dq1Q926dZGZmSn8aWlpoXXr1oiPj3+vmGJiYqCqqopRo0bJbNPV1RXa2toK44mIiEC7du1gZmYmlDk7O+PGjRv4+++/hbL4+HgUFBTAzc1NZviVsbExBgwYUOb+Dhs2TGbytTRpsLe3F5IKADAwMEDDhg3x6NGjMtt6U506dWSSCuD/hvMoasPLywt+fn5yf29f3ANAbGwsJBIJxo4dKyQV0m06OzvjyZMnSExMrFCcZRk9erRcmY6ODgoKChAXFweJRPJe7b58+RLXrl2Dvb29kFQAgEgkgqenJwDInPPl0dLSEv5dWFiIzMxMvHr1Cp06dUJubi6Sk5PfK0YiIiL6dFTrW4ipqamwtLSEiops/lO7dm2ZXoaypKSkAHjd4/A2aZm0jpSZmZnMBSLwelKwmZmZXN3Q0FDs3bsX9+/fR2lpqcyy7Ozsd8b3LhkZGcjKykJ8fDz69OmjsM7bx6aiMaWkpMDIyEhuzoN0X9+O//Lly3j48CEcHBxkLsZbt24NFRUVhIeHY9asWULbAOR6AgDF34XU25ON9fT0AECYc/AmXV1dpKWlldnWm95MhKSkw72ysrLkljVp0gS2trZy5ampqWWWNWnSRGE7wOvj0apVqwrFqkjDhg3lyjw8PHDp0iXMmTMHtWrVQvv27dG1a1f07du3zMSwrNgVfScWFhZQUVGRO+fLkpeXh82bNyMqKgpPnz6VW17WMDIiIiL6fFTrxKI6CwkJga+vLzp16oThw4fDyMgI6urqeP78ORYvXix3Uf8+pHeiO3bsiHHjxlVpTOHh4QBeD83atGmT3PIjR45g+vTpSg13eTtJeld5Re/Ul7V+Zdr4EMp7FG5JSUmZyxQ9CKBBgwYIDQ3FuXPncP78eVy6dAnLly8XJoG/naR9bN9++y3i4uIwePBgtG/fHrVq1YKKigrOnDmDnTt3fpDfAxEREVVv1TqxMDExwaNHj1BaWipzcfjy5csK9QhIL67u378vDAeSSkpKAiB/NzslJQXFxcUyvRZFRUVISUmRuQN/+PBhmJqaYu3atTKxnT17tuI7+A4GBgbQ1dVFbm6uwjvob6tMTGZmZkhISEBOTo5Mr4V0X6W9BcDrp02dOHECtra2Cl+Kd+/ePfz222+IjY1F7969hWOanJwsd9zv37//7h3/hEj39e+//5a7mJfuq7SO9Jgquntf0Z6BN2loaMDOzk6YHB4XF4evv/4aO3bswPz589+5vrQnSNF3kpycjNLSUpnfR1mJUXZ2NuLi4uDg4IBvvvlGZpn0aW5ERET0+avWcyzs7e2Rnp6OP/74Q6a8IhO3AcDW1hZaWlrYvXs3cnNzhfLc3Fzs3r0bNWvWRKdOnWTWyc3NRWhoqExZaGgocnNzZR5Pq6qqCpFIJHPHWywWY9u2bRXcu3dTUVHBgAEDcOPGjTLfBv3y5cv3iql79+4oKSnBjh07ZMrDwsJkjhUAHDt2DPn5+Rg6dCj69Okj9zd+/HjUqFEDERERAF4fd01NTYSGhqKgoEBo5+nTp3Lf5afO3t4eIpEIwcHBEIvFQnl6ejoiIyNhYmKC5s2bA3g9f8fQ0BDnz5+X+Y4eP36MmJiYSm1X0VOrWrRoAUDx8C5FateujTZt2uDUqVO4d++eUC6RSBAYGAgA6Nmzp1AunUfxdvvSJPbt3p/09HQcOHCgQrEQERHRp69a91iMGzcOR48exZIlS3Djxg00atQIly9fxrVr16Cvr//Otyzr6upi+vTp+PnnnzF+/Hg4OTkBeP10o0ePHuGbb76Rm2Ngbm6OgIAA/P3332jZsiVu3bqFiIgINGrUCMOHDxfq9e7dG+vXr8f06dPRs2dP5Obm4o8//qj0UKBHjx7ht99+U7hs1KhRmDJlCq5evYqFCxfixIkTsLKygrq6Op48eYIzZ86gZcuWwlOhKhOTi4sL9u/fj4CAAKSkpKBNmzZITEzE8ePHYW5uLjM0Jzw8HDVq1ECXLl0UxildFhsbi2fPnqFu3bqYNGkSfH194enpCQcHBxQUFGDfvn2oX7++0pOZq5NGjRphzJgxCAoKgpeXF/r27Ss8bjYvLw/Lli2TeSKU9Cld06dPR/fu3ZGeno69e/eiSZMmuHnzZoW3O2XKFOjq6qJdu3YwNjZGdnY2IiMjIRKJKvUCvDlz5sDb2xteXl7C42bj4uLw559/YsCAATI9TlZWVtizZw9WrFgBOzs74X0dZmZm6NSpE44cOQJNTU1YWlriyZMn2LdvH8zMzCqc6BAREdGnrVonFvr6+vjtt9/g6+uLiIgIiEQidOjQAZs2bcLYsWOhqan5zjbc3NxgZGSE4OBgBAQEAAC++OILrFq1SuEL8urWrYsVK1bA19cXf/zxB9TV1TFgwAB8/fXXMk++GTNmDCQSCcLDw/HLL7/A0NAQffv2hYuLi9xTpcrz4MEDhXMWgNePLDUyMsLWrVsREhKCqKgonDp1Cqqqqqhbty7atm0r8/KxysSkrq4OPz8/rFmzBrGxsTh58iRatWoFPz8/+Pr64smTJwBeD/G5fv06evbsWe5L/3r16oWTJ0/i4MGD8PT0xOjRo6GlpYUdO3bAz88PxsbGGD16NHR0dCr1ONdPwfTp01G/fn2EhoZi/fr1UFdXh6WlJZYvX4527drJ1B03bhxycnJw+PBhXLx4ERYWFvj+++9x69atSiUWrq6uiIqKwr59+5CVlYVatWqhefPmmDdvnswL7N6lVatW2Lp1K/z9/REWFia8IG/atGlyT6Pq378/EhMTcezYMZw4cQKlpaVYtGgRzMzMsGzZMqxbtw6nT5/GoUOHUL9+fUyePBlqampYsmRJheMhIiKiT5dI8k/OXv1AMjMz0adPHwwZMkRuTDcRfZ5Eq8TvrvSZk8yp1veCiIjoX65az7EAIDNGX2r79u0AUKEJzURERERE9PFV+9tfM2bMgImJCVq0aIHS0lKcP38ep0+fRps2bRQOZSIiIiIion9etU8sunXrhkOHDiE6OhqFhYXCWH0vLy+ZSbFERERERFR1Psk5FkT078M5FpxjQURE1Vu1n2NBRERERETVHxMLIiIiIiJSGvvVieiT4K+3FR4eHlBXV6/qUIiIiEgB9lgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSmFgQEREREZHSRBKJRFLVQRARvYtolbiqQ/ggJHPUqjoEIiKij4I9FkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFp+4xYsXw9rausq3WRVxVOV2P3cXLlyAtbU1IiMjP3jb3t7ecHZ2/uDtEhERUdX67F4Bm5iYiJiYGDg7O8PU1LSqw6kwf39/BAQECJ/V1NSgo6MDc3NzWFlZwcXFBc2aNftg20tNTUVkZCR69OiB5s2bf7B2P4bIyEhkZ2dj5MiRVR0KEREREZXhs+uxuHPnDgICApCamlrVobwXHx8fLF26FN999x08PT3RqFEjREREYOTIkfD19ZWr/9133+HMmTOV3k5qaioCAgJw586dSq/7vtt8X5GRkdi1a1e1iIWIiIiIFPvseiw+dV26dEGrVq1kymbOnIkFCxYgJCQE+vr6GD9+vLBMTU0Namof/2uUSCTIz89HzZo1/7FtVkR1ioWIiIjo3+yzuiJ7cziRj4+PUO7k5ITFixejqKgIISEhOHr0KB4/fgwNDQ20a9cOEydORIsWLYT6paWlCAwMRHx8PB4+fIisrCwYGhrCzs4OkyZNgr6+vlA3NTUVLi4u8PLyQuPGjREYGIgHDx6gTp068PT0hIuLC9LS0rB69WpcuHABYrEY3bt3x4IFC6CtrV2h/apVqxZWrFgBFxcXbNu2DcOGDYOWlhaA13MMDh48iAsXLgj109LS4O/vj/Pnz+PFixfQ0dFB/fr1MWTIEDg5OSEyMhJLliwBACxZskT4d/v27bF582ZcuHABPj4+WLRoEfLz8xEaGorHjx9j/PjxmDhxosJtSmVkZODXX3/FmTNnUFhYCCsrK8yYMUPm+L7Z/ttj7d9u29nZGU+ePAEAmbkUmzZtgrW1dZmx3L17F/7+/rh8+TLy8/NhZmYGJycnjB49GqqqqnLbi4mJwbp163Dy5Enk5uaiRYsWmDVrFlq3bl2h7+htx44dw+7du3H37l2UlJSgadOmGDNmDPr06SNTz9raGk5OThgyZAjWr1+PmzdvQlNTEz169MDs2bNRs2ZNmfrp6ekIDAxEXFwcnj17Bh0dHTRr1gxjx45Fp06dhHqXLl3Cb7/9hhs3bkAsFqNRo0Zwc3PDoEGD5GKNiYnB5s2bkZycDAMDAzg5OaFdu3YK96uivyEAePXqFdauXYvo6GgUFhaiVatWmDlz5nsdTyIiIqr+PqvEolevXkhPT8f+/fvh4eEBCwsLAIC5uTnEYjGmTZuGa9euwcHBAe7u7sjJycH+/fsxYcIEBAQECD0FxcXFCA4ORq9evdC9e3fUqFEDN2/eRHh4OK5cuYKQkBCoq6vLbDsuLg779u2Dq6sr9PT0EB4ejqVLl0JdXR1+fn6wsbHB5MmTcfPmTUREREBDQwPff/99hfetVq1a6NmzJw4ePIgrV66gc+fOCuuJxWJMmTIFz58/h6urKxo0aICcnBzcu3cPly9fFi4aPTw8EBgYiMGDBwsXkbVr15Zpa9euXcjKysKgQYNgaGgIY2Pjd8Y5bdo06OnpwcvLCy9evMCePXvg7e2NrVu3omnTphXeX6nZs2dj/fr1yMzMxKxZs4Ry6XeryM2bN+Ht7Q01NTW4ubnB0NAQp0+fxrp163D37l0sX75cbp2pU6fCwMAAX331FbKysrBjxw7MmDEDERERFU4ApTZs2ICtW7eiS5cu8PHxgYqKCqKjo7FgwQLMmzcP7u7uMvXv3LmDmTNnwtnZGf3798fFixcRHh4OFRUVfPvtt0K91NRUTJgwAS9fvoSDgwNatWqF/Px8/PXXXzh37pyQWJw6dQpz586FoaEhRo8ejZo1a+LYsWNYvnw5UlJSMGXKFKHN6OhozJs3D6ampvjqq6+gqqqKyMhIxMXFye1XZX5DYrEYU6dOxc2bN+Hg4AArKyvcuXMHkydPRq1atSp1PImIiOjT8FklFs2aNUObNm2wf/9+2Nraytzh3rFjBy5evIh169bJXJS7urpi2LBh8PX1xebNmwEAGhoaOHr0KGrUqCHTfps2bbB8+XLExMSgb9++MsuSkpIQGhoKExMTAEC/fv3g6OiIH374ATNmzMDo0aOFutnZ2Th06JDCO9LlkV6YP3jwoMzEIikpCQ8ePMC0adMwbtw4hXXMzc1ha2uLwMBAtGnTBg4ODgrrpaWlISwsTC7hKI+JiQlWrlwJkUgE4HWyN3bsWKxZswbr1q2rcDtSPXr0wM6dO1FYWFhmnG9btWoViouLERgYKEx4HzZsGBYuXIijR4/CxcUFHTt2lFmnRYsWWLBggfC5cePGWLBgAY4ePYqhQ4dWON7bt29j69at8PDwkLmAHz58OGbPng0/Pz84OjrKJCt3795FYGCg0DsydOhQ5ObmIiIiAjNnzhTOkRUrVuD58+dy5zDwupcNAEpKSrBy5UpoaWlh+/btqFOnDgDA3d0dEydOxPbt2+Hs7IwGDRqgpKQEq1atgp6eHrZv3y70xA0dOhTDhw+X27fdu3dX+DcUERGBmzdvwsvLCxMnThTqWlhYYPXq1cLvhIiIiD4fn93k7bIcOXIEjRo1QsuWLZGZmSn8icVi2Nra4urVqygoKAAAiEQiIakoKSlBdnY2MjMzYWNjAwC4fv26XPs9evSQuVgyMDBAw4YNoaKiIneHum3bthCLxZWeYK6jowMAyM3NfWedixcv4uXLl5Vq/22Ojo6VSioAYOzYsUJSAQAtW7aEra0tzp07h7y8PKXiqYiXL1/i2rVrsLe3l3mKlkgkgqenJ4DXd+nf9vYTp6RJ6aNHjyq1/SNHjkAkEsHR0VHmPMvMzIS9vT1yc3Px119/yaxjZWUlN+TKxsYGJSUlwjmSlZWFP//8E126dFGYVKqovP4p37p1C2lpaXBxcRGSCgBQV1fH2LFjUVpaitjYWKHu06dP4eLiIjO8T0dHR2EyVZnfUExMDFRVVTFq1CiZNlxdXSvdA0RERESfhs+qx6I8SUlJKCwslBvj/qbMzEzUq1cPABAVFYWQkBAkJiZCLBbL1Hv16pXcumZmZnJlurq6MDIygoaGhky5np4egNcXi5WRk5MDAOVemJmYmMDT0xPbtm3DgAED8MUXX8DGxgZ9+vSBpaVlpbbXoEGDStUHFA9RsrCwQHx8PJ48eYImTZpUus3KkF6IN27cWGEcKioqSElJkVv29vcnvdCu7HeUlJQEiUQCV1fXMuu8ePGi3G0DEIYLSbf/6NEjSCSSdz4auLz9lx576f5L/7dhw4ZydRV9j5X5DaWkpMDIyEhIdKU0NDRgZmaG7OzscveDiIiIPj3/msQCeD2UqLzJowYGBgCAkydPYuHChbC0tMScOXNgbGwMDQ0NlJaWYtq0aZBIJHLrSu8YV7QcgMJ2ynPv3j0AQKNGjcqtN3nyZLi4uCAuLg5XrlxBeHg4goODMXbsWEyfPr3C23t7KNiH8maPxttKSko+yjbf5c0J3W+q7HcEvN6/tWvXlvndv51clbXt993+x1TR3xARERH9+3x2iUVZF63169dHRkYGbGxsyr3YB4DDhw9DU1MT/v7+MhfXycnJHzLUSsnKykJ0dDR0dHTQtm3bd9Y3NzfH8OHDMXz4cBQWFmLatGkICgrC6NGjUbt27XIv7pWRlJQEKysruTJVVVVhqNjbd+PfpKg3oTKxSl+KeP/+fbllycnJKC0tVdhD8KHUr18fZ8+eRb169cqdYP4+7YpEIiQmJpZbT7pvivZfWiatI/3fBw8eyNVNSkpSGENFf0NmZmZISEhATk6OTK9FUVERUlJShF47IiIi+nx8dnMspI9hfXu4kqOjI168eIEdO3YoXO/N4SnSiybphFjg9Z3jLVu2fOhwKyQrKwsLFixAbm4uPD09y+1JyMnJkRu6pampKfRySI+LdEJwZYf6vEtQUJDMXfbbt2/j3LlzsLGxEbZpamoKVVVVnDt3Tmbdq1evys0/kMb66tWrCt29r127Ntq0aYNTp04JPTzA6+8vMDAQANCzZ8/32reKkE4w9/PzU9j78vYwqIqqVasWunTpgrNnzyIhIUFuufTYtGjRAvXq1UNkZCTS09OF5WKxGMHBwRCJROjevTuA1/NfjI2NERERgczMTKFuTk4O9u7dK7eNyvyGunfvjpKSErm6YWFh5c4RIiIiok/XZ9djYWlpCRUVFWzduhWvXr2ClpYWzMzMMGLECCQkJGDNmjU4f/48bGxsoK2tjbS0NJw/fx4aGhrw9/cHAPTu3RsnT56Ej48PHB0dIRaLERsbK0xM/ZjOnj0r3FnPzs5GYmIioqOjkZeXhzFjxmDs2LHlrn/hwgX8+OOP6NWrFxo2bIiaNWvi1q1bCA8PR+vWrYUEw8LCAtra2ggLC0ONGjWgq6uL2rVrCxPU39eTJ08wdepU2NvbIz09HXv27IGmpiZmzJgh1KlZsyacnZ1x4MABfPPNN+jQoQMePXqEyMhINGvWTO5t4K1bt8bp06excuVKtGnTBioqKrCxsSlzYvmcOXPg7e0NLy8v4XGzcXFx+PPPPzFgwAC5J0J9SJaWlvD29sbmzZsxcuRI9OnTB3Xq1EF6ejpu3bqFM2fOID4+/r3anjdvHjw9PTF9+nQ4OTmhZcuWKCgowI0bN2BiYoLp06dDVVUV8+bNw9y5czFu3DgMHjwYNWvWRFRUFP766y94eHgIc2dUVVUxc+ZMLFy4EOPGjcOgQYOgqqqKiIgI1KpVC2lpaTLbr8xvyMXFBfv370dAQABSUlLQpk0bJCYm4vjx4zA3N6+yIW9ERET08Xx2iUW9evXwww8/YPv27VixYgXEYrHwgjxfX1+EhYXh8OHDwgVQnTp1YGlpCScnJ6GN/v37Iy8vDzt37sSaNWugq6sLe3t7TJ06Fb179/6o8W/atAnA6zdKa2trw8zMDC4uLnBxcZF5ylFZmjVrhp49e+LixYs4evQoSkpKUK9ePXh4eMg88rZGjRr48ccfsXHjRqxevRpFRUVo37690onFunXrsHr1amzevBkFBQXCC/Lejn3WrFmQSCSIiYlBbGwsWrZsidWrV2P//v1yicWoUaOQkpKCEydOYO/evSgtLcWmTZvKTCxatWqFrVu3wt/fH2FhYcIL8qZNmyZzDD4Wb29vtGrVCr///jt27dqF/Px81K5dG02aNMGcOXPeu10zMzMEBwfjt99+w5kzZ3Do0CHo6emhWbNmGDx4sFDP3t4eGzZswJYtWxAcHIzi4mI0atQI3333ndwL8vr06QMVFRX89ttv2Lx5M2rXri2862Tq1KkyddXU1Cr8G5K+v2XNmjWIjY3FyZMn0apVK/j5+cHX11d46SERERF9PkSS6jY7lIhIAdEq8bsrfQIkcz67+zlEREQAPsM5FkRERERE9M/jrTOiCnhzInRZdHR0PtojeomIiIiqOyYWRBUwYMCAd9ZZtGgRnJ2d/4FoiIiIiKofJhZEFeDn5/fOOh/7reJERERE1RkTC6IKsLW1reoQiIiIiKo1Tt4mIiIiIiKlsceCiD4J/npb4eHhAXV19aoOhYiIiBRgjwURERERESmNiQURERERESmNiQURERERESmNiQURERERESmNiQURERERESmNiQURERERESmNiQURERERESmNiQURERERESmNiQURERERESmNiQURERERESmNiQURERERESmNiQURERERESlNJJFIJFUdBBHRu4hWieXKJHPUqiASIiIiUoQ9FkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkT/gAsXLsDa2hqRkZFVHcp78ff3h7W1NVJTU6s6FCIiIqqm1Ko6AKqeEhMTERMTA2dnZ5iamlZ1OBXm7++PgIAABAUFoVWrVlUdToUUFhYiIiICJ06cwL1795CdnQ0tLS00aNAA1tbWcHFxQaNGjao6TCIiIqJyMbEghe7cuYOAgAB06NDhk0osPjWPHz/GzJkzkZSUhPbt22PkyJEwMjJCXl4e7ty5g4iICISEhODgwYOoW7duVYdLREREVCYmFkRVpKCgAF9//TUeP36M//3vf+jZs6dcncLCQuzcuRMikagKIvy4xGIxSkpKoKmpWdWhEBER0QfAxILkSIcTAYCPj49Q7uTkhMWLF6OoqAghISE4evQoHj9+DA0NDbRr1w4TJ05EixYthPqlpaUIDAxEfHw8Hj58iKysLBgaGsLOzg6TJk2Cvr6+UDc1NRUuLi7w8vJC48aNERgYiAcPHqBOnTrw9PSEi4sL0tLSsHr1aly4cAFisRjdu3fHggULoK2t/V77mZCQgKCgINy4cQNFRUVo0KABXF1d4erqKlMvPj4e4eHhuHnzJtLT06Gurg5LS0t4enqiQ4cOcu3GxMRg8+bNSE5OhoGBAZycnNCuXTu5egcOHEBycjI8PDwUJhUAoKmpCQ8PD7nynJwcbN26FSdPnsTTp0+hra2Njh07YvLkyTA3NxfqRUZGYsmSJdi4cSNu376NsLAwPHv2DCYmJvD09ISTk5NMu6Wlpdi+fTv279+P9PR0mJubK9y+VHp6OgICAhAXF4cXL15AX18f3bp1w6RJk1C7dm2hnvSc2r17N8LDw3H8+HGkp6djw4YNsLa2LrN9IiIi+nQwsSA5vXr1Qnp6Ovbv3w8PDw9YWFgAAMzNzSEWizFt2jRcu3YNDg4OcHd3R05ODvbv348JEyYgICBAmNtQXFyM4OBg9OrVC927d0eNGjVw8+ZNhIeH48qVKwgJCYG6urrMtuPi4rBv3z64urpCT08P4eHhWLp0KdTV1eHn5wcbGxtMnjwZN2/eREREBDQ0NPD9999Xeh/37duHn376CVZWVvD09ISWlhYSEhKwYsUKpKSkYMaMGULdyMhIZGVlwcHBAcbGxnj27BnCw8MxefJkbNq0SSZpiI6Oxrx582BqaoqvvvoKqqqqiIyMRFxcnFwMJ0+eBAAMGjSoUrHn5OTA09MTaWlpcHFxQePGjZGeno6wsDCMHz8ewcHBMDExkVnHz88PhYWFGDJkCDQ0NBAWFobFixfD3Nwcbdu2Fer9+uuv2LVrlzAs6+XLl/j5559hZmYmF0daWho8PDxQXFyMgQMHwtzcHI8ePcLevXtx4cIFBAcHQ0dHR2ad77//Hpqamhg1ahREIhGMjIwqte9ERERUjUmIFIiIiJB06NBBcv78eZnykJAQSYcOHSRnz56VKc/OzpY4ODhIvLy8hLLS0lJJfn6+XNv79++XdOjQQXLs2DGhLCUlRdKhQwdJ165dJampqUL5y5cvJZ07d5ZYW1tLgoODZdqZM2eOxNbWVpKbmyuUbdq0SdKhQwfJjRs3yty358+fSzp37iz55ptv5Jb973//k9jY2EgePXoklOXl5cnVS09Pl/Tq1Usybdo0oUwsFkscHBwkvXr1kmRkZAjl2dnZEkdHR0mHDh0kERERQnmvXr0k9vb2cm2LxWJJRkaGzN+bx/F///ufpEuXLpLExESZ9VJTUyX29vaSRYsWCWXS73HEiBGSoqIiofzp06eSTp06SRYuXCiUJSUlSaytrSU+Pj4SsVgslN+6dUtibW0t6dChgyQlJUUonzlzpqRPnz6StLQ0mThu3Lgh6dixo2TTpk1CmfR78fLykhQXF8vtc0Xgf8Vyf0RERFR98HGzVClHjhxBo0aN0LJlS2RmZgp/YrEYtra2uHr1KgoKCgAAIpEINWrUAACUlJQgOzsbmZmZsLGxAQBcv35drv0ePXrI3G03MDBAw4YNoaKiAnd3d5m6bdu2hVgsrvQjUI8fP46ioiIMHDhQZh8yMzPRrVs3lJaW4ty5c0J9LS0t4d95eXnIzMyEqqoqWrdujRs3bgjLbt26hadPn8LFxUVmmJeOjg6GDh0qF0dOTo7cHX0ASEpKQp8+fWT+QkNDAQASiQRHjhxBu3btULduXZnYtbS00Lp1a8THx8u16ebmJtM7VLduXTRo0ACPHj0SymJjYyGRSDBq1CioqqoK5S1atICtra1c7HFxcbC3t4empqZMHKampjA3N0dCQoJcHCNHjoSaGjtKiYiIPkf8f3iqlKSkJBQWFqJPnz5l1snMzES9evUAAFFRUQgJCUFiYiLEYrFMvVevXsmtq2jIja6uLoyMjKChoSFTrqenBwDIysqq1D4kJycDACZPnlxmnZcvXwr/fvz4Mfz8/BAfH4/s7GyZem9Oqk5JSQEANGzYUK496XCyN+no6CAnJ0eu3MzMDH5+fgCAu3fvwtfXV1iWkZGBrKwsxMfHl/kdqKjI3y9QdFxr1aqFtLQ0ufgVPdrWwsJCJmFJTk5GaWkpwsPDER4erjAORdts0KCBwrpERET06WNiQZXWtGlTzJw5s8zlBgYGAF7PIVi4cCEsLS0xZ84cGBsbQ0NDA6WlpZg2bRokEoncuoouissrB6CwnfJI6y9ZsqTMMf7Si+K8vDx4eXkhPz8fI0aMQNOmTaGtrQ2RSIRt27bh/Pnzldr2m5o0aYJLly4hJSVF5iJcS0tL6CF4s+fgzdg7duyIcePGVXhbZR2/yh67t3355ZdyE8ClFD3tSdqDRURERJ8fJhakUFmPN61fvz4yMjJgY2NT7sU+ABw+fBiamprw9/eXuaCU9hhUlfr16wMA9PX15Yb4vO3cuXN4/vw5fvjhB7i4uMgs27hxo8xnaXLw4MEDuXaSkpLkynr16oVLly7hwIEDmDJlSoViNzAwgK6uLnJzc98Ze2VJ409OTpZ5shQgH7+5uTlEIpEwBI6IiIiIcyxIIem8greHKzk6OuLFixfYsWOHwvVevHgh/FuaeJSWlgplEokEW7Zs+dDhVkrfvn2hoaEBf39/YT7Im3JyclBUVATg/3oM3r6zHx8fLzdHpGXLljA2NkZERAQyMzNl2tu7d6/cdgYNGoRGjRohODgY0dHRFYpdRUUFAwYMwI0bN3D8+HGFdd4cxlUZ3bt3h0gkwo4dO1BSUiKU3759W2bOCfA6KevatStOnjyJv/76S64tiUSCjIyM94qDiIiIPk3ssSCFLC0toaKigq1bt+LVq1fQ0tKCmZkZRowYgYSEBKxZswbnz5+HjY0NtLW1kZaWhvPnzwsX7ADQu3dvnDx5Ej4+PnB0dIRYLEZsbKzCi/kPLSIiAmfPnpUrb9myJbp27YoFCxZg+fLlcHNzg4ODA0xMTJCRkYF79+4hJiYGoaGhMDU1Rdu2bWFoaAhfX188efIEdevWxZ07d3D48GE0bdoU9+7dE9pWVVXFzJkzsXDhQowbNw6DBg2CqqoqIiIi5OYzAK+HBfn6+mLmzJmYO3cuOnTogE6dOsHQ0BC5ublITk5GVFQUVFVVYWxsLKw3ZcoUXL16FQsXLsSJEydgZWUFdXV1PHnyBGfOnEHLli2xePHiSh+zRo0awc3NDXv27MGkSZPQq1cvvHz5Env27EGzZs2QmJgoU3/BggX46quv4OXlBUdHRzRv3hylpaVISUnBqVOn4ODggIkTJ1Y6DiIiIvo0MbEgherVq4cffvgB27dvx4oVKyAWi4UX5Pn6+iIsLAyHDx8Wkog6derA0tJSZrx9//79kZeXh507d2LNmjXQ1dWFvb09pk6dit69e3/U+MPCwhSWDxkyBF27doWLiwsaNGiAkJAQ7Nu3D9nZ2dDX10fDhg0xadIkGBoaAng9cXz9+vVYu3Ytdu/ejZKSErRo0QJr1qxBeHi4TGIBAH369IGKigp+++03bN68GbVr1xZekDd16lS5eMzNzREcHIyIiAicOHECISEhyMnJgZaWFurXr4+BAwdi4MCBMhOqdXR0sHXrVoSEhCAqKgqnTp2Cqqoq6tati7Zt21b6vRhvmjNnDgwNDbF//36sWbMG9evXx/z58/Hw4UO5xKJevXoICQnB9u3bERsbiyNHjkBDQwPGxsbo1q0b+vbt+95xEBER0adHJFF29iYR0T9AtEosVyaZw3sjRERE1QXnWBARERERkdKYWBARERERkdKYWBARERERkdKYWBARERERkdKYWBARERERkdKYWBARERERkdL4rEYi+iT4622Fh4cH1NXVqzoUIiIiUoA9FkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDQmFkREREREpDSRRCKRVHUQRETvIlollvksmaNWRZEQERGRIuyxICIiIiIipTGxICIiIiIipTGxICIiIiIipTGxICIiIiIipTGxICIiIiIipTGxICIiIiIipTGxICIiIiIipTGxICIiIiIipTGxICIiIiIipfHVtUT/YhcuXICPjw9mzJiBMWPGlFtHSkVFBdra2qhTpw5atmyJ/v37o3PnzhCJRDLr5eXlISQkBLdu3UJiYiKePXuG9u3bY/PmzR91n4iIiKhqMLEgogrp378/unbtColEgry8PDx48AAxMTE4dOgQOnbsiJ9//hm6urpC/czMTGzevBmGhoZo0aIFXrx4UYXRExER0cfGxIKIKqRFixZwcHCQKZs5cybWrl2LHTt24Ntvv8XatWuFZUZGRjh06BCMjY0BAN26dftH4yUiIqJ/FudYENF7U1VVxcyZM9G2bVucPXsWV65cEZZpaGgISQURERF9/phYEJHSBg4cCACIi4ur4kiIiIioqjCxICKlNWvWDADw4MGDKo6EiIiIqgoTCyJSmra2NgAgNze3iiMhIiKiqsLEgoiUJk0opAkGERER/fswsSAipd29excA0KhRo6oNhIiIiKoMEwsiUlp4eDgAoGvXrlUcCREREVUVvseCiN5bSUkJ1q1bhytXrqBr165o27ZtVYdEREREVYSJBRHh/PnzKCwslCvX19cXhjfdvn0bhw8fBgCZN28/efIEnTp1wo8//ii3/u7du5GdnQ0AEIvFSEtLw2+//QYA+OKLL2Bvb/+R9oiIiIj+aSKJRCKp6iCIqGpcuHABPj4+ZS5v2LAhFi5cKFNHRUUFWlpaMDY2RsuWLdG/f3906dJF4frOzs548uSJwmVOTk5YvHhxhWMVrRLLfJbM4X0RIiKi6oSJBRF9EphYEBERVW+cvE1EREREREpjYkFEREREREpjYkFEREREREpjYkFE9Jlp1KgRxo8fX9VhEBHRvwxnPxLRZ+/tid9V4UNMNv/777+xcuVKREVFITU1FRoaGrCysoK7uzu8vb2hpaX1ASL9eAoLC/HDDz8gODgYGRkZaNOmDZYvX46+fftWdWhERPQBMLEgIvoEHDp0CG5ubtDU1MTYsWPRunVrFBUVIS4uDnPnzsWNGzewefPmqg6zXOPHj0dYWBi+/vprNGvWDNu2bYODgwOio6NhZ2dX1eEREZGSmFgQ0SfBX28rPDw8oK6uXtWh/OOSkpIwfPhwNGzYECdPnoSJiYmwbMqUKbh37x4OHTpUhRG+27lz5/D777/jf//7H+bMmQMAQoI0b948nD17toojJCIiZXGOBRFRNbdy5Urk5ORgy5YtMkmFVNOmTTFjxowy13/58iXmzJkDKysr6OjoQE9PD19++SWuXr0qV3fdunWwtLREzZo1YWBgAGtra+zcuVNYnp2dja+//hqNGjWCpqYm6tati759++LSpUvl7kNYWBhUVVXh7e0tlNWoUQMTJkzAn3/+iUePHlXkUBARUTXGHgsiomouMjISjRs3LvMN5+9y//59HDhwAG5ubrCwsMDTp0/h7++P7t274+bNmzA1NQUABAQEYPr06XB1dcWMGTNQUFCAa9euISEhASNHjgQA+Pj4ICwsDFOnTkWrVq3w4sULxMXF4datW2jfvn2ZMVy+fBlffPEF9PT0ZMo7duwIALhy5Qrq16//XvtHRETVAxMLIqJq7NWrV0hJScHAgQPfuw0rKyvcuXMHKir/10k9ZswYtGjRAlu2bMH3338P4PU8DktLS4SGhpbZ1qFDh+Dl5YVffvlFKJs3b947Y3jy5InC3hZpWWpqaoX3h4iIqicOhSIiqsZevXoFANDV1X3vNjQ1NYWkoqSkBC9evICOjg6aN28uM4RJX18fjx8/xvnz58tsS19fHwkJCZVOBPLz86GpqSlXXqNGDWE5ERF92phYEBFVY9KhQ9nZ2e/dRmlpKX799Vc0a9YMmpqaMDIyQp06dXDt2jVkZWUJ9ebPnw8dHR107NgRzZo1w5QpU3DmzBmZtlauXInr16+jfv366NixIxYvXoz79++/MwYtLS0UFhbKlRcUFAjLiYjo08bEgoioGtPT04OpqSmuX7/+3m3897//xaxZs2Bvb4+QkBD88ccfiIqKgqWlJUpLS4V6LVu2RGJiIn7//XfY2dlh7969sLOzw6JFi4Q67u7uuH//PtatWwdTU1P873//g6WlJY4cOVJuDCYmJnjy5IlcubRMOs+DiIg+XUwsiIiqOScnJ/z999/4888/32v9sLAw9OzZE1u2bMHw4cPRr18/9OnTB5mZmXJ1tbW1MWzYMAQGBuLhw4dwdHTEjz/+KPQsAK+ThMmTJ+PAgQNISkqCoaEhfvzxx3JjaNu2Le7cuSMM7ZJKSEgQlhMR0aeNiQURUTU3b948aGtr46uvvsLTp0/llv/9999Ys2ZNmeurqqpCIpHIlIWGhiIlJUWm7MWLFzKfNTQ00KpVK0gkEhQXF6OkpERm6BQA1K1bF6ampgqHOb3J1dUVJSUlMi/xKywsRGBgIGxtbflEKCKizwCfCkVEVM01adIEO3fuxLBhw9CyZUuZN2+fPXsWoaGhGD9+fJnrOzk5YenSpfDw8ECXLl3w119/YceOHWjcuLFMvX79+qFevXro2rUrjI2NcevWLaxfvx6Ojo7Q1dVFZmYmzM3N4erqiv/85z/Q0dHB8ePHcf78eZmnRClia2sLNzc3LFy4EM+ePUPTpk2xfft2JCcnY8uWLR/iMBERURVjYkFEnz3JnE//P3UuLi64du0a/ve//yE8PBwbN26EpqYm2rRpg19++QVeXl5lrvvNN98gNzcXO3fuxO7du9G+fXscOnQICxYskKk3ceJE7NixA6tXr0ZOTg7Mzc0xffp0fPfddwCAmjVrYvLkyTh27Bj27duH0tJSNG3aFBs2bMCkSZPeuQ9BQUH4/vvvERwcjIyMDLRp0wYHDx6Evb29cgeHiIiqBZHk7f5xIqJqaPPmzfDw8IC6unpVh0JEREQKcI4FEREREREpjYkFEREREREpjYkFEREREREpjYkFEREREREpjYkFEREREREpjYkFEREREREpjYkFEREREREpjYkFEREREREpjYkFEREREREpjYkFEREREREpjYkFEREREREpjYkFEREREREpjYkFEREREREpjYkFEREREREpjYkFEREREREpjYkFEREREREpTa2qAyAieheJRIL8/Hy8evUK6urqVR0OERHRv46uri5EIlG5dUQSiUTyD8VDRPRe0tPTUadOnaoOg4iI6F8rKysLenp65dZhjwURVXuamppo27YtDh06BB0dnaoO55OTk5MDR0dHHr/3xOOnPB5D5fD4KYfH78PQ1dV9Zx0mFkRU7YlEIqiqqkJPT4//p/AeVFRUePyUwOOnPB5D5fD4KYfH75/DydtERERERKQ0JhZERERERKQ0JhZEVO1paGjAy8sLGhoaVR3KJ4nHTzk8fsrjMVQOj59yePz+OXwqFBERERERKY09FkREREREpDQmFkREREREpDQmFkREREREpDQmFkT0j0lOTsbkyZNhZ2eH/v37Y82aNSguLn7nehKJBNu2bYOjoyO6du0KDw8P/PXXX3L1nj9/jrlz58Le3h69evXCsmXLkJOT8zF2pUp8zOOXkJCA/9fefcc1kTT+A/+EEkIJoBAETgUUAQs2ULChIGJDVLAXwHIo6BcrtuMUsRdQPGwoRRSfs6GcvXs+3llR7Kh3FBtKkS7SMr8//GUf1iQYSkBx3q8XL81kdnZmdnazszs7u3jxYgwZMgQ9evTAyJEjER0djbKyMnkVp87Ju/2JCIVCTJgwAdbW1rhw4UJtFqHe1UUdXrt2DZMnT0bPnj1hb2+PadOm4f3797VdlHoh7/pLSEjAtGnTYG9vj759+8LX1xfPnj2TR1HqRXXr79ChQ5g9ezYcHR0r3S8b+m9IXaAdC4qi6kReXh6mT5+OsrIybNiwAT4+Pjh69CiCg4O/uuyePXuwc+dOjBs3Dps2bYKuri5mzpyJ169fM3HKysowc+ZMvHz5EitXrsSiRYtw48YN+Pv7y7NYdUbe9RcbG4uPHz9i2rRpCAkJweDBg7Fz506sWrVKnsWqM/Kuv4piY2ORkZFR20Wod3VRh6dOnYKfnx+srKywefNmBAQEoE2bNigpKZFXseqMvOsvJSUFM2bMgKqqKlatWoVff/0Vubm58PHxQWZmpjyLVidqUn8nT55ETk4OevToITVOQ/8NqTOEoiiqDkRERJCePXuSnJwcJuzIkSOka9euJD09Xepynz59InZ2diQ0NJQJKykpIc7OzmTNmjVM2OnTp4m1tTVJTk5mwq5fv06srKzIw4cPa7cw9UDe9ZednS22bHh4OLG2tpb43fdG3vUnkp2dTRwcHEhcXByxsrIi58+fr92C1CN512FOTg6xs7Mjhw4dkk8B6pm86y8yMpJ0796dFBUVMWGvX78mVlZW5MSJE7VcmrpX3fojhJDy8nJCCCFv3ryRul829N+QukLvWFAUVSf+/vtvdO3aFVpaWkxYv379IBQKcePGDanLPXjwAIWFhXB0dGTClJWVYW9vj7/++ouVfqtWrWBsbMyE2djYQEtLixXveyXv+tPW1hZb1tzcHISQBnG1U971JxIaGgorKytYW1vXbgG+AfKuw/Pnz0MoFGLo0KHyKUA9k3f9lZWVQVlZGSoqKkyYhoYGgM9Dqb531a0/AFBQ+PrpbkP/DakrtGNBUVSdSElJYR2wAYDP50NXVxcpKSmVLgdAbFkTExO8e/cOnz59YuIZGRmx4nA4HBgZGVWa/vdC3vUnSUJCArhcLgwNDauZ629HXdTfo0ePcObMGcyePbt2Mv2NkXcdPnr0CMbGxjhx4gScnZ1hY2ODcePGNZiTOnnXn5OTE8rLy7Ft2zbk5OQgIyMDwcHBaNKkCfr06VN7Bakn1a2/qqTfkH9D6grtWFAUVSfy8vLA5/PFwvl8PvLy8ipdjsvlsq7CiZYjhCA/Px8AkJ+fLzF9TU3NStP/Xsi7/r708uVL/P7773Bzc4OamlrNMv8NkHf9CYVCrF+/HhMmTGgQHTFJ5F2HWVlZSE1NxY4dOzB9+nSEhITAwMAAc+fOxb///lu7hakH8q6/5s2bY/v27fjjjz/g6OiIgQMH4t69e9i2bRtz5+J7Vt36k1VD/w2pK7RjQVEURbEUFBTAz88PhoaG8PHxqe/sfBeOHTuGrKwseHp61ndWvltCoRAfP37EkiVL4OzsDFtbW6xbtw5NmjTBnj176jt737zU1FQsWLAANjY22Lp1KzZt2gR9fX34+voiKyurvrNH/SBox4KiqDqhqakpcdq+/Px8aGpqVrpcSUkJiouLxZbjcDjMFSY+ny8x/by8vErT/17Iu/5ESktL4efnh/z8fISEhEBVVbV2ClDP5Fl/Hz9+xNatWzF58mSUlpYiPz8fhYWFAIBPnz41mOkq5d0GRWl06dKFiaOkpIROnTohKSmpNopQr+Rdf1u3boWOjg4CAwNhY2ODXr16YfPmzcjPz8fvv/9eu4WpB9WtP1k19N+QukI7FhRF1QljY2OxcaoFBQXIzMwUGzf75XLA56txFaWkpEBfXx88Hk9q+oQQpKamVpr+90Le9Qd8vmLs7++Pp0+fYsuWLdDX16+t7Nc7edZfTk4OcnNzsWbNGtjb28Pe3h5jx44FAAQEBMDNza02i1Jv5N0GW7RoITWNhjDdrLzrLzk5Ga1atWLFUVNTQ9OmTaVOjfw9qW791ST9hvQbUldox4KiqDrRvXt33Lp1izWm/8KFC1BQUICtra3U5dq3bw91dXXWC43Kyspw+fJl1pzk3bt3x4sXL/Dy5Usm7NatW8jNza107vLvhbzrDwDWrVuH//73vwgKCoKpqWntF6IeybP+dHR0sGPHDtaf6P0fXl5eWL9+vZxKVbfk3QZ79eoF4PN+K1JaWoq7d+/CwsKiNotSL+RdfwYGBnj27BlrBqiCggK8evUKBgYGtVyaulfd+qtK+g35N6SuKNV3BiiK+jG4ubnhwIEDmDdvHiZPnoz09HSEhITA1dUVAoGAieft7Y20tDQcO3YMAKCiooJJkyYhLCwMjRo1gqmpKQ4dOoTc3FxMmDCBWc7R0RGRkZFYsGABZsyYgU+fPmHz5s3o2bMn2rVrV9fFrXXyrr+IiAgcOXIEEydOBJfLZb3V18TE5Lt/+FOe9aeioiI2vezbt28BfL4K36FDh7oppJzJuw1aWFjAwcEBq1atQm5uLnR1dXHo0CF8+PAB7u7udV3cWifv+nN1dcX8+fPh7++PwYMHo6SkBPv27UNJSQmGDRtWx6WtfdWtPwB48uQJ3r59i5ycHACfZyADgEaNGsHKygpAw/8NqSsc0hAmN6Yo6ruQnJyMDRs24P79+1BXV8fgwYPh4+MDZWVlJo6XlxfS0tJw/PhxJowQgqioKBw+fBjZ2dkwMzPD3Llz0b59e1b66enp2LBhA27evAlFRUXY29tj7ty53/1JsYg868/Lywt3796VuN4dO3Y0iPcyyLv9VfT27Vu4uLhg7dq1rPcPfO/kXYdFRUUIDQ3FuXPnUFhYCAsLC/j6+qJjx451VUS5knf9XbhwAdHR0UhNTYWysjLMzc3h7e3dYE6Mq1t/AQEBOHHihFh6nTt3RlhYGPO5of+G1AXasaAoiqIoiqIoqsboMxYURVEURVEURdUY7VhQFEVRFEVRFFVjtGNBURRFURRFUVSN0Y4FRVEURVEURVE1RjsWFEVRFEVRFEXVGO1YUBRFURRFURRVY7RjQVEURVEURVFUjdGOBUVRFEVRFEVRNUY7FhRFfVPS09OhpaWFXbt2scI9PT1hbGxcP5lqIAICAsDhcJCSklIn64uKihJbX1FREQwNDbF8+fIqpyetbVDVJ9pGV65cqe+sUPWspscH2pZ+XCkpKeBwOAgICKAdC4qivi3+/v4QCASYNGmSTPHfvXuH+fPno127duDz+dDU1ESrVq0wZswYxMbGsuL26dMHGhoaUtMS/bDeuXNH4vfZ2dlQVVUFh8PB3r17paZjbGwMDofD/HG5XBgbG2Pq1Kl49eqVTOVqqFRVVbFo0SJs2LABaWlpVVq2qm2D+rElJCQgICCgzjrSVP1LSUlBQEAAEhIS6nS9tK39D+1YUBT1zXj9+jUiIiLwf//3f1BSUvpq/NTUVHTo0AFbt26Fra0t1q5dizVr1sDZ2RmJiYmIjIys1fzFxMSguLgYJiYmiIiIqDRu06ZNsXfvXuzduxchISGwsbFBREQEbGxskJmZWav5+t5MmTIFHA4HwcHBMi9T1bZByWbixIkoKiqCnZ1dfWel1iUkJGD58uX0ZO8HkpKSguXLl9dLx+JHbmtGRkYoKiqCv78/6NGZoqhvxs6dO8HhcDB27FiZ4m/cuBHp6ek4duwYhg4dKvb9u3fvajV/4eHhsLe3x9ChQzF79mwkJSWhRYsWEuNqaWlhwoQJzGdvb2/o6ekhNDQUkZGR8PPzq9W8fU/U1dXh6uqKqKgorFy5EioqKl9dpqpto76Vl5ejuLgYampq9Z2VSikqKkJRUbG+s0FR1HeMw+GAx+MBoHcsKOq7JhrTevHiRQQGBsLIyAiqqqqwsbHBjRs3AAB//vknevbsCXV1dRgYGGDFihUS07pz5w6GDx8OXV1dqKiowNzcHKtWrUJZWRkr3q1bt+Dp6QkzMzOoqamBz+ejR48eOHr0qFianp6e4HA4yM3NZU6seTweevTogZs3b4rFP3ToEKytraGnpydT+V+8eAEA6Nu3r8Tv9fX1ZUpHFnfv3kVCQgI8PDwwbtw4KCkpffWuxZf69+8PAPjnn3+kxjl9+jQ4HA62bNki8ftu3bpBIBCgtLQUQNW2hySibSQJh8OBp6enWPiBAwfQs2dP8Pl8qKmpwcbGBocPH5ZpfSIDBw5EZmYmLl++LFN8aW1DKBRi1apVsLOzg76+PrhcLpo3bw5vb29kZWUx8XJycsDj8eDq6iox/cWLF4PD4bCudObm5mLhwoUwNTWFiooKBAIBxo4di6SkJNayov3wwoULWLFiBVq2bAkej4eDBw8CAM6dO4fRo0ejRYsWUFVVhba2NpycnPDnn39KzMuRI0fQoUMH8Hg8NG/eHMuXL8eFCxfA4XAQFRXFiltcXIzVq1ejbdu24PF40NbWxpAhQ3Dv3j2Z6lXSuPjaOq4YGxujT58+uHv3LhwcHKChoYHGjRvDw8MD6enprLj5+fnw9/eHjY0NcwwyNTXFokWL8PHjR7G0CSHYtWsXbGxsoKGhAQ0NDVhaWmLp0qUAPg9rFA2Zs7e3Z4YlSmrPX3rw4AGGDx8OHR0d8Hg8tGnTBuvXr0d5eTkrXlWPb5KIhl8+efIEs2fPhoGBAdTU1NC3b188e/YMABAbG4vOnTtDVVUVxsbGCAsLk5jW7t27mXhaWlpwcnLCtWvXxOIJhUKsWbMGJiYm4PF4aNeuHWJiYqTmMS0tDd7e3mjevDm4XC4MDQ3h5eUltg2rStZ67tOnj8Tn6yqO6wc+t1t7e3sAwKRJk5ht3qdPHwDAlStXmH3ot99+g5mZGXg8HszMzPDbb7+JpS9qv1+qmA5Q/bYmaj9ZWVnw9PSErq4u+Hw+hg0bxlwUCwsLQ+vWrcHj8WBhYYG4uDixdLZt2wYnJyf89NNP4HK5MDAwwIQJEyTePSkvL8eKFStgZGQEHo+H9u3b48CBAxKfr6lK+664LegdC4pqABYtWoTy8nLMmjULJSUlCAoKgpOTE6KjozFlyhR4eXlh/PjxOHjwIJYuXQoTExPW1fSTJ0/C1dUVpqammDdvHho3bozr169j6dKlSEhIwKFDh5i4R48eRWJiIkaNGgUjIyNkZWVhz549cHV1RUxMDMaNGyeWv/79+0MgEGDp0qXIyspCcHAwBg8ejOTkZPD5fADA+/fv8ezZM/j6+spc7pYtWwIAdu3ahdmzZ0s9Qf6StKFIkk5gRMLDw6GhoQE3Nzeoq6vD2dkZe/bsQWBgIBQUZLtGI+oI6erqSo3j5OQEfX19REdHi9XFixcvcOPGDfj6+kJZWRlA9bZHTfj7+2PVqlUYMGAAVqxYAQUFBRw9ehQjR45EaGgoZsyYIVM63bp1A/D5R3rAgAGVxq2sbZSUlGDDhg1wc3PD0KFDoa6ujtu3byM8PBzXrl1DfHw8uFwutLW14eLigri4OHz48AGNGzdm0hAKhYiJiUH79u3RsWNHAJ87Fd27d8fLly8xefJktG3bFmlpadi2bRtsbGxw584dGBkZsfIyf/58lJaW4ueff4ampibMzc0BfD7h+fDhA9zd3dG0aVO8efMGu3fvRt++fXH58mX06tWLSePAgQMYO3YsWrZsiWXLlkFJSQl79uzB8ePHxcpeWlqKAQMG4O+//8bEiRMxc+ZM5ObmYteuXejRoweuXr0Ka2trmbaHJDU9rgCfh7D17dsXbm5uGDFiBO7evYuIiAjcuXMHt2/fZu7oiOrEzc2N6bj/+eefWL9+Pe7du4ezZ8+y0p04cSJiYmJgY2ODX375Bdra2khMTMThw4cRGBgIV1dXpKWlISwsDEuWLEHr1q0B/O+YIc2dO3fQu3dvKCsrY8aMGdDX18fx48excOFC3L9/X+IJuCzHt6/x8PCAhoYGlixZgoyMDAQFBaF///5YsWIFFixYAG9vb0yePBnh4eGYNm0a2rRpg549ezLLL1y4EOvXr0fXrl2xevVq5OfnIywsDPb29oiLi8OgQYOYuHPnzkVISAjs7OwwZ84cpKenY8aMGRLvvr58+RLdunVDSUkJpkyZgpYtW+Kff/7B9u3bcfnyZdy5cwdaWloylbGm9fw1dnZ2WLJkCVavXg0vLy9mv2rSpAkr3m+//YZ3795h2rRp4PP5+M9//gNfX198+PABy5Ytq/J6q9vWRAYMGICmTZsiMDAQ//zzD7Zs2YLhw4fD1dUVYWFhmDJlCng8HrZs2YIRI0bg+fPnMDExYZbfuHEjbG1t4evri8aNG+PRo0fYvXs3Ll26hIcPH0JHR4eJO3PmTOzYsQP29vaYP38+MjIy4OPjw0rvS1Vu34SiqO9WZGQkAUA6depEiouLmfC4uDgCgCgpKZHbt28z4cXFxURfX5/Y2toyYUVFRaRJkyakV69epLS0lJV+cHAwAUAuX77MhBUUFIjlo7CwkJiZmZHWrVuzwj08PAgA4u3tzQo/ePAgAUB27NjBhF26dIkAICEhIRLL6uHhQYyMjFhh//77L9HU1CQASLNmzci4cePIpk2byJ07dySm0bt3bwLgq38V60xUR9ra2sTDw4MJO3bsGAFATp06JbYeIyMjYmFhQTIyMkhGRgZJSkoiERERREtLiygpKZGHDx9KzJ/I/PnzCQDy+PFjVri/vz8BQOLj45mwqmyPZcuWEQAkOTmZCRNtI0kAsMocHx9PAJDFixeLxR06dCjh8/kkLy+PCRO1z4rrq0hJSYk4OztL/K6iytqGUCgkHz9+FAvfvXs3AUAOHDjAhJ04cYIAIFu3bmXFvXDhAgFAgoKCmDBfX1/C4/FIQkICK25KSgrh8/msehGV08zMjBQWForlRdI2evfuHdHR0SEDBw5kwkpLS4mhoSHR09MjHz58YMLz8/OJiYkJAUAiIyOZcNH+eebMGVbaubm5pFmzZqR3795i6/2SKO8V9/HaOK4Q8nk/AEA2bdrEChfle82aNaw0SkpKxPInavM3b95kwg4cOEAAkAkTJpDy8nJW/IqfJZXta7p3704UFRXJ/fv3mTChUEhGjhxJAJALFy4w4VU5vkkj2iednZ2JUChkwkNCQggAwufzycuXL5nw9PR0oqKiQsaMGcOEJSYmEg6HQ3r06MHaXm/evCFaWlrEyMiIlJWVseI6ODgwYYR83rc5HI7Y/uri4kIEAgF59eoVK9+3b98mioqKZNmyZUxYVeq7KvXcu3dvsWM/IYQkJycTAKw8XL58WWw/+fI7DQ0NVnmKi4tJly5diJKSEivcyMhI4j4kaR3VaWui9uPj48MKnzNnDvOblpuby4Tfv3+fACCLFi1ixZd0fBEd09atW8eEPXr0iAAg/fv3Z+0nDx48IAoKClJ/G2Rp3xW3BR0KRVENgLe3N7hcLvNZdKXGxsaGdcWSy+Wia9euzJVzADh//jzev3+PSZMmIScnB5mZmcyf6CrXuXPnmPjq6urM/z9+/IisrCx8/PgRDg4OePr0KfLy8sTyN2fOHNZnBwcHAGDlIyMjAwBYV5K/pkWLFrh//z5zlXz//v2YM2cOrK2t0b59e8THx4stw+PxcP78eYl/EydOlLie2NhY5OTkwMPDgwkbNGgQBAKB1OFQiYmJEAgEEAgEaNGiBSZPngxdXV3ExcWhXbt2lZZLtJ7o6GgmjBCCffv2oV27dujcuTMTXp3tUV0xMTHgcDjw8PBgtZPMzEy4uLggPz8f169flzm9xo0byzScorK2weFwoKqqCuDzbX5RGxa1sYq37Pv3748mTZqw6hX4XM9KSkoYP348gM91HRMTAzs7O/z000+scqqrq8PW1pa1T4h4e3tLfKai4jYqKChAVlYWFBUVYWNjw8pffHw83r59C09PTzRq1IgJ19DQwPTp08XS3bdvHywsLGBlZcXKY0lJCfr164dr166hqKhIQo3KpibHFRFNTU34+Piwwnx8fKCpqckarsflcpm7cGVlZcjOzkZmZiYcHR0BsLej6Gr2xo0bxe4Wynr3UJL09HT8/fffcHFxQfv27ZlwDoeDX375BQAkDjGU5fj2Nb6+vqw7rqK6dnFxQbNmzZhwgUAAc3NzVtpxcXEghGDBggWs7WVoaIhJkyYhNTWVGRonijt37lzWszWdO3dGv379WHnKzc3FiRMn4OLiAh6Px2pjxsbGMDU1lbgffE1167m2jB8/Hk2bNmU+c7lczJkzB2VlZRLvDMrb7NmzWZ9F297d3R2amppMePv27aGpqSnWrkTHF6FQiNzcXGRmZqJDhw7Q0tJi7TcnTpwAAMyaNYu1n1haWjLDdCWpavumQ6EoqgH48ha26KRE0u3NRo0ascaeP336FAAwefJkqem/f/+e+X96ejr8/f0RFxcn8aQwJyeHdTCUlD/RrdmK+RD9qBJCpOZDEmNjY4SGhiI0NBRpaWm4du0a9u7di+PHj8PZ2RmPHz9mnZAqKioyJytfkjQeGfg8DEogEKBp06as5yOcnJxw6NAhZGZmig1vMjY2Zt63IBqXbGpqKlOZRJ2HmJgYrF69GgoKCrh69SpSUlKwfv16VtzqbI/qevr0KQghsLCwkBqnYlv5GkKITMPXvtY2Dh48iKCgINy7d4959kQkOzub+b+o8xAcHIznz5/DzMwMhYWFiI2NhZOTEzNkIiMjA1lZWTh37hwEAoHEdUo6gTUzM5MY999//8Uvv/yCs2fPIicnR2LZACA5ORkAmCFUFUkKe/r0KYqKiqTmEfg87K/iiWlV1OS4UjGNiie7AKCiooIWLVqIPauybds27NixA48fP4ZQKGR9V3E7vnjxAgYGBmJDXGpKVP9t27YV+65169ZQUFAQyzMg2/Hta6pa16mpqTLlWxSWlJQEa2trJv+S9uE2bdqwOgrPnj2DUChEeHg4wsPDZcq3LKpbz7VFNFSpojZt2gCAXNcrTU33s0uXLiEwMBA3b97Ep0+fWN9V3G++dnw5ffq0TPn7WvumHQuKagCkzeoiy2wvopO1DRs2MOPLv2RoaMjEdXJywtOnTzFr1ixYW1tDS0sLioqKiIyMxP79+8VOCCrLR8UTRdHJ0YcPH76aZ2kMDAwwcuRIjBw5EuPHj8f+/ftx6tQpsXHfVZGcnIzLly+DECL1xHHfvn1iV53U1dWldmBk4e7ujtmzZ+PSpUtwdHREdHQ0FBUVWWWp7vaoSNqJ/ZcP7YvWx+FwcPr0aanbVNLJgjTZ2dmVnhSLVNY2YmNjMXr0aHTt2hUhISFo1qwZeDweysvLMWDAALHyu7u7Izg4GNHR0Vi5ciViY2NRUFDAuhslapeOjo5YuHChzOWRdLeioKAAdnZ2KCwsxOzZs2FpaQk+nw8FBQWsWbMGly5dkjn9LxFCYGlpWem0vbLUrzQ1Oa5UVXBwMObNmwcnJyf4+vrC0NAQXC4Xb968gaen51fbcX2S5fhW3TRqI+3qEq1jwoQJrP2jItHdQnmqyjHqe1xvTbb97du34eTkBFNTU6xduxYmJibMu5bGjBlTK/tNVdsg7VhQ1A+uVatWAGQ7EX7w4AHu37+PpUuXir05effu3TXKh+iEtCrDBypja2uL/fv3482bNzVKJzIykpmBRltbW+x7f39/REREiHUsamrcuHHw8/NDdHQ0evTogcOHD6Nfv34wMDBg4tTG9hDdzfnygWZJV+5atWqFM2fOoHnz5hKv+lVFSkoKysrKvjosDKi8bezduxc8Hg+XL19mndgnJiZKTKtDhw7o0KED9u3bhxUrViA6Opp5sFtEIBBAW1sbeXl5NeocAsDFixfx9u1bREREiL3Yz9/fn/VZNPONaDagiiSFtWrVChkZGXBwcKjRECB5SkpKQklJCeuuRXFxMZKSklhXzffu3QtjY2OcPn2aVZYzZ86IpWlmZoa4uDi8f/++0rsWsk7mICK6Qvz48WOx7xITEyEUCqt1hV7eRHl6/Pix2APDT548YcUR/ZuYmCg1roipqSk4HA5KSkpqvB9UVNV6bty4scRhrZKOUbJsc9Fd+oq+rCfReiVdzKjueuVh//79KC8vx+nTp1l3OAoLC1l3KwD28eXLdizp+FJd3+aRiKKoOtO/f3/o6elh7dq1Eg+iRUVFyM/PB/C/KxdfXql49OhRjcfECgQCtG3blpnOUhZXrlyROIZcKBQyY2VFt7irQygUIioqCpaWlpg6dSpGjBgh9jd27Fg8fPgQt2/frvZ6JBEIBBg4cCBiY2MRExODvLw8sauGtbE9RHdhLly4wAoPCgoSiyt6BmXJkiViU0ICVRsGJdrOvXv3/mrcytqGoqIiOBwO68ocIQQrV66Ump6HhwdSU1Oxf/9+XLp0CaNHj2bmYAc+D3MaP348bt26JXUaXVmn2pS2jc6dOyc2ZaO1tTUMDAwQFRXFOikoKCjAjh07xNJ2d3fHu3fvpN6xqMr2kJe8vDxs27aNFbZt2zbk5eVh2LBhTJhoO1asp7KyMqxdu1YsTdGzMAsWLBC7IltxeQ0NDQCy3wXV09ND9+7dcfz4cTx69IiV5po1awAAw4cPlymtuuTi4gIOh4MNGzawhgKmpaUhMjISRkZG6NSpEytucHAwax++e/eu2DFAR0cHgwYNQmxsrMR9jxDCPP9UFVWtZzMzM+Tn5+PWrVtMmFAoxKZNm8TSlmWbx8TE4PXr18znkpISbNq0CYqKinB2dmatNzExkXVxqri4GFu3bq3WeuVB2vFl9erVYvvGkCFDAAAhISGs7x4+fCg261pN0DsWFPWDU1dXR3R0NIYNGwZzc3NMnjwZpqamyMnJQWJiImJjY3H06FH06dMHrVu3Rtu2bbF+/Xp8/PgR5ubmeP78OXbu3AlLS0uJV5WqYuTIkVixYgXS0tJYV+al2bhxI/766y8MGTIEnTt3hpaWFt69e4cjR44gPj4e9vb2GDx4cLXzc+7cObx69QpTpkyRGsfNzQ0BAQEIDw9Hly5dqr0uSTw8PPDHH39g3rx50NLSYp2IAaiV7TF27FgsWbIEXl5eSExMROPGjXHmzBmJU/J26dIFAQEBCAgIQMeOHTFy5EgYGhoiLS0N8fHxOHXqFEpKSmQq26lTp6Crq8vMO/810trGiBEjcOTIETg4OMDd3R2lpaU4duxYpVMHjx8/HgsWLICPjw+EQqHEYR6rVq3CX3/9hVGjRmHUqFGwtbUFl8tFamoqTp06BSsrK7F3SkjSs2dP6OvrY968eUhJSUHTpk2RkJCAvXv3wtLSEg8fPmTiKikpYePGjRg/fjy6du2KKVOmQElJCVFRUdDR0UFycjLryuisWbNw/vx5+Pn54dKlS3BwcICmpiZevnyJixcvMndy6lPLli2xfPlyPHr0CFZWVoiPj0dERAQsLCxY0wePGDECixcvxsCBA+Hq6oq8vDzs37+feaC7opEjR2L06NGIjo7Gixcv4OLigkaNGuH58+c4e/Ysc7LapUsXKCgoYNWqVcjOzoa6ujpMTExgY2MjNb8hISHo3bs3evXqxUyDeuLECZw9exbjxo2T+s6c+mRubg4/Pz+sX78ednZ2GD16NDPdbEFBAWJiYpgTUAsLC8yYMQOhoaFwcHCAm5sb0tPTERoaig4dOoi9/2T79u3o2bMn7Ozs4O7ujk6dOkEoFCIpKQlxcXFwd3dn3iNRFVWpZy8vLwQFBWH48OGYNWsWuFwuDh8+LHFIUps2bcDn87Ft2zaoqalBW1sbenp6zAPHwOcOg42NDaZPnw4+n4/9+/fj9u3b+PXXX1nPI82cORO///47HB0dMX36dJSUlGDv3r0ShzxWp63VhuHDh2PTpk0YNGgQvLy8wOVycf78eTx48EDsub+2bdvCy8sLYWFhcHR0xPDhw5GRkYGtW7eiU6dOiI+Pr507L2JzVFEU9d2obIo7fDFVqIi06UUfPnxIxo8fTwwNDYmysjLR09Mj3bp1I4GBgSQrK4uJl5KSQkaMGEF0dXWJqqoq6dKlC4mNja3xVKaEfJ4eUUlJiWzcuFFivr+ccvD69etk7ty5xNramujp6RElJSWipaVFbG1tSVBQEPn06RMrfu/evYm6urrE/BDyv6kfRVNpjhgxggAgDx48kLoMIYSYmZkRLS0tZtpTIyMj0rZt20qXkUVxcTFp3LgxAUCmTp0qMU5VtoekMEIIuXHjBunevTtRUVEhOjo65OeffybZ2dlS29CJEyeIk5MTadSoEeFyuaRp06ZkwIABZPv27ax40qabLSgoIOrq6mT+/Pky10VlbSMsLIy0bt2aqKioEH19ffLzzz+TrKwsqfknhBBnZ2cCgLRq1UrqOgsLC0lgYCBp164d4fF4RENDg1hYWJCpU6eSGzduiJVT2lST9+/fJ/379yfa2tpEQ0OD9O7dm1y9elXq/nHw4EFiaWlJuFwuadasGQkICCCxsbFi0+cS8nmK2pCQEGJtbU3U1NSImpoaMTU1JePGjSNnz56VWrbK8l5bxxXRdJ3x8fHE3t6eqKmpEW1tbTJhwgTy7t07VtyysjKyevVq0rJlS8Llcknz5s2Jn58fefLkidiUooR8nlY2NDSUdOrUiaiqqhINDQ1iaWlJAgICWPGioqJI69atibKycqXtoaKEhAQydOhQpn1bWFiQdevWsaZnlVbmr9XTl6Ttk5KmUhWRNv1qWFgY6dixI1FRUSF8Pp84OjqSq1evisUrLy8nK1euJM2bNydcLpe0bduW7Nu3T2peMjIyyPz580mrVq2IiooK0dLSIu3atSO+vr6sKbGrOuWqrPVMCCEnT54kHTp0IFwulxgYGJAFCxaQxMREiXV08uRJ0qlTJ6KiokIAMFPGVpwmNiQkhJiamhIul0tMTU3J5s2bJeYxKiqKmJmZEWVlZWJsbEzWrVtHLl68KHFK26q2NWntp7IpcyVNgXv06FHSuXNnoqamRnR0dMjo0aNJamqqxLhlZWUkICCANGvWjHC5XGJpaUkOHDhA5s2bRwCQ9+/ffzV/hIi374rtlfP/I1AURX0Tpk+fjnPnzuHZs2esq5Wenp64cuWKxLeJUt+mqKgoTJo0CcnJyaw354aEhOCXX35hZveRlbS28SMICgrC/Pnzcf36ddja2tZ3dmRibGwMY2Nj1lu9Kaq+XLlyBfb29oiMjJTpDew/kiFDhuDSpUvIy8ur8eQM9BkLiqK+KYGBgcjKykJkZGR9Z4WSg6KiIqxduxZ+fn5V6lQAP0bbKCkpEXt+paCgAFu3boWOjg7rHSYURVFVIemZxAcPHuD06dNwcHColRnf6DMWFEV9U/T09JCbm1vf2aDkRFVVFWlpadVa9kdoG0lJSRg4cCDGjBkDExMTpKWlYc+ePUhOTsb27dvF3glBURQlqz179iA6OhqDBw+GQCBAYmIiwsLCwOVyERgYWCvroB0LiqIoivpGCAQC2NraIiYmBunp6VBSUoKlpSXWrl2LUaNG1Xf2KIr6jnXu3BlHjx7Fli1b8OHDB/D5fDg4OGDZsmXMzGE1RZ+xoCiKoiiKoiiqxugzFhRFURRFURRF1RjtWFAURVEURVEUVWO0Y0FRFEVRFEVRVI3RjgVFURRFURRFUTVGOxYURVEURVEURdUY7VhQFEVRFEVRFFVjtGNBURRFURRFUVSN0Y4FRVEURVEURVE19v8AGcHvSVN+aD8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x470 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_names = [\"femaleTeamMembersPercent\", \"helpHoursTotal\", \"codingDeliverablesHoursTotal\", \"globalLeadAdminHoursTotal\", \"uniqueCommitMessagePercent\", \"teamLeadGender\",\"teamDistribution_encoded\",\"LD1\",\"productLetterGrade_encoded\"]\n",
    "\n",
    "explainer = shap.DeepExplainer(model1, X_train)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "\n",
    "\n",
    "shap.summary_plot(shap_values, X_test, feature_names=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
